{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/hyperparameter-tuning-with-keras-tuner-283474fbfbe\n",
    "\n",
    "https://github.com/cedricconol/keras-tuner-demo/blob/master/Hyperparameter%20Tuning%20Demo%20with%20Keras%20Tuner.ipynb\n",
    "\n",
    "https://keras-team.github.io/keras-tuner/documentation/tuners/\n",
    "\n",
    "https://analyticsindiamag.com/how-to-use-keras-tuner-for-hyper-parameter-tuning-of-deep-learning-models/ -- for CNN\n",
    "\n",
    "https://www.sicara.ai/blog/hyperparameter-tuning-keras-tuner -- CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Dropout\n",
    "\n",
    "# set random seed\n",
    "from numpy.random import seed\n",
    "seed(42)\n",
    "import tensorflow\n",
    "tensorflow.random.set_seed(42)\n",
    "# set the random seed in numpy and tensorflow to 42 to get reproducible results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_student = pd.read_csv(r\"https://raw.githubusercontent.com/jaiswalvineet/Artificial-Neural-Network-ANN-with-Keras-simplified/master/Student.csv\", encoding ='utf-8')\n",
    "print(df_all_student.shape)\n",
    "df_all_student.sample(5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=df_all_student.iloc[:, 2:12]\n",
    "y=df_all_student.iloc[:, 12]\n",
    "\n",
    "labelencoder_X_School = LabelEncoder()\n",
    "X.iloc[:,1]= labelencoder_X_School.fit_transform(X.iloc[:, 1])\n",
    "\n",
    "labelencoder_X_Gender = LabelEncoder()\n",
    "X.iloc[:,2]= labelencoder_X_Gender.fit_transform(X.iloc[:, 2])\n",
    "\n",
    "\n",
    "X.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# onehotencoder = OneHotEncoder(categorical_features = [1])\n",
    "# X = onehotencoder.fit_transform(X).toarray()\n",
    "\n",
    "ct = ColumnTransformer([(\"School\", OneHotEncoder(), [1])], remainder = 'passthrough')\n",
    "X = ct.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X,index=X[:,0]).head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X[:, 1:]\n",
    "pd.DataFrame(X,index=X[:,0]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n",
    "\n",
    "#Scalling\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = Sequential()\n",
    "\n",
    "cf.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\n",
    "cf.add(Dropout(rate = 0.1))\n",
    "\n",
    "cf.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "cf.add(Dropout(rate = 0.1))\n",
    "cf.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "cf.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "cf.fit(X_train, y_train, batch_size = 10, epochs = 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_prediction =cf.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test\n",
    "X_test[0:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf.predict(X_test[0:1,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(y_prediction,index=y_prediction[:,0]).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "def kera_classifier():\n",
    "    cf = Sequential()\n",
    "    cf.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\n",
    "    cf.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "    cf.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "    cf.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return cf\n",
    "cf = KerasClassifier(build_fn = kera_classifier, batch_size = 10, epochs = 10)\n",
    "accuracies = cross_val_score(estimator = cf, X = X_train, y = y_train, cv = 2)\n",
    "mean = accuracies.mean()\n",
    "variance = accuracies.std()\n",
    "\n",
    "print(mean, variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.wrappers.scikit_learn import KerasClassifier # KerasRegressor for regression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "def kera_classifier(optimizer):\n",
    "    cf = Sequential()\n",
    "    cf.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu', input_dim = 11))\n",
    "    cf.add(Dense(units = 6, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "    cf.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "    cf.compile(optimizer = optimizer, loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "    return cf\n",
    "cf = KerasClassifier(build_fn = kera_classifier)\n",
    "parameters = {'batch_size': [10, 15],\n",
    "              'epochs': [10, 20],\n",
    "              'optimizer': ['adam', 'rmsprop']}\n",
    "gv_search = GridSearchCV(estimator = cf,\n",
    "                           param_grid = parameters,\n",
    "                           scoring = 'accuracy',\n",
    "                           cv = 2)\n",
    "gv_search = gv_search.fit(X_train, y_train)\n",
    "best_param = gv_search.best_params_\n",
    "best_acc = gv_search.best_score_\n",
    "\n",
    "print(best_param, \"\\n\")\n",
    "print(\"accuracy \", best_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(best_param, \"\\n\")\n",
    "print(\"accuracy \", best_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter Tunning using Keras Tunner\n",
    "\n",
    "#### ‘hp’ is an alias for Keras Tuner’s HyperParameters class and hp.Int uses to search int values like neurons and layers. hp.Choice uses to choose categorical values like activation function and hp.Float uses to choose floaoting value like drop out layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from kerastuner import HyperModel\n",
    "\n",
    "class ClassificationHyperModel(HyperModel):\n",
    "    def __init__(self, input_shape):\n",
    "        self.input_shape = input_shape\n",
    "    def build(self, hp):\n",
    "        model = Sequential()\n",
    "        model.add(\n",
    "            layers.Dense(\n",
    "                units=hp.Int('units', 8, 64, 4, default=8), # ranges between 8 to 64 with step size is 4 \n",
    "                activation=hp.Choice(\n",
    "                    'dense_activation',\n",
    "                    values=['relu', 'tanh', 'sigmoid'],\n",
    "                    default='relu'),\n",
    "                input_shape=input_shape\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        model.add(\n",
    "            layers.Dense(\n",
    "                units=hp.Int('units', 16, 64, 4, default=16),\n",
    "                activation=hp.Choice(\n",
    "                    'dense_activation',\n",
    "                    values=['relu', 'tanh', 'sigmoid'],\n",
    "                    default='relu')\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        model.add(\n",
    "            layers.Dropout(\n",
    "                hp.Float(\n",
    "                    'dropout',\n",
    "                    min_value=0.0,\n",
    "                    max_value=0.1,\n",
    "                    default=0.005,\n",
    "                    step=0.01)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        model.add(layers.Dense(1))\n",
    "        \n",
    "        model.compile(\n",
    "            optimizer='rmsprop',loss='mse',metrics=['mse']\n",
    "        )\n",
    "        \n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (x_train.shape[1],)\n",
    "hypermodel = ClassificationHyperModel(input_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HyperTunning using RandomSearch\n",
    "\n",
    "This hyperparameter tuning method randomly tries a combination of hyperparameters from a given search space"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_rs = RandomSearch(\n",
    "            hypermodel, # it is defined just above cell.\n",
    "            objective='mse',\n",
    "            seed=42,\n",
    "            max_trials=10,\n",
    "            executions_per_trial=2)\n",
    "\n",
    "tuner_rs.search(x_train_scaled, y_train, epochs=10, validation_split=0.2, verbose=0)\n",
    "\n",
    "best_model = tuner_rs.get_best_models(num_models=1)[0]\n",
    "loss, mse = best_model.evaluate(x_test_scaled, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning using Hyperband\n",
    "\n",
    "Hyperband is based on the algorithm by Li et. al. It optimizes random search method through adaptive resource allocation and early-stopping. Hyperband first runs random hyperparameter configurations for one iteration or two, then selects which configurations perform well, then continues tuning the best performers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_hb = Hyperband(\n",
    "            hypermodel,\n",
    "            max_epochs=5,\n",
    "            objective='mse',\n",
    "            seed=42,\n",
    "            executions_per_trial=2\n",
    "        )\n",
    "tuner_hb.search(x_train_scaled, y_train, epochs=10, validation_split=0.2, verbose=0)\n",
    "best_model = tuner_hb.get_best_models(num_models=1)[0]\n",
    "best_model.evaluate(x_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Optimization\n",
    "\n",
    "Bayesian optimization is a probabilistic model that maps the hyperparameters to a probability score on the objective function. Unlike Random Search and Hyperband models, Bayesian Optimization keeps track of its past evaluation results and uses it to build the probability model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuner_bo = BayesianOptimization(\n",
    "            hypermodel,\n",
    "            objective='mse',\n",
    "            max_trials=10,\n",
    "            seed=42,\n",
    "            executions_per_trial=2\n",
    "        )\n",
    "tuner_bo.search(x_train_scaled, y_train, epochs=10, validation_split=0.2, verbose=0)\n",
    "best_model = tuner_bo.get_best_models(num_models=1)[0]\n",
    "best_model.evaluate(x_test_scaled, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keras tunner for sklearn models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 10 Complete [00h 00m 00s]\n",
      "score: 0.85\n",
      "\n",
      "Best score So Far: 0.975\n",
      "Total elapsed time: 00h 00m 01s\n"
     ]
    }
   ],
   "source": [
    "import kerastuner as kt\n",
    "from sklearn import ensemble\n",
    "from sklearn import datasets\n",
    "from sklearn import linear_model\n",
    "from sklearn import metrics\n",
    "from sklearn import model_selection\n",
    "\n",
    "def build_model(hp):\n",
    "  model_type = hp.Choice('model_type', ['random_forest', 'ridge'])\n",
    "  if model_type == 'random_forest':\n",
    "    model = ensemble.RandomForestClassifier(\n",
    "        n_estimators=hp.Int('n_estimators', 10, 50, step=10),\n",
    "        max_depth=hp.Int('max_depth', 3, 10))\n",
    "  else:\n",
    "    model = linear_model.RidgeClassifier(\n",
    "        alpha=hp.Float('alpha', 1e-3, 1, sampling='log'))\n",
    "  return model\n",
    "\n",
    "tuner = kt.tuners.Sklearn(\n",
    "    oracle=kt.oracles.BayesianOptimization(\n",
    "        objective=kt.Objective('score', 'max'),\n",
    "        max_trials=10),\n",
    "    hypermodel=build_model,\n",
    "    scoring=metrics.make_scorer(metrics.accuracy_score),\n",
    "    cv=model_selection.StratifiedKFold(5),\n",
    "    directory='.',\n",
    "    project_name='my_project')\n",
    "\n",
    "X, y = datasets.load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = model_selection.train_test_split(\n",
    "    X, y, test_size=0.2)\n",
    "\n",
    "tuner.search(X_train, y_train)\n",
    "\n",
    "best_model = tuner.get_best_models(num_models=1)[0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
