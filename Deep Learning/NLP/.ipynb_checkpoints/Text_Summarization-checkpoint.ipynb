{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2tzbqWJjMJ4x"
   },
   "source": [
    "# Text Summarization using different methods\n",
    "\n",
    "https://www.machinelearningplus.com/nlp/text-summarization-approaches-nlp-example/\n",
    "\n",
    "Application :-\n",
    "\n",
    "https://womencourage.acm.org/archive/2014/5_TechTalk_AutomaticTextSummarization.pdf\n",
    "\n",
    "1) News Summarrization\n",
    "2) NewsLetter from whole story.\n",
    "3) Google search on basis on entity. Like shows summary in link\n",
    "4) KT Internal document.\n",
    "5) Financial research\n",
    "6) Short description from complaint and request.\n",
    "7) Legal contract analysis\n",
    "8) Social media marketing\n",
    "9) Email overload\n",
    "10) Science and R&D\n",
    "11) Help desk and customer support\n",
    "12) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MMgRoUWO-Ci"
   },
   "source": [
    "#### Gensim Method \n",
    "\n",
    "gensim is a very handy python library for performing NLP tasks. The text summarization process using gensim library is based on TextRank Algorithm\n",
    "\n",
    "TextRank is an extractive summarization technique. It is based on the concept that words which occur more frequently are significant. Hence , the sentences containing highly frequent words are important .\n",
    "\n",
    "Based on this , the algorithm assigns scores to each sentence in the text . The top-ranked sentences make it to the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0SOQaiPbMJKy"
   },
   "outputs": [],
   "source": [
    "# Importing package and summarizer\n",
    "import gensim\n",
    "from gensim.summarization import summarize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1215,
     "status": "ok",
     "timestamp": 1617427235254,
     "user": {
      "displayName": "Rimanshu Mangal",
      "photoUrl": "",
      "userId": "01816973904928346155"
     },
     "user_tz": -330
    },
    "id": "ab6A0tvHOHwv",
    "outputId": "5c7ced61-1114-436b-e815-02d191ee510f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They become high in calories, high in cholesterol, low in healthy nutrients, high in sodium mineral, high in sugar, starch, unhealthy fat, lack of protein and lack of dietary fibers.\n",
      "Processed and junk foods are the means of rapid and unhealthy weight gain and negatively impact the whole body throughout the life.\n",
      "Junk foods tastes good and looks good however do not fulfil the healthy calorie requirement of the body.\n",
      "It is found according to the Centres for Disease Control and Prevention that Kids and children eating junk food are more prone to the type-2 diabetes.\n",
      "Eating junk food daily lead us to the nutritional deficiencies in the body because it is lack of essential nutrients, vitamins, iron, minerals and dietary fibers.\n",
      "It increases risk of cardiovascular diseases because it is rich in saturated fat, sodium and bad cholesterol.\n",
      "High sodium and bad cholesterol diet increases blood pressure and overloads the heart functioning.\n",
      "One who like junk food develop more risk to put on extra weight and become fatter and unhealthier.\n",
      "Junk foods contain high level carbohydrate which spike blood sugar level and make person more lethargic, sleepy and less active and alert.\n",
      "For instance, foods like French fries, burgers, candy, and cookies, all have high amounts of sugar and fats. \n",
      "\n",
      "They become high in calories, high in cholesterol, low in healthy nutrients, high in sodium mineral, high in sugar, starch, unhealthy fat, lack of protein and lack of dietary fibers.\n",
      "Processed and junk foods are the means of rapid and unhealthy weight gain and negatively impact the whole body throughout the life.\n",
      "Eating junk food daily lead us to the nutritional deficiencies in the body because it is lack of essential nutrients, vitamins, iron, minerals and dietary fibers.\n",
      "It increases risk of cardiovascular diseases because it is rich in saturated fat, sodium and bad cholesterol.\n",
      "High sodium and bad cholesterol diet increases blood pressure and overloads the heart functioning. \n",
      "\n",
      "They become high in calories, high in cholesterol, low in healthy nutrients, high in sodium mineral, high in sugar, starch, unhealthy fat, lack of protein and lack of dietary fibers.\n",
      "They become high in calories, high in cholesterol, low in healthy nutrients, high in sodium mineral, high in sugar, starch, unhealthy fat, lack of protein and lack of dietary fibers.\n"
     ]
    }
   ],
   "source": [
    "original_text = 'Junk foods taste good that’s why it is mostly liked by everyone of any age group especially kids and school going children. They generally ask for the junk food daily because they have been trend so by their parents from the childhood. They never have been discussed by their parents about the harmful effects of junk foods over health. According to the research by scientists, it has been found that junk foods have negative effects on the health in many ways. They are generally fried food found in the market in the packets. They become high in calories, high in cholesterol, low in healthy nutrients, high in sodium mineral, high in sugar, starch, unhealthy fat, lack of protein and lack of dietary fibers. Processed and junk foods are the means of rapid and unhealthy weight gain and negatively impact the whole body throughout the life. It makes able a person to gain excessive weight which is called as obesity. Junk foods tastes good and looks good however do not fulfil the healthy calorie requirement of the body. Some of the foods like french fries, fried foods, pizza, burgers, candy, soft drinks, baked goods, ice cream, cookies, etc are the example of high-sugar and high-fat containing foods. It is found according to the Centres for Disease Control and Prevention that Kids and children eating junk food are more prone to the type-2 diabetes. In type-2 diabetes our body become unable to regulate blood sugar level. Risk of getting this disease is increasing as one become more obese or overweight. It increases the risk of kidney failure. Eating junk food daily lead us to the nutritional deficiencies in the body because it is lack of essential nutrients, vitamins, iron, minerals and dietary fibers. It increases risk of cardiovascular diseases because it is rich in saturated fat, sodium and bad cholesterol. High sodium and bad cholesterol diet increases blood pressure and overloads the heart functioning. One who like junk food develop more risk to put on extra weight and become fatter and unhealthier. Junk foods contain high level carbohydrate which spike blood sugar level and make person more lethargic, sleepy and less active and alert. Reflexes and senses of the people eating this food become dull day by day thus they live more sedentary life. Junk foods are the source of constipation and other disease like diabetes, heart ailments, clogged arteries, heart attack, strokes, etc because of being poor in nutrition. Junk food is the easiest way to gain unhealthy weight. The amount of fats and sugar in the food makes you gain weight rapidly. However, this is not a healthy weight. It is more of fats and cholesterol which will have a harmful impact on your health. Junk food is also one of the main reasons for the increase in obesity nowadays.This food only looks and tastes good, other than that, it has no positive points. The amount of calorie your body requires to stay fit is not fulfilled by this food. For instance, foods like French fries, burgers, candy, and cookies, all have high amounts of sugar and fats. Therefore, this can result in long-term illnesses like diabetes and high blood pressure. This may also result in kidney failure. Above all, you can get various nutritional deficiencies when you don’t consume the essential nutrients, vitamins, minerals and more. You become prone to cardiovascular diseases due to the consumption of bad cholesterol and fat plus sodium. In other words, all this interferes with the functioning of your heart. Furthermore, junk food contains a higher level of carbohydrates. It will instantly spike your blood sugar levels. This will result in lethargy, inactiveness, and sleepiness. A person reflex becomes dull overtime and they lead an inactive life. To make things worse, junk food also clogs your arteries and increases the risk of a heart attack. Therefore, it must be avoided at the first instance to save your life from becoming ruined.The main problem with junk food is that people don’t realize its ill effects now. When the time comes, it is too late. Most importantly, the issue is that it does not impact you instantly. It works on your overtime; you will face the consequences sooner or later. Thus, it is better to stop now.You can avoid junk food by encouraging your children from an early age to eat green vegetables. Their taste buds must be developed as such that they find healthy food tasty. Moreover, try to mix things up. Do not serve the same green vegetable daily in the same style. Incorporate different types of healthy food in their diet following different recipes. This will help them to try foods at home rather than being attracted to junk food.In short, do not deprive them completely of it as that will not help. Children will find one way or the other to have it. Make sure you give them junk food in limited quantities and at healthy periods of time.'\n",
    "\n",
    "# Passing the text corpus to summarizer \n",
    "short_summary = summarize(original_text)\n",
    "print(short_summary, \"\\n\")\n",
    "\n",
    "# Summarization by ratio\n",
    "summary_by_ratio=summarize(original_text,ratio=0.1)\n",
    "print(summary_by_ratio, \"\\n\")\n",
    "\n",
    "# Summarization by word count\n",
    "summary_by_word_count=summarize(original_text,word_count=30)\n",
    "print(summary_by_word_count)\n",
    "\n",
    "# Summarization when both ratio & word count is given\n",
    "summary=summarize(original_text, ratio=0.1, word_count=30) # in this case word count will be prefered and ratio will be ignored.\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fR2fY8MIQ3pX"
   },
   "source": [
    "#### Text Summarization with Sumy\n",
    "\n",
    "A sentence which is similar to many other sentences of the text has a high probability of being important. The approach of LexRank is that a particular sentence is recommended by other similar sentences and hence is ranked higher.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IwMOoz8lQ1mr"
   },
   "outputs": [],
   "source": [
    "# Installing and Importing sumy\n",
    "# !pip install sumy\n",
    "import sumy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1678,
     "status": "ok",
     "timestamp": 1617379070986,
     "user": {
      "displayName": "Rimanshu Mangal",
      "photoUrl": "",
      "userId": "01816973904928346155"
     },
     "user_tz": -330
    },
    "id": "2X1eCD3hOHzJ",
    "outputId": "710793f2-eb06-4adc-9ec5-73f0a3070a42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    }
   ],
   "source": [
    "# sumy.summarizers\n",
    "\n",
    "import nltk; nltk.download('punkt')\n",
    "\n",
    "# Importing the parser and tokenizer\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "\n",
    "# Import the LexRank summarizer\n",
    "from sumy.summarizers.lex_rank import LexRankSummarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1095,
     "status": "ok",
     "timestamp": 1617379076431,
     "user": {
      "displayName": "Rimanshu Mangal",
      "photoUrl": "",
      "userId": "01816973904928346155"
     },
     "user_tz": -330
    },
    "id": "QvqflPjbOH2k",
    "outputId": "939f3f42-7028-4001-fd9f-5a61129f27cc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is found according to the Centres for Disease Control and Prevention that Kids and children eating junk food are more prone to the type-2 diabetes.\n",
      "It is more of fats and cholesterol which will have a harmful impact on your health.\n",
      "Children will find one way or the other to have it.\n"
     ]
    }
   ],
   "source": [
    "# Initializing the parser\n",
    "my_parser = PlaintextParser.from_string(original_text,Tokenizer('english'))\n",
    "\n",
    "# Creating a summary of 3 sentences.\n",
    "lex_rank_summarizer = LexRankSummarizer()\n",
    "lexrank_summary = lex_rank_summarizer(my_parser.document,sentences_count=3)\n",
    "\n",
    "# Printing the summary\n",
    "for sentence in lexrank_summary:\n",
    "  print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d3cQzGTwUn-6"
   },
   "source": [
    "#### LSA (Latent semantic analysis)\n",
    "\n",
    "Latent Semantic Analysis is a unsupervised learning algorithm that can be used for extractive text summarization.\n",
    "\n",
    "It extracts semantically significant sentences by applying singular value decomposition(SVD) to the matrix of term-document frequency. To learn more about this algorithm, check out here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1079,
     "status": "ok",
     "timestamp": 1617379427106,
     "user": {
      "displayName": "Rimanshu Mangal",
      "photoUrl": "",
      "userId": "01816973904928346155"
     },
     "user_tz": -330
    },
    "id": "Bb6mPZBHUoS3",
    "outputId": "2229db69-4a36-4848-9273-61c881641f08"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Junk foods taste good that’s why it is mostly liked by everyone of any age group especially kids and school going children.\n",
      "To make things worse, junk food also clogs your arteries and increases the risk of a heart attack.\n",
      "Therefore, it must be avoided at the first instance to save your life from becoming ruined.The main problem with junk food is that people don’t realize its ill effects now.\n"
     ]
    }
   ],
   "source": [
    "# Import the summarizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "\n",
    "# creating the summarizer\n",
    "lsa_summarizer=LsaSummarizer()\n",
    "lsa_summary= lsa_summarizer(my_parser.document,3)\n",
    "\n",
    "# Printing the summary\n",
    "for sentence in lsa_summary:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAjOnK_TVT5N"
   },
   "source": [
    "#### Luhn\n",
    "\n",
    "Luhn Summarization algorithm’s approach is based on TF-IDF (Term Frequency-Inverse Document Frequency). It is useful when very low frequent words as well as highly frequent words(stopwords) are both not significant.\n",
    "\n",
    "Based on this, sentence scoring is carried out and the high ranking sentences make it to the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1061,
     "status": "ok",
     "timestamp": 1617379590014,
     "user": {
      "displayName": "Rimanshu Mangal",
      "photoUrl": "",
      "userId": "01816973904928346155"
     },
     "user_tz": -330
    },
    "id": "OZoF0KWrUoVa",
    "outputId": "1295baa9-11ab-4c35-f328-632842b13aa8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "They become high in calories, high in cholesterol, low in healthy nutrients, high in sodium mineral, high in sugar, starch, unhealthy fat, lack of protein and lack of dietary fibers.\n",
      "It is found according to the Centres for Disease Control and Prevention that Kids and children eating junk food are more prone to the type-2 diabetes.\n",
      "Eating junk food daily lead us to the nutritional deficiencies in the body because it is lack of essential nutrients, vitamins, iron, minerals and dietary fibers.\n"
     ]
    }
   ],
   "source": [
    "# Import the summarizer\n",
    "from sumy.summarizers.luhn import LuhnSummarizer\n",
    "\n",
    "#  Creating the summarizer\n",
    "luhn_summarizer=LuhnSummarizer()\n",
    "luhn_summary=luhn_summarizer(my_parser.document,sentences_count=3)\n",
    "\n",
    "# Printing the summary\n",
    "for sentence in luhn_summary:\n",
    "  print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zGgPveQ0VyCU"
   },
   "source": [
    "KL-Sum algorithm\n",
    "\n",
    "It selects sentences based on similarity of word distribution as the original text. It aims to lower the KL-divergence criteria (learn more). It uses greedy optimization approach and keeps adding sentences till the KL-divergence decreases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1088,
     "status": "ok",
     "timestamp": 1617379964387,
     "user": {
      "displayName": "Rimanshu Mangal",
      "photoUrl": "",
      "userId": "01816973904928346155"
     },
     "user_tz": -330
    },
    "id": "u7iJz2y1UoYs",
    "outputId": "ed7262cb-2a07-492e-895c-295867422e00"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It is found according to the Centres for Disease Control and Prevention that Kids and children eating junk food are more prone to the type-2 diabetes.\n",
      "High sodium and bad cholesterol diet increases blood pressure and overloads the heart functioning.\n",
      "Junk food is the easiest way to gain unhealthy weight.\n"
     ]
    }
   ],
   "source": [
    "from sumy.summarizers.kl import KLSummarizer\n",
    "\n",
    "# Instantiating the  KLSummarizer\n",
    "kl_summarizer=KLSummarizer()\n",
    "kl_summary=kl_summarizer(my_parser.document,sentences_count=3)\n",
    "\n",
    "# Printing the summary\n",
    "for sentence in kl_summary:\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xkhQ-xNNXKgn"
   },
   "source": [
    "#### What is Abstractive Text Summarization?\n",
    "\n",
    "Abstractive summarization is the new state of art method, which generates new sentences that could best represent the whole text. This is better than extractive methods where sentences are just selected from original text for the summary.\n",
    "\n",
    "How to easily implement abstractive summarization?\n",
    "\n",
    "A simple and effective way is through the Huggingface’s transformers library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yLJFv56LXapJ"
   },
   "source": [
    "#### Summarization with T5 Transformers\n",
    "\n",
    "T5 is an encoder-decoder model. It converts all language problems into a text-to-text format.\n",
    "\n",
    "First, you need to import the tokenizer and corresponding model through below command.\n",
    "\n",
    "It is preferred to use T5ForConditionalGeneration model when the input and output are both sequences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12209,
     "status": "ok",
     "timestamp": 1617427185961,
     "user": {
      "displayName": "Rimanshu Mangal",
      "photoUrl": "",
      "userId": "01816973904928346155"
     },
     "user_tz": -330
    },
    "id": "JdCQENeyiCNk",
    "outputId": "e99d18b4-14c4-40da-9562-61b5178a7a5f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ed/d5/f4157a376b8a79489a76ce6cfe147f4f3be1e029b7144fa7b8432e8acb26/transformers-4.4.2-py3-none-any.whl (2.0MB)\n",
      "\u001b[K     |████████████████████████████████| 2.0MB 7.6MB/s \n",
      "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from transformers) (3.8.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.41.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
      "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from transformers) (20.9)\n",
      "Collecting sacremoses\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
      "\u001b[K     |████████████████████████████████| 890kB 34.6MB/s \n",
      "\u001b[?25hCollecting tokenizers<0.11,>=0.10.1\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/71/23/2ddc317b2121117bf34dd00f5b0de194158f2a44ee2bf5e47c7166878a97/tokenizers-0.10.1-cp37-cp37m-manylinux2010_x86_64.whl (3.2MB)\n",
      "\u001b[K     |████████████████████████████████| 3.2MB 53.5MB/s \n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->transformers) (3.4.1)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->transformers) (2.4.7)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n",
      "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
      "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
      "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2020.12.5)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
      "Building wheels for collected packages: sacremoses\n",
      "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp37-none-any.whl size=893262 sha256=76cd6708256044775495cd08406fd3a863eb30ac5fbc89b98751f20751d8c059\n",
      "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
      "Successfully built sacremoses\n",
      "Installing collected packages: sacremoses, tokenizers, transformers\n",
      "Successfully installed sacremoses-0.0.43 tokenizers-0.10.1 transformers-4.4.2\n",
      "Collecting SentencePiece\n",
      "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f5/99/e0808cb947ba10f575839c43e8fafc9cc44e4a7a2c8f79c60db48220a577/sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2MB)\n",
      "\u001b[K     |████████████████████████████████| 1.2MB 8.6MB/s \n",
      "\u001b[?25hInstalling collected packages: SentencePiece\n",
      "Successfully installed SentencePiece-0.1.95\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers\n",
    "# !pip install SentencePiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6379,
     "status": "ok",
     "timestamp": 1617427374994,
     "user": {
      "displayName": "Rimanshu Mangal",
      "photoUrl": "",
      "userId": "01816973904928346155"
     },
     "user_tz": -330
    },
    "id": "KJJ9ObMuXNdI",
    "outputId": "c872afe7-5ffb-43ee-87d2-614cde06bd8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad> junk food is the source of constipation and other diseases. it is rich in saturated\n"
     ]
    }
   ],
   "source": [
    "# !pip install transformers\n",
    "# !pip install SentencePiece\n",
    "\n",
    "# Importing requirements\n",
    "from transformers import T5Tokenizer, T5Config, T5ForConditionalGeneration\n",
    "\n",
    "# Instantiating the model and tokenizer \n",
    "my_model = T5ForConditionalGeneration.from_pretrained('t5-small')\n",
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')\n",
    "# print(tokenizer)\n",
    "# Concatenating the word \"summarize:\" to raw text\n",
    "text = \"summarize:\" + original_text\n",
    "\n",
    "# encoding the input text\n",
    "input_ids=tokenizer.encode(text, return_tensors='pt', max_length=512, truncation=True)\n",
    "\n",
    "# The syntax will be: transformers.PreTrainedModel.generate (input_ids=None, max_length=None, min_length=None, num_beams=None)\n",
    "\n",
    "# Generating summary ids\n",
    "summary_ids = my_model.generate(input_ids)\n",
    "\n",
    "# Decoding the tensor and printing the summary.\n",
    "t5_summary = tokenizer.decode(summary_ids[0])\n",
    "print(t5_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2xToZwD6L4iT"
   },
   "source": [
    "#### Summarization with BART Transformers\n",
    "transformers library of HuggingFace supports summarization with BART models.\n",
    "\n",
    "Import the model and tokenizer. For problems where there is need to generate sequences , it is preferred to use BartForConditionalGeneration model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3nEsBoFtQ4Ad"
   },
   "outputs": [],
   "source": [
    "test_text = \"Indian cricket players T Natarajan and Shardul Thakur recently took delivery of their respective new Mahindra Thar SUVs. The off-roader was gifted to them and four other crickets by Anand Mahindra, Chairman - Mahindra & Mahindra, as a goodwill gesture for their incredible performance in the India-Australia test tour earlier this year. The test series was India's first win in Australia since 1988 with the young Indian cricket team taking a 2-1 series victory. Apart from Natarajan and Thakur, the other players that will receive the Thar SUV include Mohammad Siraj, Washington Sundar, Shubman Gill and Navdeep Saini.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 33853,
     "status": "ok",
     "timestamp": 1617428838039,
     "user": {
      "displayName": "Rimanshu Mangal",
      "photoUrl": "",
      "userId": "01816973904928346155"
     },
     "user_tz": -330
    },
    "id": "qoVLgM-cXZ3u",
    "outputId": "fa423a5a-5f9a-4f7b-8dab-e21479b8e54b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indian cricket players T Natarajan and Shardul Thakur recently took delivery of their respective new Mahindra Thar SUVs. The off-roader was gifted to them and four other crickets as a goodwill gesture. The test series was India's first win in Australia since 1988 with the young Indian cricket team taking a 2-1 series victory.\n"
     ]
    }
   ],
   "source": [
    "# Importing the model\n",
    "from transformers import BartForConditionalGeneration, BartTokenizer, BartConfig\n",
    "\n",
    "# Loading the model and tokenizer for bart-large-cnn\n",
    "\n",
    "tokenizer=BartTokenizer.from_pretrained('facebook/bart-large-cnn')\n",
    "model=BartForConditionalGeneration.from_pretrained('facebook/bart-large-cnn')\n",
    "\n",
    "# Encoding the inputs and passing them to model.generate()\n",
    "inputs = tokenizer.batch_encode_plus([test_text],return_tensors='pt')\n",
    "summary_ids = model.generate(inputs['input_ids'], early_stopping=True)\n",
    "\n",
    "# Decoding and printing the summary\n",
    "bart_summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "print(bart_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NG5gNdheM7ZV"
   },
   "source": [
    "#### Summarization with GPT-2 Transformers\n",
    "GPT-2 transformer is another major player in text summarization, introduced by OpenAI. Thanks to transformers, the process followed is same just like with BART Transformers.\n",
    "\n",
    "First, you have to import the tokenizer and model. Make sure that you import a LM Head type model, as it is necessary to generate sequences. Next, load the pretrained gpt-2 model and tokenizer ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 6958,
     "status": "ok",
     "timestamp": 1617428936933,
     "user": {
      "displayName": "Rimanshu Mangal",
      "photoUrl": "",
      "userId": "01816973904928346155"
     },
     "user_tz": -330
    },
    "id": "No74-EjDXNgi",
    "outputId": "4d2ffdd7-03b1-4124-8510-2a5fcdaeeacf"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "Input length of input_ids is 100, but ``max_length`` is set to 20.This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Indian cricket players T Natarajan and Shardul Thakur recently took delivery of their respective new Mahindra Thar SUVs. The off-roader was gifted to them and four other crickets by Anand Mahindra, Chairman - Mahindra & Mahindra, as a goodwill gesture for their incredible performance in the India-Australia test tour earlier this year. The test series was India's first win in Australia since 1988 with the young Indian cricket team taking a 2\n"
     ]
    }
   ],
   "source": [
    "# Importing model and tokenizer\n",
    "from transformers import GPT2Tokenizer,GPT2LMHeadModel\n",
    "\n",
    "# Instantiating the model and tokenizer with gpt-2\n",
    "tokenizer=GPT2Tokenizer.from_pretrained('gpt2')\n",
    "model=GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Encoding text to get input ids & pass them to model.generate()\n",
    "inputs=tokenizer.batch_encode_plus([test_text],return_tensors='pt', max_length=100, truncation=True)\n",
    "summary_ids=model.generate(inputs['input_ids'],early_stopping=True)\n",
    "\n",
    "# Decoding and printing summary\n",
    "\n",
    "GPT_summary=tokenizer.decode(summary_ids[0],skip_special_tokens=True)\n",
    "print(GPT_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "idVgLHiJR8G1"
   },
   "source": [
    "# IN this case BERT performs well for Cricket text from news."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zNZ70y-hOf51"
   },
   "source": [
    "# Most transformers are unfortunately completely constrained, which is the case for BERT (512 tokens max).\n",
    "\n",
    "# If you want to use transformers without being limited to a sequence length, you should take a look at Transformer-XL or XLNet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vjar5LvrNazx"
   },
   "source": [
    "#### Summarization with XLM Transformers\n",
    "Another transformer type that could be used for summarization are XLM Transformers.\n",
    "\n",
    "You can import the XLMWithLMHeadModel as it supports generation of sequences.You can load the pretrained xlm-mlm-en-2048 model and tokenizer with weights using from_pretrained() method.\n",
    "\n",
    "The nexts steps are same as the last three cases. The encoded input text is passed to generate() function with returns id sequence for the summary. You can decode and print the summary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 65596,
     "status": "ok",
     "timestamp": 1617428414594,
     "user": {
      "displayName": "Rimanshu Mangal",
      "photoUrl": "",
      "userId": "01816973904928346155"
     },
     "user_tz": -330
    },
    "id": "ZYMu2IN1NbGJ",
    "outputId": "c5a8b17c-c52f-4361-d04d-aa6d4198af1e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of XLMWithLMHeadModel were not initialized from the model checkpoint at xlm-mlm-en-2048 and are newly initialized: ['transformer.position_ids']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Input length of input_ids is 100, but ``max_length`` is set to 20.This can lead to unexpected behavior. You should consider increasing ``config.max_length`` or ``max_length``.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "junk foods taste good that's why it is mostly liked by everyone of any age group especially kids and school going children. they generally ask for the junk food daily because they have been trend so by their parents from the childhood. they never have been discussed by their parents about the harmful effects of junk foods over health. according to the research by scientists, it has been found that junk foods have negative effects on the health in many ways. they are generally fried food found in the market\n"
     ]
    }
   ],
   "source": [
    "# Importing model and tokenizer\n",
    "from transformers import XLMWithLMHeadModel, XLMTokenizer\n",
    "\n",
    "# Instantiating the model and tokenizer \n",
    "tokenizer=XLMTokenizer.from_pretrained('xlm-mlm-en-2048')\n",
    "model=XLMWithLMHeadModel.from_pretrained('xlm-mlm-en-2048')\n",
    "\n",
    "# Encoding text to get input ids & pass them to model.generate()\n",
    "inputs=tokenizer.batch_encode_plus([original_text],return_tensors='pt',max_length=100, truncation=True)\n",
    "summary_ids=model.generate(inputs['input_ids'],early_stopping=True)\n",
    "\n",
    "# Decode and print the summary\n",
    "XLM_summary=tokenizer.decode(summary_ids[0],skip_special_tokens=True)\n",
    "print(XLM_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "52Ep_0j4OvkS"
   },
   "source": [
    "# You can notice that the XLM_summary isn’t very good. It is because , even though it supports summaization , the model was not finetuned for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kbJgyxz7QSp9"
   },
   "source": [
    "# Till Now BERT performs best for summary generation."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMmEQpVyuwL6R1B8VJkarnn",
   "collapsed_sections": [],
   "name": "Text_Summarization.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
