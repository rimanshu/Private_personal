{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras for deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras is high level library which use very less line of code. Theono is low level library.\n",
    "\n",
    "https://medium.com/coinmonks/deep-learning-with-python-15e4fa47c609"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Type of activation functions :-\n",
    "\n",
    "Sigmoid Function - Value Between (0-1) - Exponantial increase.\n",
    "Threshold Function(Step Function): 0 upto certain value and after threshold it will be 1. Gratual increase.\n",
    "ReLu Function : y=max(x,0) - If the value of less than 0 then value is 0 or if greather than 0 then it will be same as input.\n",
    "Leaky Relu fucntion : max((.01x),x) - If the value of less than 0 then value is 0.01 of x or if greather than 0 then it will be same as input\n",
    "PRelu : Parametric Relu which is max(alpha*x, x)\n",
    "Swish Relu (Self Gated Function) : Formula is max(sigmoid(x),x) .... mostly used in LSTM or when more than 40 neural network is used.\n",
    "Hyperbolic Tangent Function(tanh)= value between (-1to1) = 1+e(-2x)/1+e(-2x)\n",
    "\n",
    "Softplus : ln(1+exp(x)) -- ln is natural log.\n",
    "Softmax : It is used for multiclass classification. sigmoid used for binary classification. exp(x)/sum(exp(x)).\n",
    "x is weight*input + bais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model and add layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Dense(10,input_shape=(2,),activation='relu')) # dense 10 means 10 hidden neurons or layers and input 2 is 2 input.\n",
    "model.add(Dense(1,)) # here dense 1 is output has 1 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add two Dense layers with 50 neurons and relu activation ---- Two hidden layers will be there with 50 neurons each\n",
    "model.add(Dense(50,activation=\"relu\"))\n",
    "model.add(Dense(50,activation=\"relu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smmary your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile Your model before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='mse') # for regression due to loss\n",
    "# Compile your model\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy']) # for classification due to loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,y_train,epochs=5)\n",
    "# Train for 100 epochs using a validation split of 0.2\n",
    "model.fit(sensors_train, parcels_train, validation_split = 2, epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of new set of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=model.predict(X_test)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evalute your results\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate your model accuracy on the test data\n",
    "accuracy = model.evaluate(coord_test,competitors_test)[1] # X_test and y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataset\n",
    "df=pd.read_csv('data.csv')\n",
    "#Turn Response Variable into labeled codes\n",
    "df.response =pd.Categorical(df.response)\n",
    "df.response=df.response.cat.codes\n",
    "\n",
    "# Transform into a categorical variable\n",
    "darts.competitor = pd.Categorical(darts.competitor)\n",
    "\n",
    "# Assign a number to each category (label encoding)\n",
    "darts.competitor = darts.competitor.cat.codes \n",
    "\n",
    "# Use to_categorical on your labels\n",
    "coordinates = darts.drop(['competitor'], axis=1)\n",
    "competitors = to_categorical(darts.competitor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a sequential model\n",
    "model = Sequential()\n",
    "  \n",
    "# Add 3 dense layers of 128, 64 and 32 neurons each\n",
    "model.add(Dense(128, input_shape=(2,), activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "  \n",
    "# Add a dense layer with as many neurons as competitors\n",
    "model.add(Dense(4, activation='softmax')) -- #Softmax is for multiclass\n",
    "  \n",
    "# Compile your model using categorical_crossentropy loss\n",
    "model.compile(loss='categorical_crossentropy', # categorical_crossentropy is for multiclass classification\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilabel Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a hidden layer of 64 neurons and a 20 neuron's input\n",
    "model.add(Dense(64,input_shape=(20,),activation='relu'))\n",
    "\n",
    "# Add an output layer of 3 neurons with sigmoid activation\n",
    "model.add(Dense(3,activation='sigmoid')) # for multlable\n",
    "\n",
    "# Compile your model with adam and binary crossentropy loss\n",
    "model.compile(optimizer='adam',\n",
    "           loss='binary_crossentropy', # for multlable \n",
    "           metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Train for 100 epochs using a validation split of 0.2\n",
    "model.fit(sensors_train, parcels_train, validation_split = 2, epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The history callback is returned by default every time you train a model with the .fit() method. To access these metrics you can access the history dictionary inside the returned callback object and the corresponding keys.\n",
    "The irrigation machine model you built in the previous lesson is loaded for you to train, along with its features and labels (X and y). This time you will store the model's historycallback and use the validation_data parameter as it trains.\n",
    "You will plot the results stored in history with plot_accuracy() and plot_loss(), two simple matplotlib functions. You can check their code in the console by typing print(inspect.getsource(plot_loss)). \n",
    "Let's see the behind the scenes of our training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your model and save it's history\n",
    "history = model.fit(X_train, y_train, epochs=50,\n",
    "               validation_data=(X_test, y_test))\n",
    "\n",
    "# Plot train vs test loss during training\n",
    "plot_loss(history.history['loss'], history.history['val_loss'])\n",
    "\n",
    "# Plot train vs test accuracy during training\n",
    "plot_accuracy(history.history['acc'], history.history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping # it is used to prevent overfitting problem\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "#The early stopping callback is useful since it allows for you to stop the model training if it no longer improves\n",
    "#after a given number of epochs. To make use of this functionality you need to pass the callback inside a list to the model's \n",
    "#callback parameter in the .fit() method.\n",
    "\n",
    "early_stopping=EarlyStopping(monitor='val_loss',patience=5)\n",
    "\n",
    "model.fit(X_train,y_train,epochs=100,validation_data=(X_test,y_test),callbacks=[early_stopping])\n",
    "\n",
    "model_sav=ModelCheckpoint('best_model.hdf5',save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the early stopping callback\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define a callback to monitor val_acc\n",
    "monitor_val_acc = EarlyStopping(monitor='val_acc',\n",
    "                       patience=5)\n",
    "\n",
    "# Train your model using the early stopping callback\n",
    "model.fit(X_train, y_train, \n",
    "           epochs=1000, validation_data=(X_test,y_test),\n",
    "           callbacks=[monitor_val_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the EarlyStopping and ModelCheckpoint callbacks\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Early stop on validation accuracy\n",
    "monitor_val_acc = EarlyStopping(monitor='val_acc', patience=3)\n",
    "\n",
    "# Save the best model as best_banknote_model.hdf5\n",
    "modelCheckpoint = ModelCheckpoint('best_banknote_model.hdf5', save_best_only=True)\n",
    "\n",
    "# Fit your model for a stupid amount of epochs\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=10000000,\n",
    "                    callbacks=[monitor_val_acc,modelCheckpoint],\n",
    "                    validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ini_weights=model.get_weights()\n",
    "\n",
    "#List for storing accuracies\n",
    "train_acc=[]\n",
    "test_acc=[]\n",
    "\n",
    "for train_size in train_sizes:\n",
    "    #Splitting a fraction according to train_size\n",
    "    X_train_frac,_,y_train_frac,_=train_test_split(Xtrain,y_train,train_size=train_size)\n",
    "    \n",
    "    #Set Model Initial Weigthts\n",
    "    model.set_weights(initial_weights)\n",
    "    \n",
    "    #Fit our model on training set fraction\n",
    "    model.fit(X_train_frac,y_train_frac,epochs=100,verbose=0,callbacks=[EarlyStopping(monitor='loss',patience=1)])\n",
    "    \n",
    "    #Get the accuracy for this training set function\n",
    "    train_acc=model.evaluate(X_train_frac,y_train_frac,verbose=0)[1]\n",
    "    train_acc.append(train_acc)\n",
    "    test_accs.append(model.evaluate(X_test, y_test, verbose=0)[1])\n",
    "    \n",
    "# Plot train_accs vs test_accs\n",
    "plot_results(train_accs, test_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your model for 60 epochs, using X_test and y_test as validation data\n",
    "history = model.fit(X_train, y_train, epochs=60, validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "# Extract from the history object loss and val_loss to plot the learning curve\n",
    "plot_loss(history.history['loss'], history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed\n",
    "np.random.seed(27)\n",
    "\n",
    "# Activation functions to try\n",
    "activations = ['relu', 'leaky_relu', 'sigmoid', 'tanh']\n",
    "\n",
    "# Loop over the activation functions\n",
    "activation_results = {}\n",
    "\n",
    "for act in activations:\n",
    "  # Get a new model with the current activation\n",
    "  model = get_model(act)\n",
    "  # Fit the model\n",
    "  history = model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=20, verbose=0)\n",
    "  activation_results[act] = history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a fresh new model with get_model\n",
    "model = get_model()\n",
    "\n",
    "# Train your model for 5 epochs with a batch size of 1\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=1)\n",
    "print(\"\\n The accuracy when using a batch of size 1 is: \",\n",
    "model.evaluate(X_test, y_test)[1]) # Accuracy is 99%\n",
    "\n",
    "#######################\n",
    "model = get_model()\n",
    "\n",
    "# Fit your model for 5 epochs with a batch of size the training set\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=X_train.shape[0])\n",
    "print(\"\\n The accuracy when using the whole training set as a batch was: \",\n",
    "model.evaluate(X_test, y_test)[1]) # Accuracy is 47% because we have increased teh batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import batch normalization from keras layers\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "# Build your deep network\n",
    "batchnorm_model = Sequential()\n",
    "batchnorm_model.add(Dense(50, input_shape=(64,), activation='relu', kernel_initializer='normal'))\n",
    "batchnorm_model.add(BatchNormalization())\n",
    "batchnorm_model.add(Dense(50, activation='relu', kernel_initializer='normal'))\n",
    "batchnorm_model.add(BatchNormalization())\n",
    "batchnorm_model.add(Dense(50, activation='relu', kernel_initializer='normal'))\n",
    "batchnorm_model.add(BatchNormalization())\n",
    "batchnorm_model.add(Dense(10, activation='softmax', kernel_initializer='normal'))\n",
    "\n",
    "# Compile your model with sgd\n",
    "batchnorm_model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
