{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep learning Concepts "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras is high level library which use very less line of code. Theono is low level library.\n",
    "\n",
    "https://medium.com/coinmonks/deep-learning-with-python-15e4fa47c609"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "W = weight     \n",
    "X = inputfeature      \n",
    "Y = hidden layer output (assume 1 hidden layer)      \n",
    "Z = final output    \n",
    "\n",
    "Y = W1X1 + W2X2 + W3X3 + Bais ---- Step1     \n",
    "Z = act(y) --- Step 2 like relu activation function to get output from hidden layer.      \n",
    "O = Z*W4 ----- Step3 like step 1        \n",
    "O1 = act(O) --- Step 4 like sigmoid activation function          \n",
    "\n",
    "At the ouput O1 is the predicted output so lets say pred value and input or orignal value is actual value\n",
    "\n",
    "loss = (actual value - pred value) --- ultimate goal is to optimize this loss by optimizer like gredient decent or other\n",
    "\n",
    "Loss function is for one record and multiple record will have cost function which is summition of loss function.\n",
    "\n",
    "Optimization of loss function by adjusting or updating the weight from back to first through back propogation.\n",
    "\n",
    "Hows the weights been updated ?? ---\n",
    "\n",
    "let suppose W4 was last weight and it should be updated by\n",
    "\n",
    "W4new = W4 - n*(dL/dW4) --- n is learning rate and it multiply by derative of loss by W4 weight and whole value sub from weight\n",
    "\n",
    "Above derative can be written as ----- dL/dW4 = (dL/dO1)*(dO1/dW4) ----- chain rule\n",
    "\n",
    "Finally == dO21/dO11 * dO11/dW11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "vanishing Gredient decent problem -----\n",
    "\n",
    "Vanishing gredient decent problem due to sigmoid function. In past(1990) we used sigmoid function in hidden layer instead of relu.\n",
    "\n",
    "Reasercher was not able to create ANN because Relu was not invented and Sigmoid will have vanishing gredient problem.\n",
    "\n",
    "When we try to optimize the loss function then due to chain rule we multiply derivative of hidden layer output with loss and weight.\n",
    "\n",
    "Derivative of output means we try to find derivative of sigmoid function coz we used sigmoid activation fucntion in hidden layer.\n",
    "\n",
    "So MOST important is ----- Derivative of sigmoid fucntion is ranging between 0 to .25.\n",
    "\n",
    "As the number of layer increases the derivative becomes very less and dO21/dO11 * dO11/dW11 tends to zero.\n",
    "\n",
    "so after few epochs weights will not be updated and derivative becomes vanishes.\n",
    "\n",
    "W4new = W4 - n*(dO21/dO11 * dO11/dW11)\n",
    "\n",
    "W4new = W4 - o.oooooooooooooo7\n",
    "W4new = w4\n",
    "\n",
    "Exploding Gredient Problem -----\n",
    "\n",
    "Exploding problem is due to higher weights and varrying weights while optimization. so it will never come to global minima i.e 0\n",
    "\n",
    "dO21/dO11 = .25 * W21 -- final formula\n",
    "\n",
    "\n",
    "RELU activation function comes into picture to remove vanishing gredient problem.\n",
    "\n",
    "Relu function gives the max(0,x) so its derivative would be always 1 for all possitive number and 0 for all negative number.\n",
    "\n",
    "coz relu gives 45 degree line for all possitive value so tan45 would be 1 and all negative value relu gives contants so\n",
    "its derivative is 0.\n",
    "\n",
    "so derivative of relu fucntion is either 0 or 1. Derivative of tanh is -1 which is threshold function.\n",
    "\n",
    "It will create big problem when relu derivative gives 0 value so that Wnew is equal to Wold. This will create dead neuron or\n",
    "dead activation function. So leaky relu fucntion comes into picture."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Drop Out And Regularisation -----\n",
    "\n",
    "Mostly neural network overfits the model coz it has multiple hidden layer. single layer nueral network underfits the model.\n",
    "\n",
    "In Machine Lerning, DT overfit the model so RF comes into picture which have feature sampling and row sampling. Mean create\n",
    "multiple decision tree with some features. Same we need do in Neural network also to use some features randomly.\n",
    "\n",
    "Drop out means it drop out or deactivate some input neurons from input layer and some neurons from hidden and some from output.\n",
    "\n",
    "It have Drop out ratio that is p value which ranges from 0 to 1. Its just percentage of number of features or neurons.\n",
    "\n",
    "We are using drop out ratio instead of regulirazation L1 and L2.\n",
    "\n",
    "So MOST IMP que is we can do th drop out in traning data what about test data ?\n",
    "\n",
    "So, in test data we have to multiply drop out ratio p to every weight. i.e w*p.\n",
    "\n",
    "How do we select p value by hyperparameter or in general for deep neural network p value would be high that is more than .5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weight Initializaton --\n",
    "fan_in = number of input      \n",
    "fan_out = number of output       \n",
    "\n",
    "Weights should be have good varaince and should not be the same. Weight should have small value but not very small.\n",
    "\n",
    "Techniques to initiliaze weights -     \n",
    "1) Uniform distribution - Value should be taken in range (-1/number of inputs , 1/number of inputs)      \n",
    "2) He init - Two types one is he uniform U(-sqr(6/fan_in), sqrt(6/fan_in))       \n",
    "and second one is he normal N(0,var) here 0 is mean and var is varience. Var = sqrt(2/fan_in)        \n",
    "3) Xavier/Gorat :--- 1) Xavier Normal -- N(0,var) where var = sqrt(2/fan_in + fan_out)          \n",
    "2) Xavier uniform -- U(-sqr(6/fan_in), sqrt(6/fan_out)          "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Type of activation functions :-\n",
    "\n",
    "Sigmoid Function - Value Between (0-1) - Exponantial increase.       \n",
    "Threshold Function(Step Function): 0 upto certain value and after threshold it will be 1. Gratual increase.        \n",
    "ReLu Function : y=max(x,0) - If the value of less than 0 then value is 0 or if greather than 0 then it will be same as input.    \n",
    "Leaky Relu fucntion : max((.01x),x) - If the value of less than 0 then value is 0.01 of x or if greather than 0 then it will be same as input          \n",
    "PRelu : Parametric Relu which is max(alpha*x, x)             \n",
    "Swish Relu (Self Gated Function) : Formula is max(sigmoid(x),x) .... mostly used in LSTM or when more than 40 neural network is used.            \n",
    "Hyperbolic Tangent Function(tanh)= value between (-1to1) = 1+e(-2x)/1+e(-2x)               \n",
    "\n",
    "Softplus : ln(1+exp(x)) -- ln is natural log.                  \n",
    "Softmax : It is used for multiclass classification. sigmoid used for binary classification. exp(x)/sum(exp(x)).       \n",
    "x is weight*input + bais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
