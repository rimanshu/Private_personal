{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras for deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keras is high level library which use very less line of code. Theono is low level library.\n",
    "\n",
    "https://medium.com/coinmonks/deep-learning-with-python-15e4fa47c609"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = weight\n",
    "X = inputfeature\n",
    "Y = hidden layer output (assume 1 hidden layer)\n",
    "Z = final output\n",
    "\n",
    "Y = W1X1 + W2X2 + W3X3 + Bais ---- Step1\n",
    "Z = act(y) --- Step 2 like relu activation function to get output from hidden layer.\n",
    "O = Z*W4 ----- Step3 like step 1\n",
    "O1 = act(O) --- Step 4 like sigmoid activation function\n",
    "\n",
    "At the ouput O1 is the predicted output so lets say pred value and input or orignal value is actual value\n",
    "\n",
    "loss = (actual value - pred value) --- ultimate goal is to optimize this loss by optimizer like gredient decent or other\n",
    "\n",
    "Loss function is for one record and multiple record will have cost function which is summition of loss function.\n",
    "\n",
    "Optimization of loss function by adjusting or updating the weight from back to first through back propogation.\n",
    "\n",
    "Hows the weights been updated ?? ---\n",
    "\n",
    "let suppose W4 was last weight and it should be updated by\n",
    "\n",
    "W4new = W4 - n*(dL/dW4) --- n is learning rate and it multiply by derative of loss by W4 weight and whole value sub from weight\n",
    "\n",
    "Above derative can be written as ----- dL/dW4 = (dL/dO1)*(dO1/dW4) ----- chain rule\n",
    "\n",
    "Finally == dO21/dO11 * dO11/dW11"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vanishing Gredient decent problem -----\n",
    "\n",
    "Vanishing gredient decent problem due to sigmoid function. In past(1990) we used sigmoid function in hidden layer instead of relu.\n",
    "\n",
    "Reasercher was not able to create ANN because Relu was not invented and Sigmoid will have vanishing gredient problem.\n",
    "\n",
    "When we try to optimize the loss function then due to chain rule we multiply derivative of hidden layer output with loss and weight.\n",
    "\n",
    "Derivative of output means we try to find derivative of sigmoid function coz we used sigmoid activation fucntion in hidden layer.\n",
    "\n",
    "So MOST important is ----- Derivative of sigmoid fucntion is ranging between 0 to .25.\n",
    "\n",
    "As the number of layer increases the derivative becomes very less and dO21/dO11 * dO11/dW11 tends to zero.\n",
    "\n",
    "so after few epochs weights will not be updated and derivative becomes vanishes.\n",
    "\n",
    "W4new = W4 - n*(dO21/dO11 * dO11/dW11)\n",
    "\n",
    "W4new = W4 - o.oooooooooooooo7\n",
    "W4new = w4\n",
    "\n",
    "Exploding Gredient Problem -----\n",
    "\n",
    "Exploding problem is due to higher weights and varrying weights while optimization. so it will never come to global minima i.e 0\n",
    "\n",
    "dO21/dO11 = .25 * W21 -- final formula\n",
    "\n",
    "\n",
    "RELU activation function comes into picture to remove vanishing gredient problem.\n",
    "\n",
    "Relu function gives the max(0,x) so its derivative would be always 1 for all possitive number and 0 for all negative number.\n",
    "\n",
    "coz relu gives 45 degree line for all possitive value so tan45 would be 1 and all negative value relu gives contants so\n",
    "its derivative is 0.\n",
    "\n",
    "so derivative of relu fucntion is either 0 or 1. Derivative of tanh is -1 which is threshold function.\n",
    "\n",
    "It will create big problem when relu derivative gives 0 value so that Wnew is equal to Wold. This will create dead neuron or\n",
    "dead activation function. So leaky relu fucntion comes into picture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Drop Out And Regularisation -----\n",
    "\n",
    "Mostly neural network overfits the model coz it has multiple hidden layer. single layer nueral network underfits the model.\n",
    "\n",
    "In Machine Lerning, DT overfit the model so RF comes into picture which have feature sampling and row sampling. Mean create\n",
    "multiple decision tree with some features. Same we need do in Neural network also to use some features randomly.\n",
    "\n",
    "Drop out means it drop out or deactivate some input neurons from input layer and some neurons from hidden and some from output.\n",
    "\n",
    "It have Drop out ratio that is p value which ranges from 0 to 1. Its just percentage of number of features or neurons.\n",
    "\n",
    "We are using drop out ratio instead of regulirazation L1 and L2.\n",
    "\n",
    "So MOST IMP que is we can do th drop out in traning data what about test data ?\n",
    "\n",
    "So, in test data we have to multiply drop out ratio p to every weight. i.e w*p.\n",
    "\n",
    "How do we select p value by hyperparameter or in general for deep neural network p value would be high that is more than .5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Weight Initializaton --\n",
    "fan_in = number of input\n",
    "fan_out = number of output\n",
    "\n",
    "Weights should be have good varaince and should not be the same. Weight should have small value but not very small.\n",
    "\n",
    "Techniques to initiliaze weights -\n",
    "1) Uniform distribution - Value should be taken in range (-1/number of inputs , 1/number of inputs)\n",
    "2) He init - Two types one is he uniform U(-sqr(6/fan_in), sqrt(6/fan_in))\n",
    "and second one is he normal N(0,var) here 0 is mean and var is varience. Var = sqrt(2/fan_in)\n",
    "3) Xavier/Gorat :--- 1) Xavier Normal -- N(0,var) where var = sqrt(2/fan_in + fan_out)\n",
    "2) Xavier uniform -- U(-sqr(6/fan_in), sqrt(6/fan_out)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Type of activation functions :-\n",
    "\n",
    "Sigmoid Function - Value Between (0-1) - Exponantial increase.\n",
    "Threshold Function(Step Function): 0 upto certain value and after threshold it will be 1. Gratual increase.\n",
    "ReLu Function : y=max(x,0) - If the value of less than 0 then value is 0 or if greather than 0 then it will be same as input.\n",
    "Leaky Relu fucntion : max((.01x),x) - If the value of less than 0 then value is 0.01 of x or if greather than 0 then it will be same as input\n",
    "PRelu : Parametric Relu which is max(alpha*x, x)\n",
    "Swish Relu (Self Gated Function) : Formula is max(sigmoid(x),x) .... mostly used in LSTM or when more than 40 neural network is used.\n",
    "Hyperbolic Tangent Function(tanh)= value between (-1to1) = 1+e(-2x)/1+e(-2x)\n",
    "\n",
    "Softplus : ln(1+exp(x)) -- ln is natural log.\n",
    "Softmax : It is used for multiclass classification. sigmoid used for binary classification. exp(x)/sum(exp(x)).\n",
    "x is weight*input + bais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dense\n",
    "from keras.models import Sequential\n",
    "import warnings \n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define model and add layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "model.add(Dense(10,input_shape=(2,),activation='relu')) # dense 10 means 10 hidden neurons or layers and input 2 is 2 input.\n",
    "model.add(Dense(1,)) # here dense 1 is output has 1 neurons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add two Dense layers with 50 neurons and relu activation ---- Two hidden layers will be there with 50 neurons each\n",
    "model.add(Dense(50,activation=\"relu\"))\n",
    "model.add(Dense(50,activation=\"relu\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Smmary your model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile Your model before training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam',loss='mse')\n",
    "# Compile your model\n",
    "model.compile(loss='binary_crossentropy', optimizer='sgd', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train,y_train,epochs=5)\n",
    "# Train for 100 epochs using a validation split of 0.2\n",
    "model.fit(sensors_train, parcels_train, validation_split = 2, epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prediction of new set of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred=model.predict(X_test)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evalute your results\n",
    "model.evaluate(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate your model accuracy on the test data\n",
    "accuracy = model.evaluate(coord_test,competitors_test)[1] # X_test and y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multiclass Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from keras.utils import to_categorical"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataset\n",
    "df=pd.read_csv('data.csv')\n",
    "#Turn Response Variable into labeled codes\n",
    "df.response =pd.Categorical(df.response)\n",
    "df.response=df.response.cat.codes\n",
    "\n",
    "# Transform into a categorical variable\n",
    "darts.competitor = pd.Categorical(darts.competitor)\n",
    "\n",
    "# Assign a number to each category (label encoding)\n",
    "darts.competitor = darts.competitor.cat.codes \n",
    "\n",
    "# Use to_categorical on your labels\n",
    "coordinates = darts.drop(['competitor'], axis=1)\n",
    "competitors = to_categorical(darts.competitor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a sequential model\n",
    "model = Sequential()\n",
    "  \n",
    "# Add 3 dense layers of 128, 64 and 32 neurons each\n",
    "model.add(Dense(128, input_shape=(2,), activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "  \n",
    "# Add a dense layer with as many neurons as competitors\n",
    "model.add(Dense(4, activation='softmax')) -- #Softmax is for multiclass\n",
    "  \n",
    "# Compile your model using categorical_crossentropy loss\n",
    "model.compile(loss='categorical_crossentropy', # categorical_crossentropy is for multiclass classification\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilabel Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate a Sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Add a hidden layer of 64 neurons and a 20 neuron's input\n",
    "model.add(Dense(64,input_shape=(20,),activation='relu'))\n",
    "\n",
    "# Add an output layer of 3 neurons with sigmoid activation\n",
    "model.add(Dense(3,activation='sigmoid')) # for multlable\n",
    "\n",
    "# Compile your model with adam and binary crossentropy loss\n",
    "model.compile(optimizer='adam',\n",
    "           loss='binary_crossentropy', # for multlable \n",
    "           metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "# Train for 100 epochs using a validation split of 0.2\n",
    "model.fit(sensors_train, parcels_train, validation_split = 2, epochs = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keras callback"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The history callback is returned by default every time you train a model with the .fit() method. To access these metrics you can access the history dictionary inside the returned callback object and the corresponding keys.\n",
    "The irrigation machine model you built in the previous lesson is loaded for you to train, along with its features and labels (X and y). This time you will store the model's historycallback and use the validation_data parameter as it trains.\n",
    "You will plot the results stored in history with plot_accuracy() and plot_loss(), two simple matplotlib functions. You can check their code in the console by typing print(inspect.getsource(plot_loss)). \n",
    "Let's see the behind the scenes of our training!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your model and save it's history\n",
    "history = model.fit(X_train, y_train, epochs=50,\n",
    "               validation_data=(X_test, y_test))\n",
    "\n",
    "# Plot train vs test loss during training\n",
    "plot_loss(history.history['loss'], history.history['val_loss'])\n",
    "\n",
    "# Plot train vs test accuracy during training\n",
    "plot_accuracy(history.history['acc'], history.history['val_acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import EarlyStopping # it is used to prevent overfitting problem\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "\n",
    "#The early stopping callback is useful since it allows for you to stop the model training if it no longer improves\n",
    "#after a given number of epochs. To make use of this functionality you need to pass the callback inside a list to the model's \n",
    "#callback parameter in the .fit() method.\n",
    "\n",
    "early_stopping=EarlyStopping(monitor='val_loss',patience=5)\n",
    "\n",
    "model.fit(X_train,y_train,epochs=100,validation_data=(X_test,y_test),callbacks=[early_stopping])\n",
    "\n",
    "model_sav=ModelCheckpoint('best_model.hdf5',save_best_only=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the early stopping callback\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "# Define a callback to monitor val_acc\n",
    "monitor_val_acc = EarlyStopping(monitor='val_acc',\n",
    "                       patience=5)\n",
    "\n",
    "# Train your model using the early stopping callback\n",
    "model.fit(X_train, y_train, \n",
    "           epochs=1000, validation_data=(X_test,y_test),\n",
    "           callbacks=[monitor_val_acc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the EarlyStopping and ModelCheckpoint callbacks\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "# Early stop on validation accuracy\n",
    "monitor_val_acc = EarlyStopping(monitor='val_acc', patience=3)\n",
    "\n",
    "# Save the best model as best_banknote_model.hdf5\n",
    "modelCheckpoint = ModelCheckpoint('best_banknote_model.hdf5', save_best_only=True)\n",
    "\n",
    "# Fit your model for a stupid amount of epochs\n",
    "history = model.fit(X_train, y_train,\n",
    "                    epochs=10000000,\n",
    "                    callbacks=[monitor_val_acc,modelCheckpoint],\n",
    "                    validation_data=(X_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ini_weights=model.get_weights()\n",
    "\n",
    "#List for storing accuracies\n",
    "train_acc=[]\n",
    "test_acc=[]\n",
    "\n",
    "for train_size in train_sizes:\n",
    "    #Splitting a fraction according to train_size\n",
    "    X_train_frac,_,y_train_frac,_=train_test_split(Xtrain,y_train,train_size=train_size)\n",
    "    \n",
    "    #Set Model Initial Weigthts\n",
    "    model.set_weights(initial_weights)\n",
    "    \n",
    "    #Fit our model on training set fraction\n",
    "    model.fit(X_train_frac,y_train_frac,epochs=100,verbose=0,callbacks=[EarlyStopping(monitor='loss',patience=1)])\n",
    "    \n",
    "    #Get the accuracy for this training set function\n",
    "    train_acc=model.evaluate(X_train_frac,y_train_frac,verbose=0)[1]\n",
    "    train_acc.append(train_acc)\n",
    "    test_accs.append(model.evaluate(X_test, y_test, verbose=0)[1])\n",
    "    \n",
    "# Plot train_accs vs test_accs\n",
    "plot_results(train_accs, test_accs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train your model for 60 epochs, using X_test and y_test as validation data\n",
    "history = model.fit(X_train, y_train, epochs=60, validation_data=(X_test, y_test), verbose=0)\n",
    "\n",
    "# Extract from the history object loss and val_loss to plot the learning curve\n",
    "plot_loss(history.history['loss'], history.history['val_loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activation Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a seed\n",
    "np.random.seed(27)\n",
    "\n",
    "# Activation functions to try\n",
    "activations = ['relu', 'leaky_relu', 'sigmoid', 'tanh']\n",
    "\n",
    "# Loop over the activation functions\n",
    "activation_results = {}\n",
    "\n",
    "for act in activations:\n",
    "  # Get a new model with the current activation\n",
    "  model = get_model(act)\n",
    "  # Fit the model\n",
    "  history = model.fit(X_train, y_train, validation_data=(X_test,y_test), epochs=20, verbose=0)\n",
    "  activation_results[act] = history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a fresh new model with get_model\n",
    "model = get_model()\n",
    "\n",
    "# Train your model for 5 epochs with a batch size of 1\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=1)\n",
    "print(\"\\n The accuracy when using a batch of size 1 is: \",\n",
    "model.evaluate(X_test, y_test)[1]) # Accuracy is 99%\n",
    "\n",
    "#######################\n",
    "model = get_model()\n",
    "\n",
    "# Fit your model for 5 epochs with a batch of size the training set\n",
    "model.fit(X_train, y_train, epochs=5, batch_size=X_train.shape[0])\n",
    "print(\"\\n The accuracy when using the whole training set as a batch was: \",\n",
    "model.evaluate(X_test, y_test)[1]) # Accuracy is 47% because we have increased teh batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import batch normalization from keras layers\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "# Build your deep network\n",
    "batchnorm_model = Sequential()\n",
    "batchnorm_model.add(Dense(50, input_shape=(64,), activation='relu', kernel_initializer='normal'))\n",
    "batchnorm_model.add(BatchNormalization())\n",
    "batchnorm_model.add(Dense(50, activation='relu', kernel_initializer='normal'))\n",
    "batchnorm_model.add(BatchNormalization())\n",
    "batchnorm_model.add(Dense(50, activation='relu', kernel_initializer='normal'))\n",
    "batchnorm_model.add(BatchNormalization())\n",
    "batchnorm_model.add(Dense(10, activation='softmax', kernel_initializer='normal'))\n",
    "\n",
    "# Compile your model with sgd\n",
    "batchnorm_model.compile(optimizer='sgd', loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
