{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/decision-tree-algorithm-explained-83beb6e78ef4\n",
    "\n",
    "https://mlcourse.ai/articles/topic3-dt-knn/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Machine Learning is a application where system gets the ability to automatically learn and improve based on experiences.\n",
    "#Supervised Learning - If you already have data and awnsers like person will pay loan or not.\n",
    "#Unsupervised Learning - Grouped liked information together like images of tree together.\n",
    "#Reinforcement Learning - You dont have the data prior to the starting.\n",
    "\n",
    "#When data is small and less complex we will use the Naive baise and Logistic regression.\n",
    "#When data is bit complex then we will use decision tree but it has over fitting problem.\n",
    "#When data is large so we will use random forest because it uses esemble library to make many trees.\n",
    "\n",
    "#Decision tree is tree shaped daigram used to determine a course of action. Each branch of the tree represent a possible decision\n",
    "#occurence or reaction.\n",
    "\n",
    "#Decision tree can be used for classification and regression.When target variable is categorical then use classification tree\n",
    "#else we will use regression tree when target variable will be continious and discreat.\n",
    "\n",
    "#Disadvantages - Overfitting, High Varience, Low biase\n",
    "\n",
    "# How we select the ordering of feathers. Which one variable will be the top or parent.\n",
    "#It is based on ENTROPY. ENTOPY is measures of randomness.. Low entropy will be the parent one.\n",
    "#GINI IMPURITY -  Creteria of probability of misclassification.\n",
    "#Informarion gain - It is measure of decrease in entropy after the dataset is split. GAIN = E1 - E2\n",
    "\n",
    "#Machine Learning Algorithms works on numbers not lables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall the game of \"20 Questions\", which is often referenced when introducing decision trees. You've probably played this game -- one person thinks of a celebrity while the other tries to guess by asking only \"Yes\" or \"No\" questions. What question will the guesser ask first? Of course, they will ask the one that narrows down the number of the remaining options the most. Asking \"Is it Angelina Jolie?\" would, in the case of a negative response, leave all but one celebrity in the realm of possibility. In contrast, asking \"Is the celebrity a woman?\" would reduce the possibilities to roughly half. That is to say, the \"gender\" feature separates the celebrity dataset much better than other features like \"Angelina Jolie\", \"Spanish\", or \"loves football.\" This reasoning corresponds to the concept of information gain based on entropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you look closely, these are exactly the mean values between the ages at which the target class \"switches\" from 1 to 0 or 0 to 1.\n",
    "\n",
    "the simplest heuristics for handling numeric features in a decision tree is to sort its values in ascending order and check only those thresholds where the value of the target variable changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attribute Selection Measures for decision tree .\n",
    "\n",
    "Entropy,\n",
    "Information gain,\n",
    "Gini index,\n",
    "Gain Ratio,\n",
    "Reduction in Variance\n",
    "Chi-Square\n",
    "\n",
    "While using Information Gain as a criterion, we assume attributes(independent) to be categorical, and for the Gini index, attributes(independent) are assumed to be continuous.\n",
    "\n",
    "Entropy - \n",
    "\n",
    "'''Entropy is a measure of the randomness in the information being processed. The higher the entropy, the harder it is to draw any conclusions from that information. Flipping a coin is an example of an action that provides information that is random.\n",
    "\n",
    "A branch with an entropy of zero is a leaf node and A brach with entropy more than zero needs further splitting.\n",
    "\n",
    "Entropy = sum (-plog2(p)) -- where pi is probablity of an event.Example - yes =9 and no =5 then \n",
    "E(5,9) = -(5/14log2(5/14)) -(9/14log2(9/14))\n",
    "'''\n",
    "\n",
    "Information Gain - \n",
    "'''\n",
    "Information gain or IG is a statistical property that measures how well a given attribute separates the training examples according to their target classification. Constructing a decision tree is all about finding an attribute that returns the highest information gain and the smallest entropy.\n",
    "\n",
    "How much any attribute gives information that is information gain. the reduction in entropy is called information gain. that gives the greatest information gain upon splitting\n",
    "\n",
    "Information gain is a decrease in entropy. It computes the difference between entropy before split and average entropy after split of the dataset based on given attribute values.\n",
    "\n",
    "Information gain IG(T,X) = Entropy(T) - Entropy(T,X)  -- T means before and X means after.\n",
    "'''\n",
    "\n",
    "Gini Index -\n",
    "'''\n",
    "You can understand the Gini index as a cost function used to evaluate splits in the dataset. It is calculated by subtracting the sum of the squared probabilities of each class from one. It favors larger partitions and easy to implement whereas information gain favors smaller partitions with distinct values.\n",
    "\n",
    "Gini = 1 - p(Success)2 -p(failure)2 \n",
    "\n",
    "Gini Index works with the categorical target variable “Success” or “Failure”. It performs only Binary splits. It refers the probablity of misclassification\n",
    "\n",
    "Gain Ratio = Information gain / split ratio\n",
    "\n",
    "Information gain is biased towards choosing attributes with a large number of values as root nodes. It means it prefers the attribute with a large number of distinct values.\n",
    "Gain ratio which is a modification of Information gain that reduces its bias and is usually the best option. Gain ratio overcomes the problem with information gain by taking into account the number of branches that would result before making the split. It corrects information gain by taking the intrinsic information of a split into account.\n",
    "\n",
    "Gain Ratio = Information gain / split ratio\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reduction in variance -\n",
    "\n",
    "'''\n",
    "Reduction in variance is an algorithm used for continuous target variables (regression problems). This algorithm uses the \n",
    "standard formula of variance to choose the best split. The split with lower variance is selected as the criteria to split \n",
    "the population:\n",
    "\n",
    "variance = sum(X-X(mean))2/n\n",
    "\n",
    "'''\n",
    "\n",
    "Chi-Square -\n",
    "\n",
    "'''\n",
    "\n",
    "It is one of the oldest tree classification methods. It finds out the statistical significance between the differences\n",
    "between sub-nodes and parent node. We measure it by the sum of squares of standardized differences between observed and \n",
    "expected frequencies of the target variable.\n",
    "It works with the categorical target variable “Success” or “Failure”. It can perform two or more splits. Higher the value of\n",
    "Chi-Square higher the statistical significance of differences between sub-node and Parent node.\n",
    "\n",
    "Chi Square = (O-E)2/E -- O is observed value and E is expected value.\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('D:\\\\Work\\\\Python\\\\Python ML\\\\ML\\\\9_decision_tree\\\\salaries.csv')\n",
    "df.head()\n",
    "\n",
    "inputs=df.drop('salary_more_then_100k', axis='columns')\n",
    "target=df['salary_more_then_100k']\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.preprocessing import *\n",
    "\n",
    "Le_company=LabelEncoder()\n",
    "Le_job=LabelEncoder()\n",
    "Le_degree=LabelEncoder()\n",
    "\n",
    "inputs['company_n'] = Le_company.fit_transform(inputs['company'])\n",
    "inputs['job_n'] = Le_company.fit_transform(inputs['job'])\n",
    "inputs['degree_n'] = Le_company.fit_transform(inputs['degree'])\n",
    "\n",
    "inputs.head()\n",
    "\n",
    "inputs_n = inputs.drop(['company','job','degree'], axis='columns')\n",
    "inputs_n\n",
    "\n",
    "from sklearn import tree\n",
    "\n",
    "model =tree.DecisionTreeClassifier() # Not using traina dn test data coz we want to make ot simple for it now.\n",
    "model.fit(inputs_n,target) #criterion='gini' by default\n",
    "model.score(inputs_n,target) #1.0\n",
    "\n",
    "model.predict([[2,2,1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adjusting Your Algorithms\n",
    "#\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import *\n",
    "\n",
    "model =RandomForestClassifier(class_weight='balanced')\n",
    "#Weight option is also takes manual input like - class_weight={0:1,1:4}\n",
    "\n",
    "model =RandomForestClassifier(class_weight='balanced_subsample')\n",
    "\n",
    "model =LogisticRegression(class_weight='balanced')\n",
    "\n",
    "model = SVC(kernel='linear',class_weight='balanced', probability=True)\n",
    "\n",
    "#GridSearchCV evalutes all combination of parameters defined in paarmeter grid in model.\n",
    "#https://towardsdatascience.com/understanding-hyperparameters-and-its-optimisation-techniques-f0debba07568- Hyperparameter Techniques\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#create parameter grid\n",
    "param_grid ={\n",
    "    'max-depth' :{80,45,72}\n",
    "    'n_estimator':{100,200,300}}\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "#initiate the gridsearchmodel\n",
    "grid_search_model = GridSearchCV(estimator=model,\n",
    "                                param_grid=param_grid,cv=5,n_jobs=-1, scoring='f1')\n",
    "\n",
    "# Define the parameter sets to test\n",
    "param_grid = {'n_estimators': [1, 30], 'max_features': ['auto', 'log2'], 'max_depth': [4, 8], 'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Define the model to use\n",
    "model = RandomForestClassifier(random_state=5)\n",
    "\n",
    "# Combine the parameter sets with the defined model\n",
    "CV_model = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='recall', n_jobs=-1)\n",
    "\n",
    "# Fit the model to our training data and obtain best parameters\n",
    "CV_model.fit(X_train, y_train)\n",
    "CV_model.best_params_\n",
    "\n",
    "# Input the optimal parameters in the model\n",
    "model = RandomForestClassifier(class_weight={0:1,1:12}, criterion='gini',\n",
    "            max_depth=8, max_features='log2', min_samples_leaf=10, n_estimators=30, n_jobs=-1, random_state=5)\n",
    "\n",
    "# Get results from your model\n",
    "get_model_results(X_train, y_train, X_test, y_test, model)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################### Exercise ##########################\n",
    "# Project titanic dataset\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1], dtype=int64)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Loan repayment use cases\n",
    "\n",
    "#Importing neccessary packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn import tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "#from sklearn.cross_validation import train_test_split - depreciated\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#Loading data file\n",
    "balance_data =pd.read_csv('C:\\\\Users\\\\rimanshu\\\\Desktop\\\\Jupyter\\\\Loan Repayment.csv')\n",
    "balance_data.head()\n",
    "\n",
    "df = balance_data.rename(columns={\"not.fully.paid\": \"not_fully_paid\"})\n",
    "\n",
    "df.not_fully_paid.unique()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Length : 9578\n",
      "Dataset Shape : (9578, 14)\n"
     ]
    }
   ],
   "source": [
    "print(\"Dataset Length :\", len(balance_data))\n",
    "print(\"Dataset Shape :\", balance_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
