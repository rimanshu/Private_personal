AdaBoost is the shortcut for adaptive boosting. So what’s the differences between Adaptive boosting and Gradient boosting?

Both are boosting algorithms which means that they convert a set of weak learners into a single strong learner. They both initialize a strong learner (usually a decision tree) and iteratively create a weak learner that is added to the strong learner. They differ on how they create the weak learners during the iterative process.

At each iteration, adaptive boosting changes the sample distribution by modifying the weights attached to each of the instances. It increases the weights of the wrongly predicted instances and decreases the ones of the correctly predicted instances. The weak learner thus focuses more on the difficult instances. After being trained, the weak learner is added to the strong one according to his performance (so-called alpha weight). The higher it performs, the more it contributes to the strong learner.

On the other hand, gradient boosting doesn’t modify the sample distribution. Instead of training on a newly sample distribution, the weak learner trains on the remaining errors (so-called pseudo-residuals) of the strong learner. It is another way to give more importance to the difficult instances. At each iteration, the pseudo-residuals are computed and a weak learner is fitted to these pseudo-residuals. Then, the contribution of the weak learner (so-called multiplier) to the strong one isn’t computed according to his performance on the newly distribution sample but using a gradient descent optimization process. The computed contribution is the one minimizing the overall error of the strong learner.

In my opinion, gradient boosting and Adaboost are fundamentally different. I think the key word ‘boost’ is misleading here. Adaboost is more about ‘voting weights’ and gradient boosting is more about ‘adding gradient optimization’.

Adaboost doesn’t overfit because it is more about ‘organizing people to vote’ than ‘voting’. In fact, if you have a gradient boosting model, you can use it in adaboost along with other models.

Gradient boosting calculate essentially the gradient of the loss function with respect to the prediction (instead of the features) and this way generates an extra ‘helper prediction’ to enhance the prediction and make the weak prediction closer and closer to y_test. So it can overfit.