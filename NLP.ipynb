{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP or NLTK -- https://www.youtube.com/watch?v=lIajL8_Q-gw&index=3&list=PLcTXcpndN-Sl9eYrKM6jtcOTgC52EJnqH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My name is Rimanshu.',\n",
       " 'I like nltk.',\n",
       " 'i like python.',\n",
       " 'I need to learn ASAP']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908\n",
    "# Sentance Tokenizer\n",
    "import nltk\n",
    "#nltk.download() - to download\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "parag=\"My name is Rimanshu. I like nltk. i like python. I need to learn ASAP\"\n",
    "Arr = sent_tokenize(parag)\n",
    "Arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rimanshu\\Anaconda3\\lib\\site-packages\\nltk\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'won', \"'\", 't', 'let', 'you', 'bring', 'cake']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word Tokenizer\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "Arr=word_tokenize('This is Rimanshu and using PC')\n",
    "Arr[3]\n",
    "\n",
    "#Other way to word tokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tok2=TreebankWordTokenizer()\n",
    "from nltk.tokenize import WordPunctTokenizer # Takes out punctuations\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "tok3=WordPunctTokenizer()\n",
    "\n",
    "# All three will give same result for above mentioned string but will behave diff when can't won't words will be there.\n",
    "\n",
    "sent2 = \"I won't let you bring cake\"\n",
    "\n",
    "tok2.tokenize(sent2) #['I', 'wo', \"n't\", 'let', 'you', 'bring', 'cake']. \n",
    "#Tok2 will not direcly call as method like word_tokenize because it is object not method.\n",
    "\n",
    "tok3.tokenize(sent2) #['I', 'won', \"'\", 't', 'let', 'you', 'bring', 'cake']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', \"Can't\", 'do', 'this', 'I', \"won't\", 'do', 'that']"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Regex Expression for tokenize\n",
    "#[\\w]+- w stands for word and + stands for sequence of words. [\\d]+ d for digit.[\\s+] s means space\n",
    "\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "sent1 = \"i Can't do this. I won't do that\"\n",
    "regexp_tokenize(sent1,\"[\\w']+\") #['i', \"Can't\", 'do', 'this', 'I', \"won't\", 'do', 'that']\n",
    "regexp_tokenize(sent1,\"[\\w]+\") #['i', 'Can', 't', 'do', 'this', 'I', 'won', 't', 'do', 'that']\n",
    "regexp_tokenize(sent1,\"[\\w']\") # Will show every letter.\n",
    "\n",
    "#Another way to tokenize from regex\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "Tok1=RegexpTokenizer(\"[\\w']+\")\n",
    "Tok1.tokenize(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What', 'right', 'exactly', '?', 'I', 'wanted', 'get', 'office', '.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stop words - Doesn't have any meaning.\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import gutenberg # Contains Books,chats etc\n",
    "sample_words=stopwords.words('english')\n",
    "sample_words\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "parag1=\"What are you doing right now exactly? I wanted to get to my office.\"\n",
    "paragArr=word_tokenize(parag1)\n",
    "\n",
    "FilterArr= [item for item in paragArr if item not in sample_words]\n",
    "FilterArr\n",
    "\n",
    "#Or we can write in this way also\n",
    "filtered_sentence = []\n",
    "\n",
    "for item in paragArr:\n",
    "    if item not in sample_words:\n",
    "        filtered_sentence.append(item)\n",
    "\n",
    "print(paragArr)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Synsets (set of synonyms), Hypernyms(Parent name or abstract) and Hyponyms(Child class or more specific)\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "word1=\"weapon\"\n",
    "synArr=wordnet.synsets(word1)\n",
    "synArr #[Synset('weapon.n.01'), Synset('weapon.n.02')] ... n is noun\n",
    "\n",
    "woi=synArr[0]\n",
    "woi # Word of interest\n",
    "woi.definition() #'any instrument or instrumentality used in fighting or hunting'\n",
    "woi.name() #'weapon.n.01'\n",
    "woi.pos() # n result. n means noun and a means adjective. pos means part of speaches\n",
    "\n",
    "woi.hypernyms() #[Synset('instrument.n.01')]\n",
    "woi.hyponyms() # More specific like [Synset('bow.n.04'),Synset('bow_and_arrow.n.01') etc]\n",
    "\n",
    "woi2=woi.hyponyms()[1] #Synset('bow_and_arrow.n.01')\n",
    "woi2.definition() #'a weapon consisting of arrows and the bow to shoot them'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POS - Part of speach\n",
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "#One is a State of the Union address from 2005, and the other is from 2006 from past President George W. Bush.\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:5]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            namedEnt = nltk.ne_chunk(tagged, binary=True)\n",
    "            namedEnt.draw()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fail', 'fall_back', 'lose', 'losings'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lemmas (Stem words like run for running or ran words), synonyms, Antonyms\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "sArr = wordnet.synsets('win')\n",
    "sArr\n",
    "sArr[2]\n",
    "\n",
    "woi = sArr[2]\n",
    "woi.pos()\n",
    "woi.definition()\n",
    "woi.lemmas() # [Lemma('win.v.01.win')]. You cann have more than one lemmas.\n",
    "\n",
    "#initializing the arrays\n",
    "SynArr =[]\n",
    "AntArr =[]\n",
    "\n",
    "for syn in sArr:\n",
    "    for lem in syn.lemmas():\n",
    "        SynArr.append(lem.name()) # name method will give you the name of lemmas\n",
    "\n",
    "SynArr # Synonyms of the word\n",
    "\n",
    "set(SynArr) # It will convert into object from array. It is set of element\n",
    "\n",
    "len(SynArr)\n",
    "\n",
    "woi.lemmas()[0].antonyms()[0].name()\n",
    "\n",
    "for syn in sArr:\n",
    "    for lem in syn.lemmas():\n",
    "        for ant in lem.antonyms():\n",
    "            AntArr.append(ant.name())\n",
    "        \n",
    "AntArr\n",
    "\n",
    "set(AntArr)\n",
    "#len(set(AntArr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6375861597263857"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Similarities between two diff words by wu palmer, path, LCH\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "#or import * - it will import all the things\n",
    "sarr1=wordnet.synsets('cake')\n",
    "sarr2=wordnet.synsets('loaf')\n",
    "sarr3=wordnet.synsets('bread')\n",
    "\n",
    "cake=sarr1[0]\n",
    "loaf=sarr2[0]\n",
    "loafb=sarr2[1]\n",
    "bread=sarr3[0]\n",
    "\n",
    "#wu Palmer similarity\n",
    "\n",
    "cake.wup_similarity(loaf) #0.26666666666666666\n",
    "loafb.wup_similarity(loaf) #0.7142857142857143\n",
    "bread.wup_similarity(loafb) #0.7692307692307693\n",
    "\n",
    "loaf.hypernyms() #[Synset('bread.n.01')]\n",
    "bread.hypernyms()\n",
    "\n",
    "ref=loaf.hypernyms()[0]\n",
    "ref\n",
    "\n",
    "loaf.shortest_path_distance(ref) #1\n",
    "\n",
    "bread.shortest_path_distance(ref) # 0 means both are same. as number increases it will differ.\n",
    "\n",
    "catarr=wordnet.synsets('cat')\n",
    "dogarr=wordnet.synsets('dog')\n",
    "\n",
    "coi=catarr[0]\n",
    "doi=dogarr[0]\n",
    "\n",
    "doi.wup_similarity(coi) #0.8571428571428571\n",
    "\n",
    "doi.path_similarity(coi) #.2\n",
    "\n",
    "doi.path_similarity(doi) # 1 is maximum value\n",
    "\n",
    "doi.lch_similarity(coi) #lch means leacock chodorow. 2.0281482472922856\n",
    "\n",
    "doi.lch_similarity(doi) # Maximum value is \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('black', 'knight'),\n",
       " ('clop', 'clop'),\n",
       " ('head', 'knight'),\n",
       " ('mumble', 'mumble'),\n",
       " ('squeak', 'squeak'),\n",
       " ('saw', 'saw'),\n",
       " ('holy', 'grail'),\n",
       " ('run', 'away'),\n",
       " ('french', 'guard'),\n",
       " ('cartoon', 'character')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bigram like lets go, united kingdom, words combination of two words.\n",
    "\n",
    "from nltk.corpus import webtext\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "\n",
    "textwords=[w.lower() for w in webtext.words('pirates.txt')]\n",
    "textwords\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(textwords)\n",
    "finder\n",
    "finder.nbest(BigramAssocMeasures.likelihood_ratio,10)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "ignored_words = set(stopwords.words('english'))\n",
    "ignored_words\n",
    "\n",
    "filterStops = lambda w: len(w) < 3 or w in ignored_words # Lambda function to filter out words\n",
    "\n",
    "finder.apply_word_filter(filterStops)\n",
    "finder.nbest(BigramAssocMeasures.likelihood_ratio,10)\n",
    "\n",
    "#Trigrams \n",
    "\n",
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.metrics import TrigramAssocMeasures\n",
    "\n",
    "from nltk.corpus import webtext\n",
    "\n",
    "textwords=[w.lower() for w in webtext.words('grail.txt')]\n",
    "textwords\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(textwords)\n",
    "finder\n",
    "finder.nbest(BigramAssocMeasures.likelihood_ratio,10)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "ignored_words = set(stopwords.words('english'))\n",
    "ignored_words\n",
    "\n",
    "filterStops = lambda w: len(w) < 3 or w in ignored_words # Lambda function to filter out words\n",
    "\n",
    "finder.apply_word_filter(filterStops)\n",
    "finder.nbest(BigramAssocMeasures.likelihood_ratio,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'writ'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Stemming - Pick out the root word from the given words.Like cooking will become cook as part of stemming\n",
    "#Stemming algorithms work by cutting off the end or the beginning of the word, \n",
    "#taking into account a list of common prefixes and suffixes that can be found in an inflected word. \n",
    "#This indiscriminate cutting can be successful in some occasions, but not always, \n",
    "#and that is why we affirm that this approach presents some limitations. Below we illustrate the method with examples in \n",
    "#both English and Spanish.\n",
    "\n",
    "from nltk.stem import PorterStemmer    # Least agressive stemming method. Most common use.\n",
    "from nltk.stem import LancasterStemmer # Most agressive stemming method. it is used for huge data. Most common use\n",
    "from nltk.stem import RegexpStemmer    # When we use small datasets to work with. Quite focus for result to getting.\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "pstemmer = PorterStemmer()\n",
    "pstemmer.stem('dancing') #danc\n",
    "pstemmer.stem('dancer') #dancer\n",
    "pstemmer.stem('cooking') #cook\n",
    "\n",
    "#Or we can do in this way also\n",
    "text=\"It is important to by very pythonly while you are pythoning with python.All pythoners have pythoned poorly at least once.\"\n",
    "words = word_tokenize(text)\n",
    "\n",
    "for w in words:\n",
    "    print(pstemmer.stem(w))\n",
    "    \n",
    "\n",
    "lstemmer =LancasterStemmer()\n",
    "lstemmer.stem('dancing') # dant\n",
    "lstemmer.stem('cooking') #cook\n",
    "\n",
    "rstemmer=RegexpStemmer('ing')\n",
    "rstemmer.stem('skiing') #ski\n",
    "rstemmer.stem('dancing') #danc\n",
    "rstemmer.stem('king') #k\n",
    "lstemmer.stem('king') #king\n",
    "\n",
    "rstemmer1=RegexpStemmer('er')\n",
    "rstemmer1.stem('writer') #writ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bus'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lemmatization, on the other hand, takes into consideration the morphological analysis of the words. \n",
    "#To do so, it is necessary to have detailed dictionaries which the algorithm can look through to link the form back to its lemma.\n",
    "#Again, you can see how it works with the same example words.\n",
    "# https://blog.bitext.com/what-is-the-difference-between-stemming-and-lemmatization/\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lzr=WordNetLemmatizer()\n",
    "lzr.lemmatize('dancing') #dancing\n",
    "lzr.lemmatize('working') #working. It assume it is noun. it will give diff result with verb.\n",
    "\n",
    "lzr.lemmatize('dancing', pos='v') # dance\n",
    "lzr.lemmatize('working', pos='a')\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stm=PorterStemmer()\n",
    "\n",
    "stm.stem('believes') #believ. Chopping of the word by using suffix and preffix\n",
    "\n",
    "lzr.lemmatize('believes') #belief. It uses the base word\n",
    "\n",
    "stm.stem('buses') #buse\n",
    "lzr.lemmatize('buses') #bus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Orignal line :i won't go school\n",
      "\n",
      " Edited line :i will not go school\n"
     ]
    }
   ],
   "source": [
    "#Regular Expression Replacer. https://www.guru99.com/python-regular-expressions-complete-tutorial.html\n",
    "#In Python regular expression supports various things like Modifiers, Identifiers, and White space characters.\n",
    "\n",
    "#re.match(),re.search(),re.findall(),re.split(),re.sub(),re.compile()\n",
    "\n",
    "#+ = matches 1 or more , ? = matches 0 or 1, * = 0 or more, $ match end of a string,^ match start of a string,|matcheseitheror x/y\n",
    "\n",
    "#\\d= any number (a digit),\\w=One word character including digits \\s= space, \\t =tab, \\n = new line, \\e = escape\n",
    "\n",
    "#\\D=One non-digit like others W, S and so on\\\\\n",
    "\n",
    "#$ End of string ,^ Start of string, ab|cd - Matches ab or de. [ab-d]\tOne character of: a, b, c, d\n",
    "#[^ab-d]\t One character except: a, b, c, d , () Items within parenthesis are retrieved\n",
    "#(a(bc)) Items within the sub-parenthesis are retrieved\n",
    "\n",
    "#[ab]{2,} - 2 or more continuous occurrences of a or b. + One or more. * Zero or more\n",
    "#? 0 or 1.  . - One character except new line\n",
    "\n",
    "import re\n",
    "#Complile fucntion only checks the pattern\n",
    "regex = re.compile('\\s+') #Here the '\\s' matches any WHITESPACES character. By adding a '+' notation at the end will make the pattern match \n",
    "#at least 1 or more spaces. So, this pattern will match even tab '\\t' characters as well.\n",
    "\n",
    "text = \"101 COM    Computers 205 MAT   Mathematics 189 ENG   English\"\n",
    "\n",
    "re.split('\\s+', text) #['101', 'COM', 'Computers', '205', 'MAT', 'Mathematics', '189', 'ENG', 'English']\n",
    "\n",
    "regex_num = re.compile('\\d+') #'\\d' is a regular expression which matches any digit\n",
    "regex_num.findall(text)  #Adding a '+' symbol to it mandates the presence of at least 1 digit to be present in order to be found.\n",
    "#Similar to '+', there is a '*' symbol which requires 0 or more digits in order to be found\n",
    "\n",
    "regex_num = re.compile('\\d+')\n",
    "s = regex_num.search(text) #But unlike findall which returns the matched portions of the text as a list, regex.search() returns a particular \n",
    "#match object that contains the starting and ending positions of the first occurrence of the pattern.\n",
    "\n",
    "# replace one or more spaces with single space. replace texts, use the regex.sub()\n",
    "regex = re.compile('\\s+')\n",
    "print(regex.sub(' ', text))\n",
    "\n",
    "regex = re.compile('((?!\\n)\\s+)') #negative lookahead (?!\\n). It checks for an upcoming newline character and excludes it from the pattern.\n",
    "print(regex.sub(' ', text)) # ? is for match and then ! is for exclude and then \\n is for new line\n",
    "\n",
    "# 1. extract all course numbers\n",
    "re.findall('[0-9]+', text) #the pattern [0-9]+ instructs to match all number from 0 to 9.\n",
    "\n",
    "# 2. extract all course codes\n",
    "re.findall('[A-Z]{3}', text) #number will certainly have exactly 3 digits, the pattern could have been [0-9]{3} instead.\n",
    "\n",
    "# 3. extract all course names\n",
    "re.findall('[A-Za-z]{4,}', text)\n",
    "\n",
    "course_pattern = '([0-9]+)\\s*([A-Z]{3})\\s*([A-Za-z]{4,})' # Full Pattern\n",
    "re.findall(course_pattern, text)\n",
    "\n",
    "#What is greedy matching in regex? The default behavior of regular expressions is to be greedy. That means it tries to extract\n",
    "#as much as possible until it conforms to a pattern even when a smaller part would have been syntactically sufficient.\n",
    "\n",
    "text = \"< body>Regex Greedy Matching Example < /body>\"\n",
    "re.findall('<.*>', text) # It will fetch whole string coz ? is not there\n",
    "\n",
    "re.findall('<.*?>', text)  #> ['< body>', '< /body>']\n",
    "\n",
    "re.search('<.*?>', text).group() #> '< body>'. if wanted to retrieve only matched.\n",
    "\n",
    "#######################################EXAMPLES ----------------\n",
    "\n",
    "text = '01, Jan 2015'\n",
    "print(re.findall('\\d{4}', text))  # {n} Matches repeat n times.\n",
    "print(re.findall('\\d{2,4}', text))\n",
    "#> ['2015']\n",
    "#> ['01', '2015']\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 COM Computers 205 MAT Mathematics 184 ENG\n",
      "101 COM Computers 205 MAT Mathematics 184 ENG\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['< body>Regex Greedy Matching Example < /body>']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "regex=re.compile(r'won\\'t') # \\ escape character to not confuse double quotes 't'.\n",
    "fst=\"i won't go school\"\n",
    "\n",
    "sst=regex.sub(\"will not\", fst)\n",
    "\n",
    "print(\"\\n Orignal line :\" +fst)\n",
    "print(\"\\n Edited line :\" +sst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Synonyms Replacement with WordMap\n",
    "\n",
    "class WordReplacer(object):\n",
    "    def_init_(self, word_map):\n",
    "        self.word_map = word_map\n",
    "    def replace (self, word):\n",
    "        return self.word_map.get(word, word)\n",
    "\n",
    "from examplereplacer import WordReplacer\n",
    "\n",
    "replacer = WordReplacer ({'bday' : 'Birthday', 'sup' : 'wassup'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
