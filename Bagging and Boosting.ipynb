{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting and Bagging "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://medium.com/greyatom/a-quick-guide-to-boosting-in-ml-acf7c1585cb5\\\n",
    "#https://www.youtube.com/watch?v=m-S9Hojj1as&index=16&list=PLVBorYCcu-xWBPu3o73uj2FJ_7dp6g-pr\n",
    "#https://www.analyticsvidhya.com/blog/2017/03/imbalanced-classification-problem/ - Deal with class imbalace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.youtube.com/watch?v=m-S9Hojj1as&index=16&list=PLVBorYCcu-xWBPu3o73uj2FJ_7dp6g-pr -Bagging and Boosting\n",
    "#Random forest is ensemble of decision tree. When single model is overfit or diff in accuracy is high so we will use ensemble.\n",
    "\n",
    "#Bagging - It is ensemble method where we use multiples models of same algoriths for diff subset of dataset randomdly.\n",
    "#And take the votes to the output.\n",
    "#Bagging is an ensemble technique mainly used to reduce the variance of our predictions by combining the results of multiple \n",
    "#classifiers modelled on different sub-samples of the same dataset. In Bagging, individual learner are trained in parallel.\n",
    "\n",
    "#Bossting - In this case, we give more emphasis on selecting datasets which gives wrong predicton in order to imporve accuracy.\n",
    "#Boosting builds on weak learners, and in an iterative fashion. In each iteration, a new learner is added, while all existing \n",
    "#learners are kept unchanged. All learners are weighted based on their performance (e.g., accuracy), and after a weak learner\n",
    "#is added, the data are re-weighted: examples that are misclassified gain more weights, while examples that are correctly\n",
    "#classified lose weights. Thus, future weak learners focus more on examples that previous weak learners misclassified.\n",
    "#Boosting helps in reduse the vairiance and overfitting of the model.\n",
    "\n",
    "#from sklearn.ensemble import RandomForestClassifier, BaggingClassifier, VotingClassifier AdaBoostClassifier\n",
    "\n",
    "#Types of Boosting Algorithms\n",
    "#AdaBoost (Adaptive Boosting) - Mostly overfit like decision tree\n",
    "#Gradient Tree Boosting\n",
    "#XGBoost\n",
    "\n",
    "#Boosting is a method of converting a set of weak learners into a strong learner. \n",
    "#Many algorithms works with the concept of boosting but today, we will talk about three major boosting algorithms and will \n",
    "#look into their differences:\n",
    "\n",
    "#All these boosting methods differ on how they create the weak learners during the iterative process.\n",
    "\n",
    "#GBM generates learners during the learning process and  build first learner to predict labels and calculate the loss and it will build a second learner to predict the loss after the first step and so on. It minimizes the loss function (difference of actual and predictions).\n",
    "\n",
    "#Adaboost requires a set of weak learners before real learning process and assign the weights to these learners to be a strong learner. The weight of each learner is learned by whether it predicts a sample correctly or not. If a learner is mispredict a sample, the weight of the learner is reduced a bit. It will repeat such process until converge and accuracy increases.\n",
    "\n",
    "#XGBoost is an advanced implementation of GBM having high predictive power (including a variety of regularisation (choose or define), reducing overfitting and improves overall performance) and is many times faster (parallel processing) and also works best in bias-variance tradeoff situations.\n",
    "\n",
    "#algorithms #ml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AdaBoost is the shortcut for adaptive boosting. So what’s the differences between Adaptive boosting and Gradient boosting?\n",
    "\n",
    "Both are boosting algorithms which means that they convert a set of weak learners into a single strong learner. They both initialize a strong learner (usually a decision tree) and iteratively create a weak learner that is added to the strong learner. They differ on how they create the weak learners during the iterative process.\n",
    "\n",
    "At each iteration, adaptive boosting changes the sample distribution by modifying the weights attached to each of the instances. It increases the weights of the wrongly predicted instances and decreases the ones of the correctly predicted instances. The weak learner thus focuses more on the difficult instances. After being trained, the weak learner is added to the strong one according to his performance (so-called alpha weight). The higher it performs, the more it contributes to the strong learner.\n",
    "\n",
    "On the other hand, gradient boosting doesn’t modify the sample distribution. Instead of training on a newly sample distribution, the weak learner trains on the remaining errors (so-called pseudo-residuals) of the strong learner. It is another way to give more importance to the difficult instances. At each iteration, the pseudo-residuals are computed and a weak learner is fitted to these pseudo-residuals. Then, the contribution of the weak learner (so-called multiplier) to the strong one isn’t computed according to his performance on the newly distribution sample but using a gradient descent optimization process. The computed contribution is the one minimizing the overall error of the strong learner.\n",
    "\n",
    "In my opinion, gradient boosting and Adaboost are fundamentally different. I think the key word ‘boost’ is misleading here. Adaboost is more about ‘voting weights’ and gradient boosting is more about ‘adding gradient optimization’.\n",
    "\n",
    "Adaboost doesn’t overfit because it is more about ‘organizing people to vote’ than ‘voting’. In fact, if you have a gradient boosting model, you can use it in adaboost along with other models.\n",
    "\n",
    "Gradient boosting calculate essentially the gradient of the loss function with respect to the prediction (instead of the features) and this way generates an extra ‘helper prediction’ to enhance the prediction and make the weak prediction closer and closer to y_test. So it can overfit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
