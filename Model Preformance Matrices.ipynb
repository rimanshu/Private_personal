{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performace metrices "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Accuracy is the fraction of predictions our model got right. (TP+TN)/TP+TN+FP+FN\n",
    "\n",
    "#Recall or Sensitivity or True Positive Rate -- TP/TP+FN\n",
    "#Number of items correctly identified as positive out of total true positives. High recall means you’re not missing many positives\n",
    "\n",
    "#Precision - Number of items correctly identified as positive out of total items identified as positive. \n",
    "#High precision means low “false alarm rate” (if you test positive, you’re probably positive) - TP/TP+FP\n",
    "\n",
    "#True Negative Rate or Specificity\n",
    "#Number of items correctly identified as negative out of total true negatives.\n",
    "\n",
    "#Fscore is (2*precision *recall)/precision +recall --- harmonic means not airthmetic and geometric mean.\n",
    "\n",
    "#from sklearn.metrics import average_precision_score\n",
    "#from sklearn.metrics import precisionn_recall_score\n",
    "\n",
    "#Calculate the average precision and the PR curve\n",
    "average_precision =average_precision_score(y_test, y_pred)\n",
    "\n",
    "#Obtain precision and recall\n",
    "precision, recall, _ = precision_recall_curve(y_test,y_pred)\n",
    "\n",
    "#ROC curve - receiver operaring characterstic curve\n",
    "\n",
    "#Its is curve between true positive rate(y Axis) and false postive rate(X axis)\n",
    "\n",
    "#Obtain model probabilites \n",
    "probs=model.predict_prob(X_test)\n",
    "\n",
    "#Print ROC curve using probabities\n",
    "print(model.roc_auc_score(y_test,prob[:,1]))\n",
    "\n",
    "#Confusion Matrix and Classification Report\n",
    "from sklearn.matrix import classification_report, confusion_matrix\n",
    "\n",
    "#Obtain Prediction\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#Print classification report using prediction - it contains precesion, recall, f1 score, support\n",
    "\n",
    "print(classification_report(y_test, y_pred)\n",
    "      \n",
    "#Print confusion matrix using prediction \n",
    "print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performace and tunning of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Validation is a technique which involves reserving a particular sample of a dataset on which you do not train the model. \n",
    "#Later, you test your model on this sample before finalizing it.\n",
    "\n",
    "#Kfold cross validation\n",
    "#We also need a good ratio of testing data points. As we have seen above, less amount of data points can lead to a variance error while \n",
    "#testing the effectiveness of the model\n",
    "#We should iterate on the training and testing process multiple times. We should change the train and test dataset distribution\n",
    "#In K-Folds Cross Validation we split our data into k different subsets (or folds). We use k-1 subsets to train our data and \n",
    "#leave the last subset (or the last fold) as test data. We then average the model against each of the folds and then \n",
    "#finalize our model. After that we test it against the test set.\n",
    "\n",
    "#Mostly we choose k is 10. Low value of k is more baised and high value of k is tends to variance.\n",
    "\n",
    "\n",
    "#LOOCV cross validation.\n",
    "#we reserve only one data point from the available dataset, and train the model on the rest of the data. \n",
    "#This process iterates for each data point. This will have low baise and hig variance.\n",
    "\n",
    "#Stratified k-fold cross validation\n",
    "#Stratification is the process of rearranging the data so as to ensure that each fold is a good representative of the whole. \n",
    "#For example, in a binary classification problem where each class comprises of 50% of the data, it is best to arrange the \n",
    "#data such that in every fold, each class comprises of about half the instances.\n",
    "\n",
    "#A randomly selected fold might not adequately represent the minor class, particularly in cases where there is a huge class imbalance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper parameter by GrisearchCV, Random Search, Bayesian Optimization.\n",
    "#https://towardsdatascience.com/understanding-hyperparameters-and-its-optimisation-techniques-f0debba07568\n",
    "#https://medium.com/@aneesha/svm-parameter-tuning-in-scikit-learn-using-gridsearchcv-2413c02125a\n",
    "\n",
    "#Hyperparameters are important because they directly control the behaviour of the training algorithm and have a significant \n",
    "#impact on the performance of the model is being trained.\n",
    "\n",
    "#1) Grid serachCV\n",
    "#Grid search trains the algorithm for all combinations by using the two set of hyperparameters (learning rate and number of \n",
    "#layers) and measures the performance using “Cross Validation” technique. This validation technique gives assurance that our \n",
    "#trained model got most of the patterns from the dataset. One of the best methods to do validation by using “K-Fold Cross \n",
    "#Validation” which helps to provide ample data for training the model and ample data for validations.\n",
    "\n",
    "#Randomly samples the search space and evaluates sets from a specified probability distribution. For example, \n",
    "#Instead of trying to check all 100,000 samples, we can check 1000 random parameters.\n",
    "\n",
    "#bayesian Parameter.\n",
    "\n",
    "param_grid = {'n_estimators': [1, 30], 'max_features': ['auto', 'log2'],  'max_depth': [4, 8], 'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Define the model to use\n",
    "model = RandomForestClassifier(random_state=5)\n",
    "\n",
    "# Combine the parameter sets with the defined model\n",
    "CV_model = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='recall', n_jobs=-1)\n",
    "\n",
    "#K -Fold Cross validation is the technique where we keep a part of dataset and do not train on it and use that part \n",
    "#for testing or validation.\n",
    "#Grid Search : This is used to tune our hyperparameters and get the best set of parameters\n",
    "\n",
    "#The question is how GridSearch and Cross Validation are inter linked. Is it necessary that I have to use both of them \n",
    "#simultaneously or I can use them independently.\n",
    "\n",
    "#Please explain how this below line is evaluated :\n",
    "grid = GridSearchCV(estimator=lasso_clf, model_grid, cv=LeaveOneOut(train.shape[0]),scoring=‘mean_squared_error’)\n",
    "\n",
    "#You can just cross validate your model using k-fold or if you use another technique for validation you can just use grid \n",
    "#search to find the optimum parameters for your model. The reason we link both is usually you want to cross validate your\n",
    "#model and want to have best parameters too at the same time(machine learning competitions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example with code\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import *\n",
    "\n",
    "model =RandomForestClassifier(class_weight='balanced')\n",
    "#Weight option is also takes manual input like - class_weight={0:1,1:4}\n",
    "\n",
    "model =RandomForestClassifier(class_weight='balanced_subsample')\n",
    "\n",
    "model =LogisticRegression(class_weight='balanced')\n",
    "\n",
    "model = SVC(kernel='linear',class_weight='balanced', probability=True)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#create parameter grid\n",
    "param_grid ={\n",
    "    'max-depth' :{80,45,72}\n",
    "    'n_estimator':{100,200,300}}\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "#initiate the gridsearchmodel\n",
    "grid_search_model = GridSearchCV(estimator=model,\n",
    "                                param_grid=param_grid,cv=5,n_jobs=-1, scoring='f1')\n",
    "\n",
    "# Define the parameter sets to test\n",
    "param_grid = {'n_estimators': [1, 30], 'max_features': ['auto', 'log2'], 'max_depth': [4, 8], 'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Define the model to use\n",
    "model = RandomForestClassifier(random_state=5)\n",
    "\n",
    "# Combine the parameter sets with the defined model\n",
    "CV_model = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='recall', n_jobs=-1)\n",
    "#here cv tells how you want to split your dataset for validation. scoring is the metric you are using to test your model’s\n",
    "#accuracy, in this case since it is regression model you have used mean_squared_error as the metric.\n",
    "# Fit the model to our training data and obtain best parameters\n",
    "CV_model.fit(X_train, y_train)\n",
    "CV_model.best_params_\n",
    "\n",
    "# Input the optimal parameters in the model\n",
    "model = RandomForestClassifier(class_weight={0:1,1:12}, criterion='gini',\n",
    "            max_depth=8, max_features='log2', min_samples_leaf=10, n_estimators=30, n_jobs=-1, random_state=5)\n",
    "\n",
    "# Get results from your model\n",
    "get_model_results(X_train, y_train, X_test, y_test, model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
