{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performace metrices "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://mlwhiz.com/blog/2021/02/03/roc-auc-curves-explained/\n",
    "\n",
    "Example for recall and precision \n",
    "\n",
    "cancer detection usecase will use recall\n",
    "\n",
    "Credit card limit decrease usecase will use precision because if FP will be there then customer will not satisfy.\n",
    "\n",
    "What does a high ROC AUC score mean?\n",
    "\n",
    "The Area Under the Curve (AUC) is the measure of the ability of a classifier to distinguish between classes and is used as a summary of the ROC curve. The higher the AUC, the better the performance of the model at distinguishing between the positive and negative classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://sachinkalsi.github.io/blog/category/ml/2018/08/20/top-8-performance-metrics-one-should-know.html\n",
    "#Accuracy is the fraction of predictions our model got right. (TP+TN)/TP+TN+FP+FN\n",
    "\n",
    "#Recall or Sensitivity or True Positive Rate -- TP/TP+FN\n",
    "#Number of items correctly identified as positive out of total true positives. High recall means you’re not missing many positives\n",
    "\n",
    "#Precision - Number of items correctly identified as positive out of total items identified as positive. \n",
    "#High precision means low “false alarm rate” (if you test positive, you’re probably positive) - TP/TP+FP\n",
    "\n",
    "#True Negative Rate or Specificity\n",
    "#Number of items correctly identified as negative out of total true negatives.\n",
    "\n",
    "#Fscore is (2*precision *recall)/precision +recall --- harmonic means not airthmetic and geometric mean.\n",
    "\n",
    "#from sklearn.metrics import average_precision_score\n",
    "#from sklearn.metrics import precisionn_recall_score\n",
    "\n",
    "#Calculate the average precision and the PR curve\n",
    "average_precision =average_precision_score(y_test, y_pred)\n",
    "\n",
    "#Obtain precision and recall\n",
    "precision, recall, _ = precision_recall_curve(y_test,y_pred)\n",
    "\n",
    "#ROC curve - receiver operaring characterstic curve\n",
    "\n",
    "#Its is curve between true positive rate(x Axis) and false postive rate(Y axis)\n",
    "\n",
    "#Obtain model probabilites \n",
    "probs=model.predict_prob(X_test)\n",
    "\n",
    "#Print ROC curve using probabities\n",
    "print(model.roc_auc_score(y_test,prob[:,1]))\n",
    "\n",
    "#Confusion Matrix and Classification Report\n",
    "from sklearn.matrix import classification_report, confusion_matrix\n",
    "\n",
    "#Obtain Prediction\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "#Print classification report using prediction - it contains precesion, recall, f1 score, support\n",
    "\n",
    "print(classification_report(y_test, y_pred)\n",
    "      \n",
    "#Print confusion matrix using prediction \n",
    "print(confusion_matrix(y_test, predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performace and tunning of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Cross Validation is a technique which involves reserving a particular sample of a dataset on which you do not train the model. \n",
    "#Later, you test your model on this sample before finalizing it.\n",
    "#Cross Validation - Data is divide into let say 4 blocks and in first round it will take 1st 3 blocks or 75% of data to be train \n",
    "#and last block means 25% data would be for test and summerize the data. Then 1,2,4 blocks for train and 3rd block for test and so on.\n",
    "\n",
    "#Cross-validation is a resampling procedure used to evaluate machine learning models on a limited data sample. \n",
    "#The procedure has a single parameter called k that refers to the number of groups that a given data sample is to be split into.\n",
    "#As such, the procedure is often called k-fold cross-validation\n",
    "\n",
    "#Kfold cross validation\n",
    "#We also need a good ratio of testing data points. As we have seen above, less amount of data points can lead to a variance error while \n",
    "#testing the effectiveness of the model\n",
    "#We should iterate on the training and testing process multiple times. We should change the train and test dataset distribution\n",
    "#In K-Folds Cross Validation we split our data into k different subsets (or folds). We use k-1 subsets to train our data and \n",
    "#leave the last subset (or the last fold) as test data. We then average the model against each of the folds and then \n",
    "#finalize our model. After that we test it against the test set.\n",
    "\n",
    "#Mostly we choose k is 10. Low value of k is more baised and high value of k is tends to variance.\n",
    "\n",
    "\n",
    "#LOOCV cross validation.\n",
    "#we reserve only one data point from the available dataset, and train the model on the rest of the data. \n",
    "#This process iterates for each data point. This will have low baise and hig variance.\n",
    "\n",
    "#Stratified k-fold cross validation\n",
    "#Stratification is the process of rearranging the data so as to ensure that each fold is a good representative of the whole. \n",
    "#For example, in a binary classification problem where each class comprises of 50% of the data, it is best to arrange the \n",
    "#data such that in every fold, each class comprises of about half the instances.\n",
    "\n",
    "#A randomly selected fold might not adequately represent the minor class, particularly in cases where there is a huge class imbalance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyper parameter by GrisearchCV, Random Search, Bayesian Optimization.\n",
    "#https://towardsdatascience.com/understanding-hyperparameters-and-its-optimisation-techniques-f0debba07568\n",
    "#https://medium.com/@aneesha/svm-parameter-tuning-in-scikit-learn-using-gridsearchcv-2413c02125a\n",
    "\n",
    "#Hyperparameters are important because they directly control the behaviour of the training algorithm and have a significant \n",
    "#impact on the performance of the model is being trained.\n",
    "\n",
    "#1) Grid serachCV\n",
    "#Grid search trains the algorithm for all combinations by using the two set of hyperparameters (learning rate and number of \n",
    "#layers) and measures the performance using “Cross Validation” technique. This validation technique gives assurance that our \n",
    "#trained model got most of the patterns from the dataset. One of the best methods to do validation by using “K-Fold Cross \n",
    "#Validation” which helps to provide ample data for training the model and ample data for validations.\n",
    "\n",
    "#Randomly samples the search space and evaluates sets from a specified probability distribution. For example, \n",
    "#Instead of trying to check all 100,000 samples, we can check 1000 random parameters.\n",
    "\n",
    "#bayesian Parameter.\n",
    "\n",
    "param_grid = {'n_estimators': [1, 30], 'max_features': ['auto', 'log2'],  'max_depth': [4, 8], 'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Define the model to use\n",
    "model = RandomForestClassifier(random_state=5)\n",
    "\n",
    "# Combine the parameter sets with the defined model\n",
    "CV_model = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='recall', n_jobs=-1)\n",
    "\n",
    "#K -Fold Cross validation is the technique where we keep a part of dataset and do not train on it and use that part \n",
    "#for testing or validation.\n",
    "#Grid Search : This is used to tune our hyperparameters and get the best set of parameters\n",
    "\n",
    "#The question is how GridSearch and Cross Validation are inter linked. Is it necessary that I have to use both of them \n",
    "#simultaneously or I can use them independently.\n",
    "\n",
    "#Please explain how this below line is evaluated :\n",
    "grid = GridSearchCV(estimator=lasso_clf, model_grid, cv=LeaveOneOut(train.shape[0]),scoring=‘mean_squared_error’)\n",
    "\n",
    "#You can just cross validate your model using k-fold or if you use another technique for validation you can just use grid \n",
    "#search to find the optimum parameters for your model. The reason we link both is usually you want to cross validate your\n",
    "#model and want to have best parameters too at the same time(machine learning competitions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Example with code\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import *\n",
    "\n",
    "model =RandomForestClassifier(class_weight='balanced')\n",
    "#Weight option is also takes manual input like - class_weight={0:1,1:4}\n",
    "\n",
    "model =RandomForestClassifier(class_weight='balanced_subsample')\n",
    "\n",
    "model =LogisticRegression(class_weight='balanced')\n",
    "\n",
    "model = SVC(kernel='linear',class_weight='balanced', probability=True)\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "#create parameter grid\n",
    "param_grid ={\n",
    "    'max-depth' :{80,45,72}\n",
    "    'n_estimator':{100,200,300}}\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "#initiate the gridsearchmodel\n",
    "grid_search_model = GridSearchCV(estimator=model,\n",
    "                                param_grid=param_grid,cv=5,n_jobs=-1, scoring='f1')\n",
    "\n",
    "# Define the parameter sets to test\n",
    "param_grid = {'n_estimators': [1, 30], 'max_features': ['auto', 'log2'], 'max_depth': [4, 8], 'criterion': ['gini', 'entropy']\n",
    "}\n",
    "\n",
    "# Define the model to use\n",
    "model = RandomForestClassifier(random_state=5)\n",
    "\n",
    "# Combine the parameter sets with the defined model\n",
    "CV_model = GridSearchCV(estimator=model, param_grid=param_grid, cv=5, scoring='recall', n_jobs=-1)\n",
    "#here cv tells how you want to split your dataset for validation. scoring is the metric you are using to test your model’s\n",
    "#accuracy, in this case since it is regression model you have used mean_squared_error as the metric.\n",
    "# Fit the model to our training data and obtain best parameters\n",
    "CV_model.fit(X_train, y_train)\n",
    "CV_model.best_params_\n",
    "\n",
    "# Input the optimal parameters in the model\n",
    "model = RandomForestClassifier(class_weight={0:1,1:12}, criterion='gini',\n",
    "            max_depth=8, max_features='log2', min_samples_leaf=10, n_estimators=30, n_jobs=-1, random_state=5)\n",
    "\n",
    "# Get results from your model\n",
    "get_model_results(X_train, y_train, X_test, y_test, model)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
