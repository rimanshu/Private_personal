{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entity Extraction using spacy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.machinelearningplus.com/spacy-tutorial-nlp/\n",
    "\n",
    "https://www.machinelearningplus.com/nlp/training-custom-ner-model-in-spacy/\n",
    "\n",
    "https://github.com/susanli2016/NLP-with-Python/blob/master/NER_NLTK_Spacy.ipynb\n",
    "\n",
    "https://github.com/akshayashokbhor/Custom-Trained-Named-entity-recognizer-using-Spacy-library-for-resume-Data-extraction/blob/main/resume_auto_ML.ipynb\n",
    "\n",
    "https://akshay-bhor.medium.com/custom-trained-named-entity-recognizer-using-spacy-library-for-resume-data-extraction-d419cfd3fba0\n",
    "\n",
    "https://manivannan-ai.medium.com/how-to-train-ner-with-custom-training-data-using-spacy-188e0e508c6\n",
    "\n",
    "https://medium.com/@radu.gheorghe/entity-extraction-with-spacy-234d3d11e3ba\n",
    "\n",
    "https://github.com/DataTurks-Engg/Entity-Recognition-In-Resumes-SpaCy\n",
    "\n",
    "https://www.kaggle.com/dattaraj/demo-of-using-custom-ner-model-on-covid-19-dataset\n",
    "\n",
    "https://github.com/rsreetech/CustomNERwithspaCy/blob/master/CustomNERwithSpacy.ipynb\n",
    "\n",
    "https://stackoverflow.com/questions/63297351/building-a-custom-named-entity-recognition-with-spacy-using-random-text-as-a-s\n",
    "\n",
    "https://stackoverflow.com/questions/63297607/improving-the-recall-of-a-custom-named-entity-recognition-ner-in-spacy\n",
    "\n",
    "https://stackoverflow.com/questions/61266715/how-do-you-build-training-dataset-from-scratch-for-a-custom-multi-class-standfor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -U spacy\n",
    "# !python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "from spacy.util import minibatch, compounding\n",
    "from pathlib import Path\n",
    "from spacy.training import Example\n",
    "from spacy import displacy\n",
    "\n",
    "nlp=spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('11', 'CARDINAL'), ('dog', 'ANIMAL')]\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "from spacy.matcher import PhraseMatcher\n",
    "from spacy.tokens import Span\n",
    "\n",
    "animals = ['cat', 'dog', 'fish', 'bird']\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')  # or any other model\n",
    "patterns = [nlp(animal) for animal in animals]  # process each word to create phrase pattern\n",
    "matcher = PhraseMatcher(nlp.vocab)\n",
    "matcher.add('ANIMAL', None, *patterns)  # add patterns to matcher\n",
    "\n",
    "doc = nlp(\"These are the 11 best dog breeds, based on factors including health, personality, and overall popularity\")\n",
    "matches = matcher(doc)\n",
    "\n",
    "for match_id, start, end in matches:\n",
    "    # create a new Span for each match and use the match_id (ANIMAL) as the label\n",
    "    span = Span(doc, start, end, label=match_id)\n",
    "    doc.ents = list(doc.ents) + [span]  # add span to doc.ents\n",
    "\n",
    "print([(ent.text, ent.label_) for ent in doc.ents])  # [('cat', 'ANIMAL')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from spacy.pipeline import EntityRuler\n",
    "# # Initialize\n",
    "# ruler = EntityRuler(nlp)\n",
    "# pattern=[{\"label\": \"WORK_OF_ART\", \"pattern\": \"My guide to statistics\"}]\n",
    "\n",
    "# ruler.add_patterns(pattern)\n",
    "\n",
    "# nlp.add_pipe(ruler)\n",
    "\n",
    "# doc = nlp(\" I recently published my work fanfiction by Dr.X . Right now I'm studying the book of my friend .You should try My guide to statistics for clear concepts.\")\n",
    "# print([(ent.text, ent.label_) for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Apple Apple PROPN NNP nsubj Xxxxx True False\n",
      "is be AUX VBZ aux xx True True\n",
      "looking look VERB VBG ROOT xxxx True False\n",
      "at at ADP IN prep xx True True\n",
      "buying buy VERB VBG pcomp xxxx True False\n",
      "U.K. U.K. PROPN NNP dobj X.X. False False\n",
      "startup startup NOUN NN advcl xxxx True False\n",
      "for for ADP IN prep xxx True True\n",
      "$ $ SYM $ quantmod $ False False\n",
      "1 1 NUM CD compound d False False\n",
      "billion billion NUM CD pobj xxxx True False\n",
      "Pipelines  ['tok2vec', 'tagger', 'parser', 'ner', 'attribute_ruler', 'lemmatizer']\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Apple is looking at buying U.K. startup for $1 billion\")\n",
    "\n",
    "for token in doc:\n",
    "    print(token.text, token.lemma_, token.pos_, token.tag_, token.dep_,\n",
    "            token.shape_, token.is_alpha, token.is_stop)\n",
    "    \n",
    "print(\"Pipelines \", nlp.pipe_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Noun phrases: ['Sebastian Thrun', 'self-driving cars', 'Google', 'few people', 'the company', 'him', 'I', 'you', 'very senior CEOs', 'major American car companies', 'my hand', 'I', 'Thrun', 'an interview', 'Recode']\n",
      "Verbs: ['start', 'work', 'drive', 'take', 'tell', 'shake', 'turn', 'be', 'talk', 'say']\n",
      "['Sebastian Thrun', 'PERSON']\n",
      "['2007', 'DATE']\n",
      "['American', 'NORP']\n",
      "['Thrun', 'PERSON']\n",
      "['Recode', 'PERSON']\n",
      "['earlier this week', 'DATE']\n",
      "\n",
      "\n",
      "['Sebastian Thrun', 5, 20, 'PERSON']\n",
      "['2007', 71, 75, 'DATE']\n",
      "['American', 173, 181, 'NORP']\n",
      "['Thrun', 271, 276, 'PERSON']\n",
      "['Recode', 299, 305, 'PERSON']\n",
      "['earlier this week', 306, 323, 'DATE']\n",
      "\n",
      "\n",
      "[(Sebastian Thrun, 'PERSON', 'People, including fictional'), (2007, 'DATE', 'Absolute or relative dates or periods'), (American, 'NORP', 'Nationalities or religious or political groups'), (Thrun, 'PERSON', 'People, including fictional'), (Recode, 'PERSON', 'People, including fictional'), (earlier this week, 'DATE', 'Absolute or relative dates or periods')]\n"
     ]
    }
   ],
   "source": [
    "# Process whole documents\n",
    "text = (\"When Sebastian Thrun started working on self-driving cars at \"\n",
    "        \"Google in 2007, few people outside of the company took him \"\n",
    "        \"seriously. “I can tell you very senior CEOs of major American \"\n",
    "        \"car companies would shake my hand and turn away because I wasn’t \"\n",
    "        \"worth talking to,” said Thrun, in an interview with Recode earlier \"\n",
    "        \"this week.\")\n",
    "doc = nlp(text)\n",
    "\n",
    "# Analyze syntax\n",
    "print(\"Noun phrases:\", [chunk.text for chunk in doc.noun_chunks])\n",
    "print(\"Verbs:\", [token.lemma_ for token in doc if token.pos_ == \"VERB\"])\n",
    "\n",
    "# Find named entities, phrases and concepts\n",
    "for entity in doc.ents:\n",
    "    print([entity.text, entity.label_])\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print([ent.text, ent.start_char, ent.end_char, ent.label_])\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "print([(i, i.label_, str(spacy.explain(i.label_))) for i in doc.ents])\n",
    "\n",
    "# [(i.pos_, i.tag_) for i in doc] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('San Francisco', 0, 13, 'GPE')]\n",
      "['San', 'B', 'GPE']\n",
      "['Francisco', 'I', 'GPE']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\riman\\Anaconda3\\lib\\site-packages\\spacy\\displacy\\__init__.py:97: UserWarning: [W011] It looks like you're calling displacy.serve from within a Jupyter notebook or a similar environment. This likely means you're already running a local web server, so there's no need to make displaCy start another one. Instead, you should be able to replace displacy.serve with displacy.render to show the visualization.\n",
      "  warnings.warn(Warnings.W011)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">\n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    San Francisco\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " considers banning sidewalk delivery robots</div>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'ent' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"San Francisco considers banning sidewalk delivery robots\")\n",
    "\n",
    "# document level\n",
    "ents = [(e.text, e.start_char, e.end_char, e.label_) for e in doc.ents]\n",
    "print(ents)\n",
    "\n",
    "# token level\n",
    "ent_san = [doc[0].text, doc[0].ent_iob_, doc[0].ent_type_]\n",
    "ent_francisco = [doc[1].text, doc[1].ent_iob_, doc[1].ent_type_]\n",
    "print(ent_san)  # ['San', 'B', 'GPE']\n",
    "print(ent_francisco)  # ['Francisco', 'I', 'GPE']\n",
    "\n",
    "displacy.serve(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ABC', 'CRORG']\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><!DOCTYPE html>\n",
       "<html lang=\"en\">\n",
       "    <head>\n",
       "        <title>displaCy</title>\n",
       "    </head>\n",
       "\n",
       "    <body style=\"font-size: 16px; font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Helvetica, Arial, sans-serif, 'Apple Color Emoji', 'Segoe UI Emoji', 'Segoe UI Symbol'; padding: 4rem 2rem; direction: ltr\">\n",
       "<figure style=\"margin-bottom: 6rem\">\n",
       "<div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">i am working in \n",
       "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    ABC\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">CRORG</span>\n",
       "</mark>\n",
       " as senior data science</div>\n",
       "</figure>\n",
       "</body>\n",
       "</html></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Using the 'ent' visualizer\n",
      "Serving on http://0.0.0.0:5000 ...\n",
      "\n",
      "Shutting down server on port 5000.\n"
     ]
    }
   ],
   "source": [
    "ner=nlp.get_pipe(\"ner\")\n",
    "TRAIN_DATA=[('ABC is a worldwide organization',{'entities':[(0,3,'CRORG')]}),\n",
    "           ('we stand with ABC',{'entities':[(24,26,'CRORG')]}),\n",
    "           ('we supports ABC',{'entities':[(15,17,'CRORG')]})]\n",
    "ner.add_label('CRORG')\n",
    "# Disable pipeline components that dont need to change\n",
    "pipe_exceptions = [\"ner\"]\n",
    "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n",
    "\n",
    "with nlp.disable_pipes(*unaffected_pipes):\n",
    "    for iteration in range(30):\n",
    "        random.shuffle(TRAIN_DATA)\n",
    "        for raw_text,entity_offsets in TRAIN_DATA:\n",
    "            doc=nlp.make_doc(raw_text)\n",
    "            nlp.update([Example.from_dict(doc,entity_offsets)])\n",
    "            \n",
    "test_data = \"i am working in ABC as senior data science\"\n",
    "doc=nlp(test_data)\n",
    "for ent in doc.ents:\n",
    "    print([ent.text,ent.label_])\n",
    "    \n",
    "# print(nlp.pipe_names)\n",
    "displacy.serve(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing NER on E-commerce article\n",
    "\n",
    "article_text=\"\"\"India that previously comprised only a handful of players in the e-commerce space, is now home to many biggies and giants battling out with each other to reach the top. This is thanks to the overwhelming internet and smartphone penetration coupled with the ever-increasing digital adoption across the country. These new-age innovations not only gave emerging startups a unique platform to deliver seamless shopping experiences but also provided brick and mortar stores with a level-playing field to begin their online journeys without leaving their offline legacies.\n",
    "In the wake of so many players coming together on one platform, the Indian e-commerce market is envisioned to reach USD 84 billion in 2021 from USD 24 billion in 2017. Further, with the rate at which internet penetration is increasing, we can expect more and more international retailers coming to India in addition to a large pool of new startups. This, in turn, will provide a major Philip to the organized retail market and boost its share from 12% in 2017 to 22-25% by 2021. \n",
    "Here’s a view to the e-commerce giants that are dominating India’s online shopping space:\n",
    "Amazon – One of the uncontested global leaders, Amazon started its journey as a simple online bookstore that gradually expanded its reach to provide a large suite of diversified products including media, furniture, food, and electronics, among others. And now with the launch of Amazon Prime and Amazon Music Limited, it has taken customer experience to a godly level, which will remain undefeatable for a very long time. \n",
    "Flipkart – Founded in 2007, Flipkart is recognized as the national leader in the Indian e-commerce market. Just like Amazon, it started operating by selling books and then entered other categories such as electronics, fashion, and lifestyle, mobile phones, etc. And now that it has been acquired by Walmart, one of the largest leading platforms of e-commerce in the US, it has also raised its bar of customer offerings in all aspects and giving huge competition to Amazon. \n",
    "Snapdeal – Started as a daily deals platform in 2010, Snapdeal became a full-fledged online marketplace in 2011 comprising more than 3 lac sellers across India. The platform offers over 30 million products across 800+ diverse categories from over 125,000 regional, national, and international brands and retailers. The Indian e-commerce firm follows a robust strategy to stay at the forefront of innovation and deliver seamless customer offerings to its wide customer base. It has shown great potential for recovery in recent years despite losing Freecharge and Unicommerce. \n",
    "ShopClues – Another renowned name in the Indian e-commerce industry, ShopClues was founded in July 2011. It’s a Gurugram based company having a current valuation of INR 1.1 billion and is backed by prominent names including Nexus Venture Partners, Tiger Global, and Helion Ventures as its major investors. Presently, the platform comprises more than 5 lac sellers selling products in nine different categories such as computers, cameras, mobiles, etc. \n",
    "Paytm Mall – To compete with the existing e-commerce giants, Paytm, an online payment system has also launched its online marketplace – Paytm Mall, which offers a wide array of products ranging from men and women fashion to groceries and cosmetics, electronics and home products, and many more. The unique thing about this platform is that it serves as a medium for third parties to sell their products directly through the widely-known app – Paytm. \n",
    "Reliance Retail – Given Reliance Jio’s disruptive venture in the Indian telecom space along with a solid market presence of Reliance, it is no wonder that Reliance will soon be foraying into retail space. As of now, it has plans to build an e-commerce space that will be established on online-to-offline market program and aim to bring local merchants on board to help them boost their sales and compete with the existing industry leaders. \n",
    "Big Basket – India’s biggest online supermarket, Big Basket provides a wide variety of imported and gourmet products through two types of delivery services – express delivery and slotted delivery. It also offers pre-cut fruits along with a long list of beverages including fresh juices, cold drinks, hot teas, etc. Moreover, it not only provides farm-fresh products but also ensures that the farmer gets better prices. \n",
    "Grofers – One of the leading e-commerce players in the grocery segment, Grofers started its operations in 2013 and has reached overwhelming heights in the last 5 years. Its wide range of products includes atta, milk, oil, daily need products, vegetables, dairy products, juices, beverages, among others. With its growing reach across India, it has become one of the favorite supermarkets for Indian consumers who want to shop grocery items from the comforts of their homes. \n",
    "Digital Mall of Asia – Going live in 2020, Digital Mall of Asia is a very unique concept coined by the founders of Yokeasia Malls. It is designed to provide an immersive digital space equipped with multiple visual and sensory elements to sellers and shoppers. It will also give retailers exclusive rights to sell a particular product category or brand in their respective cities. What makes it unique is its zero-commission model enabling retailers to pay only a fixed amount of monthly rental instead of paying commissions. With its one-of-a-kind features, DMA is expected to bring\n",
    "never-seen transformation to the current e-commerce ecosystem while addressing all the existing e-commerce worries such as counterfeiting. \"\"\"\n",
    "\n",
    "doc=nlp(article_text)\n",
    "# for ent in doc.ents:\n",
    "#     print([ent.text,ent.label_]) # Mistagged for flipkart, paytm etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data\n",
    "TRAIN_DATA = [\n",
    "              (\"Walmart is a leading e-commerce company\", {\"entities\": [(0, 7, \"ORG\")]}),\n",
    "              (\"I reached Chennai yesterday.\", {\"entities\": [(19, 28, \"GPE\")]}),\n",
    "              (\"I recently ordered a book from Amazon\", {\"entities\": [(24,32, \"ORG\")]}),\n",
    "              (\"I was driving a BMW\", {\"entities\": [(16,19, \"PRODUCT\")]}),\n",
    "              (\"I ordered this from ShopClues\", {\"entities\": [(20,29, \"ORG\")]}),\n",
    "              (\"Fridge can be ordered in Amazon \", {\"entities\": [(0,6, \"PRODUCT\")]}),\n",
    "              (\"I bought a new Washer\", {\"entities\": [(16,22, \"PRODUCT\")]}),\n",
    "              (\"I bought a old table\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
    "              (\"I bought a fancy dress\", {\"entities\": [(18,23, \"PRODUCT\")]}),\n",
    "              (\"I rented a camera\", {\"entities\": [(12,18, \"PRODUCT\")]}),\n",
    "              (\"I rented a tent for our trip\", {\"entities\": [(12,16, \"PRODUCT\")]}),\n",
    "              (\"I rented a screwdriver from our neighbour\", {\"entities\": [(12,22, \"PRODUCT\")]}),\n",
    "              (\"I repaired my computer\", {\"entities\": [(15,23, \"PRODUCT\")]}),\n",
    "              (\"I got my clock fixed\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
    "              (\"I got my truck fixed\", {\"entities\": [(16,21, \"PRODUCT\")]}),\n",
    "              (\"Flipkart started it's journey from zero\", {\"entities\": [(0,8, \"ORG\")]}),\n",
    "              (\"I recently ordered from Max\", {\"entities\": [(24,27, \"ORG\")]}),\n",
    "              (\"Flipkart is recognized as leader in market\",{\"entities\": [(0,8, \"ORG\")]}),\n",
    "              (\"I recently ordered from Swiggy\", {\"entities\": [(24,29, \"ORG\")]})\n",
    "              ]\n",
    "\n",
    "# Adding labels to the `ner`\n",
    "\n",
    "for _, annotations in TRAIN_DATA:\n",
    "    for ent in annotations.get(\"entities\"):\n",
    "        ner.add_label(ent[2])\n",
    "        \n",
    "# Disable pipeline components you dont need to change\n",
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "unaffected_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DATE  |  2016\n",
      "WORK_OF_ART  |  Rafał Kuć - Running High Performance\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "[E973] Unexpected type for NER data",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-f2b77d87331d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mtext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mannotations\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mDATA\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m         \u001b[0mexample\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mExample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfrom_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"entities\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mannotations\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m         \u001b[0mnlp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mexample\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msgd\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;31m#         nlp.update([text], [annotations], sgd=optimizer)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\training\\example.pyx\u001b[0m in \u001b[0;36mspacy.training.example.Example.from_dict\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\training\\example.pyx\u001b[0m in \u001b[0;36mspacy.training.example.annotations_to_doc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\spacy\\training\\example.pyx\u001b[0m in \u001b[0;36mspacy.training.example._add_entities_to_doc\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: [E973] Unexpected type for NER data"
     ]
    }
   ],
   "source": [
    "from spacy.tokens import Doc\n",
    "from spacy.training import Example\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u\"#bbuzz 2016: Rafał Kuć - Running High Performance And Fault Tolerant Elasticsearch\")\n",
    "for entity in doc.ents:\n",
    "    print(entity.label_, ' | ', entity.text)\n",
    "    \n",
    "#  #bbuzz isn’t money. And I doubt that Rafał was Running High while giving that presentation\n",
    "\n",
    "nlp = spacy.blank('en') # new, empty model. Let's say it's for the English language\n",
    "\n",
    "nlp.vocab.vectors.name = 'example_model_training' # give a name to our list of vectors\n",
    "\n",
    "# add NER pipeline\n",
    "ner = nlp.create_pipe('ner') # our pipeline would just do NER\n",
    "\n",
    "nlp.add_pipe('ner', last=True) # we add the pipeline to the model\n",
    "\n",
    "DATA = [\n",
    "  (u\"Search Analytics: Business Value & BigData NoSQL Backend, Otis Gospodnetic \", {'entities': [(58,75,'PERSON')]}),\n",
    "  (u\"Introduction to Elasticsearch by Radu \", {'entities': [(16,29,'TECH'), (32, 36, 'PERSON')]}),\n",
    "  # ...\n",
    "]\n",
    "\n",
    "ner.add_label('PERSON')\n",
    "ner.add_label('TECH')\n",
    "# # ...\n",
    "\n",
    "optimizer = nlp.begin_training()\n",
    "\n",
    "# nlp.update([text], [annotations], sgd = optimizer)\n",
    "\n",
    "for i in range(20):\n",
    "    random.shuffle(DATA)\n",
    "    for text, annotations in DATA:\n",
    "        doc = nlp.make_doc(text)\n",
    "        example = Example.from_dict(doc, {\"entities\": annotations})\n",
    "        nlp.update([example], sgd=optimizer)\n",
    "#         nlp.update([text], [annotations], sgd=optimizer)\n",
    "        \n",
    "\n",
    "doc = nlp(u\"#bbuzz 2016: Rafał Kuć - Running High Performance And Fault Tolerant Elasticsearch\")\n",
    "for entity in doc.ents:\n",
    "    print(entity.label_, ' | ', entity.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
