{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2017/06/word-embeddings-count-word2veec/\n",
    "\n",
    "https://www.machinelearningplus.com/spacy-tutorial-nlp/\n",
    "\n",
    "https://www.guru99.com/word-embedding-word2vec.html\n",
    "\n",
    "https://towardsdatascience.com/introduction-to-word-embedding-and-word2vec-652d0c2060fa\n",
    "\n",
    "https://towardsdatascience.com/word-embedding-with-word2vec-and-fasttext-a209c1d3e12c\n",
    "\n",
    "https://towardsdatascience.com/a-beginners-guide-to-word-embedding-with-gensim-word2vec-model-5970fa56cc92 -- use case\n",
    "\n",
    "https://towardsdatascience.com/text-classification-with-nlp-tf-idf-vs-word2vec-vs-bert-41ff868d1794 -- usecase\n",
    "\n",
    "https://www.kaggle.com/ananyabioinfo/text-classification-using-word2vec -- best usecase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word Embedding is a type of word representation that allows words with similar meaning to be understood by machine learning algorithms. Technically speaking, it is a mapping of words into vectors of real numbers using the neural network, probabilistic model, or dimension reduction on word co-occurrence matrix. There are various word embedding models available such as word2vec (Google), Glove (Stanford) and fastest (Facebook).\n",
    "\n",
    "Word Embedding is also called as distributed semantic model or distributed represented or semantic vector space or vector space model. As you read these names, you come across the word semantic which means categorizing similar words together. For example fruits like apple, mango, banana should be placed close whereas books will be far away from these words\n",
    "\n",
    "In very simplistic terms, Word Embeddings are the texts converted into numbers and there may be different numerical representations of the same text\n",
    "\n",
    "The different types of word embeddings can be broadly classified into two categories-\n",
    "\n",
    "Frequency based Embedding - Latent Semetic analyisis like Countvec and TFIDF\n",
    "\n",
    "Prediction based Embedding - Word2vec\n",
    "\n",
    "word2vec is a two-layer network where there is input one hidden layer and output.\n",
    "\n",
    "Word embedding helps in Compute similar words, Create a group of related words, feature generation, document clustering, text classification, and natural language processing tasks\n",
    "\n",
    "Word embedding is used to suggest similar words to the word being subjected to the prediction model. Along with that it also suggests dissimilar words, as well as most common words.\n",
    "\n",
    "It is used for semantic grouping which will group things of similar characteristic together and dissimilar far away.\n",
    "\n",
    "Word2vec learns word by predicting its surrounding context. For example, let us take the word \"He loves Football.\"Word2vec reconstructs the linguistic context of words. Before going further let us understand, what is linguistic context? In general life scenario when we speak or write to communicate, other people try to figure out what is objective of the sentence. For example, \"What is the temperature of India\", here the context is the user wants to know \"temperature of India\" which is context. In short, the main objective of a sentence is context. Word or sentence surrounding spoken or written language (disclosure) helps in determining the meaning of context. Word2vec learns vector representation of words through the contexts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec learns word by predicting its surrounding context. For example, let us take the word \"He loves Football.\"\n",
    "\n",
    "loves =  Vin. P(Vout / Vin) is calculated\t\n",
    "where,\t\n",
    "Vin is the input word. \t\n",
    "P is the probability of likelihood.\t\n",
    "Vout is the output word. \n",
    "\n",
    "Word loves moves over each word in the corpus. Syntactic as well as the Semantic relationship between words is encoded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2vec Architecture\n",
    "There are two architectures used by word2vec\n",
    "\n",
    "Continuous Bag of words (CBOW)\n",
    "skip gram\n",
    "\n",
    "Skip-gram and CBOW convert unsupervised representation to supervised form for model training.\n",
    "\n",
    "In CBOW, the current word is predicted using the window of surrounding context windows. For example, if wi-1,wi-2,wi+1,wi+2are given words or context, this model will provide wi\n",
    "\n",
    "Skip-Gram performs opposite of CBOW which implies that it predicts the given sequence or context from the word. You can reverse the example to understand it. If wi is given, this will predict the context or wi-1,wi-2,wi+1,wi+2.\n",
    "\n",
    "Word2vec provides an option to choose between CBOW (continuous Bag of words) and skip-gram. Such parameters are provided during training of the model. One can have the option of using negative sampling or hierarchical softmax layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CBOW is several times faster than skip gram and provides a better frequency for frequent words whereas skip gram needs a small amount of training data and represents even rare words or phrases.\n",
    "\n",
    "If no activation function is used output would be linear but the functionality of linear function is limited. To achieve complex functionality such as object detection, image classification, typing text using voice and many other non-linear outputs is needed which is achieved using activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If speaking in a general sense for word embedding options available are Differentiated Softmax, CNN-Softmax, Importance Sampling, Adaptive Importance sampling, Noise Contrastive Estimations, Negative Sampling, Self-Normalization, and infrequent Normalization.\n",
    "\n",
    "Speaking specifically about Word2vec we have negative sampling available.\n",
    "\n",
    "Negative Sampling is a way to sample the training data. It is somewhat like stochastic gradient descent, but with some difference. Negative sampling looks only for negative training examples. It is based on noise contrastive estimation and randomly samples words, not in the context. It is a fast training method and chooses the context randomly. If the predicted word appears in the randomly chosen context both the vectors are close to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import nltk\n",
    "nltk.download()\n",
    "import gensim\n",
    "from nltk.corpus import abc\n",
    "\n",
    "model= gensim.models.Word2Vec(abc.sents()) \n",
    "\n",
    "X= list(model.wv.vocab)          \n",
    "print(X[:10])\n",
    "\n",
    "data=model.most_similar('science')          \n",
    "\n",
    "print(data)\n",
    "\n",
    "\"\"\"[('law', 0.9415997266769409), ('practice', 0.9276568293571472), ('discussion', 0.9259148836135864), ('agriculture', 0.9257254004478455), ('media', 0.9232194423675537), ('policy', 0.922248125076294), ('general', 0.9166069030761719), ('undertaking', 0.916458249092102), ('tight', 0.9129181504249573), ('board', 0.9107444286346436)]\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The biggest challenge is that it is not able to represent words that do not appear in the training dataset. Even though using a larger training set that contains more vocabulary, some rare words used very seldom can never be mapped to vectors.\n",
    "\n",
    "FastText is an extension to Word2Vec proposed by Facebook in 2016. Instead of feeding individual words into the Neural Network, FastText breaks words into several n-grams (sub-words). For instance, the tri-grams for the word apple is app, ppl, and ple (ignoring the starting and ending of boundaries of words). \n",
    "\n",
    "from gensim.models import FastText\n",
    "\n",
    "model_ted = FastText(sentences_ted, size=100, window=5, min_count=5, workers=4,sg=1)\n",
    "\n",
    "\n",
    "model_ted.wv.most_similar(\"Gastroenteritis\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "similar_words = model.most_similar('thanks')\n",
    "\n",
    "dissimlar_words = model.doesnt_match('See you later, thanks for visiting'.split())\n",
    "\n",
    "similarity_two_words = model.similarity('please','see')\n",
    "\n",
    "print(\"Please provide the similarity between these two words:\")\n",
    "\n",
    "print(similarity_two_words)\n",
    "\n",
    "similar = model.similar_by_word('kind')\n",
    "\n",
    "from gensim.matutils import softcossim\n",
    "\n",
    "from gensim import corpora\n",
    "\n",
    "\n",
    "# Prepare the similarity matrix\n",
    "similarity_matrix = fasttext_model300.similarity_matrix(dictionary, tfidf=None, threshold=0.0, exponent=2.0, nonzero_limit=100)\n",
    "\n",
    "\n",
    "# Compute soft cosine similarity\n",
    "print(softcossim(sent_1, sent_2, similarity_matrix))\n",
    "#> 0.7868705819999783\n",
    "\n",
    "from gensim.summarization import summarize, keywords\n",
    "from pprint import pprint\n",
    "\n",
    "# Summarize the paragraph\n",
    "pprint(summarize(text, word_count=20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-30-4e783bd9bc96>:17: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  model['topic']\n",
      "<ipython-input-30-4e783bd9bc96>:19: DeprecationWarning: Call to deprecated `most_similar` (Method will be removed in 4.0.0, use self.wv.most_similar() instead).\n",
      "  model.most_similar('topic')\n",
      "<ipython-input-30-4e783bd9bc96>:27: DeprecationWarning: Call to deprecated `iter` (Attribute will be removed in 4.0.0, use self.epochs instead).\n",
      "  model.train(data_part2, total_examples=model.corpus_count, epochs=model.iter)\n",
      "<ipython-input-30-4e783bd9bc96>:28: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  model['topic']\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 3.35 GiB for an array with shape (3000000, 300) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-4e783bd9bc96>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;31m# Get word embeddings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m \u001b[0mword2vec_model300\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'support'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m \u001b[1;31m# # Word2ec_accuracy\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[1;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    529\u001b[0m             \u001b[0mnegative\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 531\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minit_sims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    533\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mnegative\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36minit_sims\u001b[1;34m(self, replace)\u001b[0m\n\u001b[0;32m   1352\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'vectors_norm'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreplace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1353\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"precomputing L2-norms of word weight vectors\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1354\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors_norm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_l2_norm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreplace\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1355\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1356\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrelative_cosine_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwa\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m_l2_norm\u001b[1;34m(m, replace)\u001b[0m\n\u001b[0;32m   2387\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mm\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2388\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2389\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mm\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mdist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mREAL\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2390\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2391\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 3.35 GiB for an array with shape (3000000, 300) and data type float32"
     ]
    }
   ],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from multiprocessing import cpu_count\n",
    "import gensim.downloader as api\n",
    "\n",
    "# Download dataset\n",
    "dataset = api.load(\"text8\")\n",
    "data = [d for d in dataset]\n",
    "\n",
    "# Split the data into 2 parts. Part 2 will be used later to update the model\n",
    "data_part1 = data[:1000]\n",
    "data_part2 = data[1000:]\n",
    "\n",
    "# Train Word2Vec model. Defaults result vector size = 100\n",
    "model = Word2Vec(data_part1, min_count = 0, workers=cpu_count())\n",
    "\n",
    "# Get the word vector for given word\n",
    "model['topic']\n",
    "\n",
    "model.most_similar('topic')\n",
    "\n",
    "# Save and Load Model\n",
    "model.save('newmodel')\n",
    "model = Word2Vec.load('newmodel')\n",
    "\n",
    "# Update the model with new data.\n",
    "model.build_vocab(data_part2, update=True)\n",
    "model.train(data_part2, total_examples=model.corpus_count, epochs=model.iter)\n",
    "model['topic']\n",
    "\n",
    "# Download the models\n",
    "fasttext_model300 = api.load('fasttext-wiki-news-subwords-300')\n",
    "# word2vec_model300 = api.load('word2vec-google-news-300')\n",
    "# glove_model300 = api.load('glove-wiki-gigaword-300')\n",
    "\n",
    "# Get word embeddings\n",
    "word2vec_model300.most_similar('support')\n",
    "\n",
    "# # Word2ec_accuracy\n",
    "# word2vec_model300.evaluate_word_analogies(analogies=\"questions-words.txt\")[0]\n",
    "# #> 0.7401448525607863\n",
    "\n",
    "# # fasttext_accuracy\n",
    "# fasttext_model300.evaluate_word_analogies(analogies=\"questions-words.txt\")[0]\n",
    "# #> 0.8827876424099353\n",
    "\n",
    "# # GloVe accuracy\n",
    "# glove_model300.evaluate_word_analogies(analogies=\"questions-words.txt\")[0]\n",
    "# #> 0.7195422354510931"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the tagged document needed for Doc2Vec\n",
    "def create_tagged_document(list_of_list_of_words):\n",
    "    for i, list_of_words in enumerate(list_of_list_of_words):\n",
    "        yield gensim.models.doc2vec.TaggedDocument(list_of_words, [i])\n",
    "\n",
    "train_data = list(create_tagged_document(data))\n",
    "\n",
    "print(train_data[:1])\n",
    "#> [TaggedDocument(words=['anarchism', 'originated', ... 'social', 'or', 'emotional'], tags=[0])]\n",
    "\n",
    "# Init the Doc2Vec model\n",
    "model = gensim.models.doc2vec.Doc2Vec(vector_size=50, min_count=2, epochs=40)\n",
    "\n",
    "# Build the Volabulary\n",
    "model.build_vocab(train_data)\n",
    "\n",
    "# Train the Doc2Vec model\n",
    "model.train(train_data, total_examples=model.corpus_count, epochs=model.epochs)\n",
    "\n",
    "print(model.infer_vector(['australian', 'captain', 'elected', 'to', 'bowl']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Gradient Boosting on Decision Trees (GBDT) is a state-of-the-art Machine '\n",
      " 'Learning tool for working with heterogeneous or structured data.')\n",
      "data\n",
      "scoring\n",
      "trees\n",
      "learning\n",
      "depends\n"
     ]
    }
   ],
   "source": [
    "from gensim.summarization import summarize, keywords\n",
    "from pprint import pprint\n",
    "\n",
    "text = \"\"\"Gradient Boosting on Decision Trees (GBDT) is a state-of-the-art Machine Learning tool for working with heterogeneous or structured data. When working with data, the choice of the perfect algorithm depends highly on the type of data. For homogeneous data, like images, sound or text, the best solution is neural networks. And for structured data, for example for credit scoring, recommendations or any other tabled data the best solution is GBDT.\"\"\"\n",
    "\n",
    "# Summarize the paragraph\n",
    "pprint(summarize(text, word_count=20))\n",
    "\n",
    "# Important keywords from the paragraph\n",
    "print(keywords(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TextBlob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2018/02/natural-language-processing-for-beginners-using-textblob/\n",
    "    \n",
    "Textblob works as a string also. It will used mainly for sentiment analysis, spelling correction and language detection which uses google API.\n",
    "\n",
    "Spacy > TextBlob > NLTK\n",
    "\n",
    "It does not provide features like dependency parsing, word vectors etc. which is provided by spacy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rim\n",
      "RIMANSHU\n",
      "[Sentence(\"Analytics Vidhya is a great platform to learn data science.\"), Sentence(\"It helps community through blogs, hackathons,discussions,etc.\")]\n",
      "\n",
      "\n",
      "['Analytics', 'Vidhya']\n",
      "['Vidhya', 'is']\n",
      "['is', 'a']\n",
      "['a', 'greeeeat']\n",
      "['greeeeat', 'platform']\n",
      "['platform', 'to']\n",
      "['to', 'learn']\n",
      "['learn', 'data']\n",
      "['data', 'science']\n",
      "Sentiment(polarity=0.0, subjectivity=0.0)\n",
      "Analytics Vidhya is a greeeeat platform to learn data science.\n",
      "[('greeeeat', 0.0)]\n",
      "ar\n",
      "Please find the attached transfer invoice\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "string1=TextBlob(\"Rimanshu\")\n",
    "print(string1[:3])\n",
    "print(string1.upper())\n",
    "blob = TextBlob(\"Analytics Vidhya is a great platform to learn data science. \\n It helps community through blogs, hackathons,discussions,etc.\")\n",
    "print(blob.sentences)\n",
    "for k in blob.sentences[0].words:\n",
    "    pass\n",
    "#     print(k)\n",
    "\n",
    "#Noun phrase extraction:-\n",
    "print(\"\\n\")\n",
    "blob = TextBlob(\"Analytics Vidhya is a great platform to learn data science.\")\n",
    "for np in blob.noun_phrases:\n",
    "    pass\n",
    "#     print(np)\n",
    "\n",
    "# Part of speach tagging\n",
    "for word, tag in blob.tags:\n",
    "    pass\n",
    "#     print(word,tag)\n",
    "    \n",
    "# Singularize and pularize tagging\n",
    "for word in blob.sentences[0].words:\n",
    "    pass\n",
    "#     print(word.singularize(), word.pluralize(), word.lemmatize('v'))\n",
    "\n",
    "#TO get ngrams\n",
    "for word in blob.ngrams(2):\n",
    "    print(word)\n",
    "    \n",
    "print(blob.sentiment)\n",
    "print(blob.correct())\n",
    "print(blob.words[4].spellcheck())\n",
    "print(TextBlob('رجاء').detect_language())\n",
    "print(TextBlob('تجدون فاتورة التحويل المرفقة').translate(to ='en'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zh-CN\n",
      "en\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "str1= \"\"\"Моля, продължете с това искане за онлайн обработка\n",
    "\n",
    "** **\n",
    "\n",
    "account\n",
    "\n",
    "|\n",
    "\n",
    "factuur / credit\n",
    "\n",
    "|\n",
    "\n",
    "valuta\n",
    "\n",
    "|\n",
    "\n",
    "netto aanvraagbedrag\n",
    "\n",
    "|\n",
    "\n",
    "klantnaam\n",
    "\n",
    "|\n",
    "\n",
    "betalingsnummer\n",
    "\n",
    "|\n",
    "\n",
    "bedrijfscode uit e-mail\n",
    "\n",
    "|\n",
    "\n",
    "new field 1\n",
    "\n",
    "---|---|---|---|---|---|---|---\n",
    "\n",
    "CC213425\n",
    "\n",
    "|\n",
    "\n",
    "_[56127440](https://cora-managed-qa-beta-dev-\n",
    "ed.lightning.force.com/lightning/r/a0n6g000005kElmAAE/view)_\n",
    "\n",
    "|\n",
    "\n",
    "USD\n",
    "\n",
    "|\n",
    "\n",
    "89\n",
    "\n",
    "|\n",
    "\n",
    "sbi\n",
    "\n",
    "|\n",
    "\n",
    "Pay 0001\n",
    "\n",
    "|\n",
    "\n",
    "code11\n",
    "\n",
    "|\n",
    "\n",
    "test 1\n",
    "\n",
    "CC213426\n",
    "\n",
    "|\n",
    "\n",
    "_[81000104](https://cora-managed-qa-beta-dev-\n",
    "ed.lightning.force.com/lightning/r/a0n6g000005eIELAA2/view)_\n",
    "\n",
    "|\n",
    "\n",
    "USD\n",
    "\n",
    "|\n",
    "\n",
    "-89\n",
    "\n",
    "|\n",
    "\n",
    "axis\n",
    "\n",
    "|\n",
    "\n",
    "PAY 0002\n",
    "\n",
    "|\n",
    "\n",
    "code 123##23\n",
    "\n",
    "|\n",
    "\n",
    "test 2\n",
    "\n",
    "** **\n",
    "\n",
    "\n",
    "______________________________________________________________________\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "notify the sender immediately by e-mail/phone  & delete it from their system\n",
    "\n",
    "\"\"\"\n",
    "print(TextBlob(str12).detect_language())\n",
    "\n",
    "from langdetect import detect\n",
    "\n",
    "lang_detected = detect(str12)\n",
    "print(lang_detected)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "['ab']\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "# import statistics\n",
    "list1= [1,2,2,3]\n",
    "ddd = [\"ab\",\"ab\"]\n",
    "# test_list = Counter(list1) \n",
    "# res = test_list.most_common(1)[0][0] \n",
    "# res = statistics.mode(test_list) \n",
    "res = max(set(list1), key = list1.count) \n",
    "print(res)\n",
    "print(list(set(ddd)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "Most Informative Features\n",
      "            contains(is) = True              neg : pos    =      2.9 : 1.0\n",
      "         contains(never) = False             neg : pos    =      1.8 : 1.0\n",
      "             contains(a) = False             neg : pos    =      1.8 : 1.0\n",
      "None\n",
      "neg\n"
     ]
    }
   ],
   "source": [
    "# for text classification\n",
    "\n",
    "training = [\n",
    "('Tom Holland is a terrible spiderman.','pos'),\n",
    "('a terrible Javert (Russell Crowe) ruined Les Miserables for me...','pos'),\n",
    "('The Dark Knight Rises is the greatest superhero movie ever!','neg'),\n",
    "('Fantastic Four should have never been made.','pos'),\n",
    "('Wes Anderson is my favorite director!','neg'),\n",
    "('Captain America 2 is pretty awesome.','neg'),\n",
    "('Let\\s pretend \"Batman and Robin\" never happened..','pos'),\n",
    "]\n",
    "testing = [\n",
    "('Superman was never an interesting character.','pos'),\n",
    "('Fantastic Mr Fox is an awesome film!','neg'),\n",
    "('Dragonball Evolution is simply terrible!!','pos')\n",
    "]\n",
    "\n",
    "from textblob import classifiers\n",
    "\n",
    "model = classifiers.NaiveBayesClassifier(training)\n",
    "## decision tree classifier\n",
    "dt_classifier = classifiers.DecisionTreeClassifier(training)\n",
    "\n",
    "print(model.accuracy(testing))\n",
    "print(model.show_informative_features(3))\n",
    "\n",
    "blob = TextBlob('the weather is terrible!', classifier=model)\n",
    "print (blob.classify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp=spacy.load('en_core_web_sm')\n",
    "\n",
    "# Spacy matcher is required when you want to extract electronic engineering words in which we gave pattern like \n",
    "\n",
    "# {\"POS\": {\"IN\": [\"NOUN\", \"ADJ\"]}}, {\"LOWER\": \"engineering\"}\n",
    "\n",
    "# if we want to extract visiting manali words then pattern would be {\"LEMMA\": \"visit\"}, {\"POS\": \"PROPN\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using the attr parameter as 'LOWER'\n",
    "case_insensitive_matcher = PhraseMatcher(nlp.vocab, attr=\"LOWER\")# attr = SHAPE also we can use for pincode match and all.\n",
    "\n",
    "# Creating doc &amp; pattern\n",
    "my_doc=nlp('I wish to visit new york city')\n",
    "terms=['New York']\n",
    "pattern=[nlp(term) for term in terms]\n",
    "\n",
    "# adding pattern to the matcher\n",
    "case_insensitive_matcher.add(\"matcher\",None,*pattern)\n",
    "\n",
    "# applying matcher to the doc\n",
    "my_matches=case_insensitive_matcher(my_doc)\n",
    "\n",
    "for match_id,start,end in my_matches:\n",
    "  span=my_doc[start:end]\n",
    "  print(span.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(American, 'NORP', 'Nationalities or religious or political groups'),\n",
       " (44th, 'ORDINAL', '\"first\", \"second\", etc.'),\n",
       " (the United States, 'GPE', 'Countries, cities, states'),\n",
       " (2009, 'DATE', 'Absolute or relative dates or periods'),\n",
       " (2017, 'DATE', 'Absolute or relative dates or periods'),\n",
       " (first, 'ORDINAL', '\"first\", \"second\", etc.'),\n",
       " (African American, 'NORP', 'Nationalities or religious or political groups'),\n",
       " (first, 'ORDINAL', '\"first\", \"second\", etc.'),\n",
       " (United States, 'GPE', 'Countries, cities, states')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_obama = \"\"\"Barack Obama is an American politician who served as ...: the 44th President of the United States from 2009 to 2017. He is the first ...: African American to have served as president, ...: as well as the first born outside the contiguous United States.\"\"\" \n",
    "nlp_obama = nlp(wiki_obama)\n",
    "# [(i, i.label_, str(spacy.explain(i.label_))) for i in nlp_obama.ents]\n",
    "[(i, i.label_, str(spacy.explain(i.label_))) for i in nlp_obama.ents]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HRoV-3ry6gTA",
    "outputId": "e986adf2-8847-4f0f-bb97-06ec1cd616e6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "# https://www.analyticsvidhya.com/blog/2020/03/spacy-tutorial-learn-natural-language-processing/\n",
    "# functiona in spacy - IS_PUNCT, IS_SPACE, IS_STOP, IS_LOWER, IS_UPPER, IS_TITLE,  LIKE_NUM, LIKE_URL, LIKE_EMAIL,POS, TAG, DEP, LEMMA, SHAPE\t, noun_chunks\n",
    "import spacy\n",
    "from spacy import displacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "doc = nlp(u'Tesla is looking for buying U.S based startup for $6 million. Us will thin about it.')\n",
    "# for token in doc:\n",
    "#   print(token.text, token.pos, token.pos_, token.dep, token.dep_) # without _ will give numbers. \n",
    "nlp.pipeline\n",
    "nlp.pipe_names #['tagger', 'parser', 'ner']\n",
    "nlp.disable_pipes('tagger', 'parser')\n",
    "nlp.pipe_name\n",
    "#nlp.pipe_labels\n",
    "doc.text.split()\n",
    "[token.orth_ for token in doc] # punctuation\n",
    "[token.orth_ for token in doc if not token.is_punct | token.is_space] \n",
    "spacy.explain(\"PART\") # explain about any tag\n",
    "practice = \"practice practiced practicing\" \n",
    "[word.lemma_ for word in nlp(practice)]\n",
    "[(i.pos_, i.tag_) for i in doc]\n",
    "[(i.lemma, i.lemma_) for i in doc]\n",
    "displacy.render(doc, style= 'dep', jupyter=True, options = {'distance': 100}) # display the dependency graph\n",
    "displacy.render(doc, style= 'ent', jupyter=True, options = {'distance': 100}) # display the entity graph\n",
    "displacy.serve(doc, style ='ent') # to display at eh local server. options = {'ents': ['PRODUCT','ORG'], 'colors': {'ORG': 'red'}}\n",
    "wiki_obama = \"\"\"Barack Obama is an American politician who served as ...: the 44th President of the United States from 2009 to 2017. He is the first ...: African American to have served as president, ...: as well as the first born outside the contiguous United States.\"\"\" \n",
    "nlp_obama = nlp(wiki_obama)\n",
    "[(i, i.label_, str(spacy.explain(i.label_))) for i in nlp_obama.ents]\n",
    "\n",
    "doc1 = nlp(\"How do I turn sound on/off?\")\n",
    "doc2 = nlp(\"How do I obtain a pet?\")\n",
    "doc1.similarity(doc2)\n",
    "\n",
    "Play with default words \n",
    "print(nlp.Defaults.stop_words) # for stop words.\n",
    "print(nlp.vocab['is'].is_stop) #to see for particular word is stop word or not.\n",
    "nlp.Defaults.stop_words.add('abcdddd')\n",
    "print(nlp.vocab['abcdddd'].is_stop) # true\n",
    "nlp.Defaults.stop_words.remove('abcdddd')\n",
    "print(nlp.vocab['abcdddd'].is_stop)\n",
    "\n",
    "# Checking if POS tag is X and printing them\n",
    "print('The junk values are..')\n",
    "for token in raw_doc:\n",
    "  if token.pos_=='X':\n",
    "    print(token.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp(u'lion').vector.shape # 300\n",
    "nlp(u'This quick brown fox jumped over').vector.shape# 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = nlp(u'lion cat pet')\n",
    "for token1 in tokens:\n",
    "    for token2 in token1:\n",
    "        print(token1.text, token2.text, token1.similarity(token2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(nlp.vocab.vectors) # 684831 words are there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in tokens:\n",
    "    print(token.text, token.has_vector, token.vector_norm, token.is_oov)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary(33 unique tokens: ['Saudis', 'The', 'a', 'acknowledge', 'are']...)\n",
      "[['who', 'let', 'the', 'dogs', 'out'], ['who', 'who', 'who', 'who']]\n",
      "[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)], [(4, 4)]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('infosystems', 0.712022066116333),\n",
       " ('wmx', 0.6928527355194092),\n",
       " ('i2', 0.6768453121185303),\n",
       " ('vedanta', 0.6683526635169983),\n",
       " ('ascorbic', 0.6538587808609009),\n",
       " ('erp', 0.6521557569503784),\n",
       " ('wickremasinghe', 0.6427202224731445),\n",
       " ('indicom', 0.6379734873771667),\n",
       " ('bhp', 0.6362248659133911),\n",
       " ('pvt', 0.6345576643943787)]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim import corpora\n",
    "from pprint import pprint\n",
    "from gensim.utils import simple_preprocess\n",
    "import gensim.downloader as api\n",
    "\n",
    "# How to create a dictionary from a list of sentences?\n",
    "documents = [\"The Saudis are preparing a report that will acknowledge that\", \n",
    "             \"Saudi journalist Jamal Khashoggi's death was the result of an\", \n",
    "             \"interrogation that went wrong, one that was intended to lead\", \n",
    "             \"to his abduction from Turkey, according to two sources.\"]\n",
    "\n",
    "# Tokenize(split) the sentences into words\n",
    "texts = [[text for text in doc.split()] for doc in documents]\n",
    "\n",
    "# Create dictionary\n",
    "dictionary = corpora.Dictionary(texts)\n",
    "\n",
    "print(dictionary)\n",
    "#> Dictionary(33 unique tokens: ['Saudis', 'The', 'a', 'acknowledge', 'are']...)\n",
    "\n",
    "# List with 2 sentences\n",
    "my_docs = [\"Who let the dogs out?\",\n",
    "           \"Who? Who? Who? Who?\"]\n",
    "\n",
    "# Tokenize the docs\n",
    "tokenized_list = [simple_preprocess(doc) for doc in my_docs]\n",
    "print(tokenized_list)\n",
    "\n",
    "# Create the Corpus\n",
    "mydict = corpora.Dictionary()\n",
    "mycorpus = [mydict.doc2bow(doc, allow_update=True) for doc in tokenized_list]\n",
    "pprint(mycorpus)\n",
    "#> [[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1)], [(4, 4)]]\n",
    "\n",
    "api.info('glove-wiki-gigaword-50')\n",
    "w2v_model = api.load(\"glove-wiki-gigaword-50\")\n",
    "w2v_model.most_similar('hcl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n",
      "['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used', 'against']\n"
     ]
    }
   ],
   "source": [
    "dataset = api.load(\"text8\")\n",
    "dataset = [wd for wd in dataset]\n",
    "\n",
    "dct = corpora.Dictionary(dataset)\n",
    "corpus = [dct.doc2bow(line) for line in dataset]\n",
    "\n",
    "# Build the bigram models\n",
    "bigram = gensim.models.phrases.Phrases(dataset, min_count=3, threshold=10)\n",
    "\n",
    "# Construct bigram\n",
    "print(bigram[dataset[0][:10]])\n",
    "# ['anarchism', 'originated', 'as', 'a', 'term', 'of', 'abuse', 'first', 'used',\n",
    "#  'against', 'early', 'working_class', 'radicals', 'including', 'the', 'diggers',\n",
    "#  'of', 'the', 'english', 'revolution', 'and', 'the', 'sans_culottes', 'of', 'the',\n",
    "#  'french_revolution', 'whilst',...]\n",
    "\n",
    "# Build the trigram models\n",
    "trigram = gensim.models.phrases.Phrases(bigram[dataset], threshold=10)\n",
    "\n",
    "# Construct trigram\n",
    "print(trigram[bigram[dataset[0][:10]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosine Similarity\n",
    "\n",
    "from scipy import spartial\n",
    "\n",
    "cosin_similarity = lambda vec1, vec2 : 1- spartial.distance.cosine(vec1,vec2)\n",
    "\n",
    "king = nlp.vocab['king'].vector\n",
    "man = nlp.vocab['man'].vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.machinelearningplus.com/nlp/cosine-similarity/\n",
    "# https://stackoverflow.com/questions/18424228/cosine-similarity-between-2-number-lists\n",
    "# Define the documents\n",
    "doc_trump = \"Mr. Trump became president after winning the political election. Though he lost the support of some republican friends, Trump is friends with President Putin\"\n",
    "\n",
    "doc_election = \"President Trump says Putin had no political interference is the election outcome. He says it was a witchhunt by political parties. He claimed President Putin is a friend who had nothing to do with the election\"\n",
    "\n",
    "doc_putin = \"Post elections, Vladimir Putin became President of Russia. President Putin had served as the Prime Minister earlier in his political career\"\n",
    "\n",
    "documents = [doc_trump, doc_election, doc_putin]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>after</th>\n",
       "      <th>as</th>\n",
       "      <th>became</th>\n",
       "      <th>by</th>\n",
       "      <th>career</th>\n",
       "      <th>claimed</th>\n",
       "      <th>do</th>\n",
       "      <th>earlier</th>\n",
       "      <th>election</th>\n",
       "      <th>elections</th>\n",
       "      <th>...</th>\n",
       "      <th>the</th>\n",
       "      <th>though</th>\n",
       "      <th>to</th>\n",
       "      <th>trump</th>\n",
       "      <th>vladimir</th>\n",
       "      <th>was</th>\n",
       "      <th>who</th>\n",
       "      <th>winning</th>\n",
       "      <th>witchhunt</th>\n",
       "      <th>with</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc_trump</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_election</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc_putin</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3 rows × 48 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              after  as  became  by  career  claimed  do  earlier  election  \\\n",
       "doc_trump         1   0       1   0       0        0   0        0         1   \n",
       "doc_election      0   0       0   1       0        1   1        0         2   \n",
       "doc_putin         0   1       1   0       1        0   0        1         0   \n",
       "\n",
       "              elections  ...  the  though  to  trump  vladimir  was  who  \\\n",
       "doc_trump             0  ...    2       1   0      2         0    0    0   \n",
       "doc_election          0  ...    2       0   1      1         0    1    1   \n",
       "doc_putin             1  ...    1       0   0      0         1    0    0   \n",
       "\n",
       "              winning  witchhunt  with  \n",
       "doc_trump           1          0     1  \n",
       "doc_election        0          1     1  \n",
       "doc_putin           0          0     0  \n",
       "\n",
       "[3 rows x 48 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Scikit Learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import pandas as pd\n",
    "\n",
    "# Create the Document Term Matrix\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "count_vectorizer = CountVectorizer()\n",
    "sparse_matrix = count_vectorizer.fit_transform(documents)\n",
    "\n",
    "# OPTIONAL: Convert Sparse Matrix to Pandas Dataframe if you want to see the word frequencies.\n",
    "doc_term_matrix = sparse_matrix.todense()\n",
    "df = pd.DataFrame(doc_term_matrix, \n",
    "                  columns=count_vectorizer.get_feature_names(), \n",
    "                  index=['doc_trump', 'doc_election', 'doc_putin'])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.51480485 0.38890873]\n",
      " [0.51480485 1.         0.38829014]\n",
      " [0.38890873 0.38829014 1.        ]]\n"
     ]
    }
   ],
   "source": [
    "# Compute Cosine Similarity\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "print(cosine_similarity(df, dfx))\n",
    "#> [[ 1.          0.48927489  0.37139068]\n",
    "#>  [ 0.48927489  1.          0.38829014]\n",
    "#>  [ 0.37139068  0.38829014  1.        ]]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled0.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
