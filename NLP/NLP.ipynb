{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLP or NLTK -- https://www.youtube.com/watch?v=lIajL8_Q-gw&index=3&list=PLcTXcpndN-Sl9eYrKM6jtcOTgC52EJnqH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['My name is Rimanshu.',\n",
       " 'I like nltk.',\n",
       " 'i like python.',\n",
       " 'I need to learn ASAP']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908\n",
    "# Sentance Tokenizer\n",
    "import nltk\n",
    "#nltk.download() - to download\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n",
    "parag=\"My name is Rimanshu. I like nltk. i like python. I need to learn ASAP\"\n",
    "Arr = sent_tokenize(parag)\n",
    "Arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rimanshu\\Anaconda3\\lib\\site-packages\\nltk\\__init__.py\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "print(nltk.__file__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'won', \"'\", 't', 'let', 'you', 'bring', 'cake']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Word Tokenizer\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "Arr=word_tokenize('This is Rimanshu and using PC')\n",
    "Arr[3]\n",
    "\n",
    "#Other way to word tokenizer\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "tok2=TreebankWordTokenizer()\n",
    "from nltk.tokenize import WordPunctTokenizer # Takes out punctuations\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "tok3=WordPunctTokenizer()\n",
    "\n",
    "# All three will give same result for above mentioned string but will behave diff when can't won't words will be there.\n",
    "\n",
    "sent2 = \"I won't let you bring cake\"\n",
    "\n",
    "tok2.tokenize(sent2) #['I', 'wo', \"n't\", 'let', 'you', 'bring', 'cake']. \n",
    "#Tok2 will not direcly call as method like word_tokenize because it is object not method.\n",
    "\n",
    "tok3.tokenize(sent2) #['I', 'won', \"'\", 't', 'let', 'you', 'bring', 'cake']\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Regex Expression for tokenize\n",
    "#[\\w]+- w stands for word and + stands for sequence of words. [\\d]+ d for digit.[\\s+] s means space\n",
    "\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "sent1 = \"i Can't do this. I won't do that\"\n",
    "regexp_tokenize(sent1,\"[\\w']+\") #['i', \"Can't\", 'do', 'this', 'I', \"won't\", 'do', 'that']\n",
    "regexp_tokenize(sent1,\"[\\w]+\") #['i', 'Can', 't', 'do', 'this', 'I', 'won', 't', 'do', 'that']\n",
    "regexp_tokenize(sent1,\"[\\w']\") # Will show every letter.\n",
    "\n",
    "#Another way to tokenize from regex\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "Tok1=RegexpTokenizer(\"[\\w']+\")\n",
    "Tok1.tokenize(sent1)\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer # speacially for tweets and seperates hashtags, mentions and lot of exclaimations points.\n",
    "\n",
    "# Split scene_one into sentences: sentences\n",
    "sentences = sent_tokenize(scene_one)\n",
    "\n",
    "# Use word_tokenize to tokenize the fourth sentence: tokenized_sent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['What', 'right', 'exactly', '?', 'I', 'wanted', 'get', 'office', '.']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Stop words - Doesn't have any meaning.\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import gutenberg # Contains Books,chats etc\n",
    "sample_words=stopwords.words('english')\n",
    "sample_words\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "parag1=\"What are you doing right now exactly? I wanted to get to my office.\"\n",
    "paragArr=word_tokenize(parag1)\n",
    "\n",
    "FilterArr= [item for item in paragArr if item not in sample_words]\n",
    "FilterArr\n",
    "\n",
    "#Or we can write in this way also\n",
    "filtered_sentence = []\n",
    "\n",
    "for item in paragArr:\n",
    "    if item not in sample_words:\n",
    "        filtered_sentence.append(item)\n",
    "\n",
    "print(paragArr)\n",
    "print(filtered_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Synsets (set of synonyms), Hypernyms(Parent name or abstract) and Hyponyms(Child class or more specific)\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "word1=\"weapon\"\n",
    "synArr=wordnet.synsets(word1)\n",
    "synArr #[Synset('weapon.n.01'), Synset('weapon.n.02')] ... n is noun\n",
    "\n",
    "woi=synArr[0]\n",
    "woi # Word of interest\n",
    "woi.definition() #'any instrument or instrumentality used in fighting or hunting'\n",
    "woi.name() #'weapon.n.01'\n",
    "woi.pos() # n result. n means noun and a means adjective. pos means part of speaches\n",
    "\n",
    "woi.hypernyms() #[Synset('instrument.n.01')]\n",
    "woi.hyponyms() # More specific like [Synset('bow.n.04'),Synset('bow_and_arrow.n.01') etc]\n",
    "\n",
    "woi2=woi.hyponyms()[1] #Synset('bow_and_arrow.n.01')\n",
    "woi2.definition() #'a weapon consisting of arrows and the bow to shoot them'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#POS - Part of speach\n",
    "import nltk\n",
    "from nltk.corpus import state_union\n",
    "#One is a State of the Union address from 2005, and the other is from 2006 from past President George W. Bush.\n",
    "from nltk.tokenize import PunktSentenceTokenizer\n",
    "\n",
    "train_text = state_union.raw(\"2005-GWBush.txt\")\n",
    "sample_text = state_union.raw(\"2006-GWBush.txt\")\n",
    "\n",
    "custom_sent_tokenizer = PunktSentenceTokenizer(train_text)\n",
    "tokenized = custom_sent_tokenizer.tokenize(sample_text)\n",
    "\n",
    "def process_content():\n",
    "    try:\n",
    "        for i in tokenized[:5]:\n",
    "            words = nltk.word_tokenize(i)\n",
    "            tagged = nltk.pos_tag(words)\n",
    "            namedEnt = nltk.ne_chunk(tagged, binary=True)\n",
    "            namedEnt.draw()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(str(e))\n",
    "\n",
    "\n",
    "process_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'fail', 'fall_back', 'lose', 'losings'}"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lemmas (Stem words like run for running or ran words), synonyms, Antonyms\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "sArr = wordnet.synsets('win')\n",
    "sArr\n",
    "sArr[2]\n",
    "\n",
    "woi = sArr[2]\n",
    "woi.pos()\n",
    "woi.definition()\n",
    "woi.lemmas() # [Lemma('win.v.01.win')]. You cann have more than one lemmas.\n",
    "\n",
    "#initializing the arrays\n",
    "SynArr =[]\n",
    "AntArr =[]\n",
    "\n",
    "for syn in sArr:\n",
    "    for lem in syn.lemmas():\n",
    "        SynArr.append(lem.name()) # name method will give you the name of lemmas\n",
    "\n",
    "SynArr # Synonyms of the word\n",
    "\n",
    "set(SynArr) # It will convert into object from array. It is set of element\n",
    "\n",
    "len(SynArr)\n",
    "\n",
    "woi.lemmas()[0].antonyms()[0].name()\n",
    "\n",
    "for syn in sArr:\n",
    "    for lem in syn.lemmas():\n",
    "        for ant in lem.antonyms():\n",
    "            AntArr.append(ant.name())\n",
    "        \n",
    "AntArr\n",
    "\n",
    "set(AntArr)\n",
    "#len(set(AntArr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.6375861597263857"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Similarities between two diff words by wu palmer, path, LCH\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "#or import * - it will import all the things\n",
    "sarr1=wordnet.synsets('cake')\n",
    "sarr2=wordnet.synsets('loaf')\n",
    "sarr3=wordnet.synsets('bread')\n",
    "\n",
    "cake=sarr1[0]\n",
    "loaf=sarr2[0]\n",
    "loafb=sarr2[1]\n",
    "bread=sarr3[0]\n",
    "\n",
    "#wu Palmer similarity\n",
    "\n",
    "cake.wup_similarity(loaf) #0.26666666666666666\n",
    "loafb.wup_similarity(loaf) #0.7142857142857143\n",
    "bread.wup_similarity(loafb) #0.7692307692307693\n",
    "\n",
    "loaf.hypernyms() #[Synset('bread.n.01')]\n",
    "bread.hypernyms()\n",
    "\n",
    "ref=loaf.hypernyms()[0]\n",
    "ref\n",
    "\n",
    "loaf.shortest_path_distance(ref) #1\n",
    "\n",
    "bread.shortest_path_distance(ref) # 0 means both are same. as number increases it will differ.\n",
    "\n",
    "catarr=wordnet.synsets('cat')\n",
    "dogarr=wordnet.synsets('dog')\n",
    "\n",
    "coi=catarr[0]\n",
    "doi=dogarr[0]\n",
    "\n",
    "doi.wup_similarity(coi) #0.8571428571428571\n",
    "\n",
    "doi.path_similarity(coi) #.2\n",
    "\n",
    "doi.path_similarity(doi) # 1 is maximum value\n",
    "\n",
    "doi.lch_similarity(coi) #lch means leacock chodorow. 2.0281482472922856\n",
    "\n",
    "doi.lch_similarity(doi) # Maximum value is \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('black', 'knight'),\n",
       " ('clop', 'clop'),\n",
       " ('head', 'knight'),\n",
       " ('mumble', 'mumble'),\n",
       " ('squeak', 'squeak'),\n",
       " ('saw', 'saw'),\n",
       " ('holy', 'grail'),\n",
       " ('run', 'away'),\n",
       " ('french', 'guard'),\n",
       " ('cartoon', 'character')]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Bigram like lets go, united kingdom, words combination of two words.\n",
    "\n",
    "from nltk.corpus import webtext\n",
    "from nltk.collocations import BigramCollocationFinder\n",
    "from nltk.metrics import BigramAssocMeasures\n",
    "\n",
    "textwords=[w.lower() for w in webtext.words('pirates.txt')]\n",
    "textwords\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(textwords)\n",
    "finder\n",
    "finder.nbest(BigramAssocMeasures.likelihood_ratio,10)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "ignored_words = set(stopwords.words('english'))\n",
    "ignored_words\n",
    "\n",
    "filterStops = lambda w: len(w) < 3 or w in ignored_words # Lambda function to filter out words\n",
    "\n",
    "finder.apply_word_filter(filterStops)\n",
    "finder.nbest(BigramAssocMeasures.likelihood_ratio,10)\n",
    "\n",
    "#Trigrams \n",
    "\n",
    "from nltk.collocations import TrigramCollocationFinder\n",
    "from nltk.metrics import TrigramAssocMeasures\n",
    "\n",
    "from nltk.corpus import webtext\n",
    "\n",
    "textwords=[w.lower() for w in webtext.words('grail.txt')]\n",
    "textwords\n",
    "\n",
    "finder = BigramCollocationFinder.from_words(textwords)\n",
    "finder\n",
    "finder.nbest(BigramAssocMeasures.likelihood_ratio,10)\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "ignored_words = set(stopwords.words('english'))\n",
    "ignored_words\n",
    "\n",
    "filterStops = lambda w: len(w) < 3 or w in ignored_words # Lambda function to filter out words\n",
    "\n",
    "finder.apply_word_filter(filterStops)\n",
    "finder.nbest(BigramAssocMeasures.likelihood_ratio,10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'writ'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Stemming - Pick out the root word from the given words.Like cooking will become cook as part of stemming\n",
    "#Stemming algorithms work by cutting off the end or the beginning of the word, \n",
    "#taking into account a list of common prefixes and suffixes that can be found in an inflected word. \n",
    "#This indiscriminate cutting can be successful in some occasions, but not always, \n",
    "#and that is why we affirm that this approach presents some limitations. Below we illustrate the method with examples in \n",
    "#both English and Spanish.\n",
    "\n",
    "from nltk.stem import PorterStemmer    # Least agressive stemming method. Most common use.\n",
    "from nltk.stem import LancasterStemmer # Most agressive stemming method. it is used for huge data. Most common use\n",
    "from nltk.stem import RegexpStemmer    # When we use small datasets to work with. Quite focus for result to getting.\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "pstemmer = PorterStemmer()\n",
    "pstemmer.stem('dancing') #danc\n",
    "pstemmer.stem('dancer') #dancer\n",
    "pstemmer.stem('cooking') #cook\n",
    "\n",
    "#Or we can do in this way also\n",
    "text=\"It is important to by very pythonly while you are pythoning with python.All pythoners have pythoned poorly at least once.\"\n",
    "words = word_tokenize(text)\n",
    "\n",
    "for w in words:\n",
    "    print(pstemmer.stem(w))\n",
    "    \n",
    "\n",
    "lstemmer =LancasterStemmer()\n",
    "lstemmer.stem('dancing') # dant\n",
    "lstemmer.stem('cooking') #cook\n",
    "\n",
    "rstemmer=RegexpStemmer('ing')\n",
    "rstemmer.stem('skiing') #ski\n",
    "rstemmer.stem('dancing') #danc\n",
    "rstemmer.stem('king') #k\n",
    "lstemmer.stem('king') #king\n",
    "\n",
    "rstemmer1=RegexpStemmer('er')\n",
    "rstemmer1.stem('writer') #writ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bus'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lemmatization, on the other hand, takes into consideration the morphological analysis of the words. \n",
    "#To do so, it is necessary to have detailed dictionaries which the algorithm can look through to link the form back to its lemma.\n",
    "#Again, you can see how it works with the same example words.\n",
    "# https://blog.bitext.com/what-is-the-difference-between-stemming-and-lemmatization/\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "lzr=WordNetLemmatizer()\n",
    "lzr.lemmatize('dancing') #dancing\n",
    "lzr.lemmatize('working') #working. It assume it is noun. it will give diff result with verb.\n",
    "\n",
    "lzr.lemmatize('dancing', pos='v') # dance\n",
    "lzr.lemmatize('working', pos='a')\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "stm=PorterStemmer()\n",
    "\n",
    "stm.stem('believes') #believ. Chopping of the word by using suffix and preffix\n",
    "\n",
    "lzr.lemmatize('believes') #belief. It uses the base word\n",
    "\n",
    "stm.stem('buses') #buse\n",
    "lzr.lemmatize('buses') #bus\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Orignal line :i won't go school\n",
      "\n",
      " Edited line :i will not go school\n"
     ]
    }
   ],
   "source": [
    "#Regular Expression Replacer. https://www.guru99.com/python-regular-expressions-complete-tutorial.html\n",
    "#In Python regular expression supports various things like Modifiers, Identifiers, and White space characters.\n",
    "\n",
    "#re.match() - match a entire string or substring based on pattern.\n",
    "#re.search() - search for a pattern\n",
    "#re.findall() - find all pattern in string\n",
    "#re.split() - split a string\n",
    "#re.sub()\n",
    "#re.compile()\n",
    "\n",
    "#Note: It's important to prefix your regex patterns with r to ensure that your patterns are interpreted in the way you want \n",
    "#them to. Else, you may encounter problems to do with escape sequences in strings. For example, \"\\n\" in Python is used to \n",
    "#indicate a new line, but if you use the r prefix, it will be interpreted as the raw string \"\\n\" - that is, the character \"\\\" \n",
    "#followed by the character \"n\" - and not as a new line.\n",
    "\n",
    "#+ = matches 1 or more , ? = matches 0 or 1, * = 0 or more, $ match end of a string,^ match start of a string,\n",
    "#| matches either or x/y\n",
    "\n",
    "#\\d= any number (a digit)\n",
    "#\\w=One word character including digits \n",
    "#\\s= space, \\t =tab, \\n = new line, \\e = escape\n",
    "\n",
    "#\\D=One non-digit like others W, S and so on\\\\\n",
    "\n",
    "#$ End of string ,^ Start of string, ab|cd - Matches ab or de. [ab-d]\tOne character of: a, b, c, d\n",
    "#[^ab-d]\t One character except: a, b, c, d , () Items within parenthesis are retrieved\n",
    "#(a(bc)) Items within the sub-parenthesis are retrieved\n",
    "\n",
    "#[ab]{2,} - 2 or more continuous occurrences of a or b. + One or more. * Zero or more\n",
    "#? 0 or 1.  . - One character except new line\n",
    "\n",
    "import re\n",
    "#Complile fucntion only checks the pattern\n",
    "regex = re.compile('\\s+') #Here the '\\s' matches any WHITESPACES character. By adding a '+' notation at the end will make the pattern match \n",
    "#at least 1 or more spaces. So, this pattern will match even tab '\\t' characters as well.\n",
    "\n",
    "text = \"101 COM    Computers 205 MAT   Mathematics 189 ENG   English\"\n",
    "\n",
    "re.split('\\s+', text) #['101', 'COM', 'Computers', '205', 'MAT', 'Mathematics', '189', 'ENG', 'English']\n",
    "\n",
    "regex_num = re.compile('\\d+') #'\\d' is a regular expression which matches any digit\n",
    "regex_num.findall(text)  #Adding a '+' symbol to it mandates the presence of at least 1 digit to be present in order to be found.\n",
    "#Similar to '+', there is a '*' symbol which requires 0 or more digits in order to be found\n",
    "\n",
    "regex_num = re.compile('\\d+')\n",
    "s = regex_num.search(text) #But unlike findall which returns the matched portions of the text as a list, regex.search() returns a particular \n",
    "#match object that contains the starting and ending positions of the first occurrence of the pattern.\n",
    "\n",
    "# replace one or more spaces with single space. replace texts, use the regex.sub()\n",
    "regex = re.compile('\\s+')\n",
    "print(regex.sub(' ', text))\n",
    "\n",
    "regex = re.compile('((?!\\n)\\s+)') #negative lookahead (?!\\n). It checks for an upcoming newline character and excludes it from the pattern.\n",
    "print(regex.sub(' ', text)) # ? is for match and then ! is for exclude and then \\n is for new line\n",
    "\n",
    "# 1. extract all course numbers\n",
    "re.findall('[0-9]+', text) #the pattern [0-9]+ instructs to match all number from 0 to 9.\n",
    "\n",
    "#[A-Za-z]+ - find all small and capital letters \n",
    "#[A-Za-z\\-\\.] \"My-Website.com like patterns\n",
    "#(a-z) - 'a-z' pattern\n",
    "#(\\s+|,) - ','\n",
    "\n",
    "# 2. extract all course codes\n",
    "re.findall('[A-Z]{3}', text) #number will certainly have exactly 3 digits, the pattern could have been [0-9]{3} instead.\n",
    "\n",
    "# 3. extract all course names\n",
    "re.findall('[A-Za-z]{4,}', text)\n",
    "\n",
    "course_pattern = '([0-9]+)\\s*([A-Z]{3})\\s*([A-Za-z]{4,})' # Full Pattern\n",
    "re.findall(course_pattern, text)\n",
    "\n",
    "#What is greedy matching in regex? The default behavior of regular expressions is to be greedy. That means it tries to extract\n",
    "#as much as possible until it conforms to a pattern even when a smaller part would have been syntactically sufficient.\n",
    "\n",
    "text = \"< body>Regex Greedy Matching Example < /body>\"\n",
    "re.findall('<.*>', text) # It will fetch whole string coz ? is not there\n",
    "\n",
    "re.findall('<.*?>', text)  #> ['< body>', '< /body>']\n",
    "\n",
    "re.search('<.*?>', text).group() #> '< body>'. if wanted to retrieve only matched.\n",
    "\n",
    "#######################################EXAMPLES ----------------\n",
    "\n",
    "text = '01, Jan 2015'\n",
    "print(re.findall('\\d{4}', text))  # {n} Matches repeat n times.\n",
    "print(re.findall('\\d{2,4}', text))\n",
    "#> ['2015']\n",
    "#> ['01', '2015']\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "101 COM Computers 205 MAT Mathematics 184 ENG\n",
      "101 COM Computers 205 MAT Mathematics 184 ENG\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['< body>Regex Greedy Matching Example < /body>']"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "regex=re.compile(r'won\\'t') # \\ escape character to not confuse double quotes 't'.\n",
    "fst=\"i won't go school\"\n",
    "\n",
    "sst=regex.sub(\"will not\", fst)\n",
    "\n",
    "print(\"\\n Orignal line :\" +fst)\n",
    "print(\"\\n Edited line :\" +sst)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Synonyms Replacement with WordMap\n",
    "\n",
    "class WordReplacer(object):\n",
    "    def_init_(self, word_map):\n",
    "        self.word_map = word_map\n",
    "    def replace (self, word):\n",
    "        return self.word_map.get(word, word)\n",
    "\n",
    "from examplereplacer import WordReplacer\n",
    "\n",
    "replacer = WordReplacer ({'bday' : 'Birthday', 'sup' : 'wassup'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['He', 'has', '11', 'cats']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "re.match('abc',\"abcefg\") # match will train with beginning untill it will not any longer but search will search in whole string.\n",
    "re.search('ce',\"abcefg\") # in this case match will return nothing\n",
    "word_pattern='\\w+'\n",
    "digit_pattern='\\d+'\n",
    "re.match(word_pattern,\"hi there!\")\n",
    "re.match(digit_pattern,'102')\n",
    "\n",
    "match_digits_words = ('(\\d+|\\w+)') # find digits and words both.\n",
    "re.findall(match_digits_words,\"He has 11 cats\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the regex module\n",
    "import re\n",
    "\n",
    "# Write a pattern to match sentence endings: sentence_endings\n",
    "sentence_endings = r\"[.?!]\"\n",
    "\n",
    "# Split my_string on sentence endings and print the result\n",
    "print(re.split(sentence_endings, my_string))\n",
    "\n",
    "# Find all capitalized words in my_string and print the result\n",
    "capitalized_words = r\"[A-Z]\\w+\"\n",
    "print(re.findall(capitalized_words, my_string))\n",
    "\n",
    "# Split my_string on spaces and print the result\n",
    "spaces = r\"\\s+\"\n",
    "print(re.split(spaces, my_string))\n",
    "\n",
    "# Find all digits in my_string and print the result\n",
    "digits = r\"\\d+\"\n",
    "print(re.findall(digits, my_string))\n",
    "\n",
    "# Search for the first occurrence of \"coconuts\" in scene_one: match\n",
    "match = re.search(\"coconuts\", scene_one)\n",
    "\n",
    "# Print the start and end indexes of match\n",
    "print(match.start(), match.end())\n",
    "\n",
    "# Write a regular expression to search for anything in square brackets: pattern1\n",
    "pattern1 = r\"\\[.*\\]\"\n",
    "\n",
    "# Use re.search to find the first text in square brackets\n",
    "print(re.search(pattern1, scene_one))\n",
    "\n",
    "# Find the script notation at the beginning of the fourth sentence and print it\n",
    "pattern2 = r\"[\\w\\s]+:\"\n",
    "print(re.match(pattern2, sentences[3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary modules\n",
    "from nltk.tokenize import regexp_tokenize\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "# Define a regex pattern to find hashtags: pattern1\n",
    "pattern1 = r\"#\\w+\"\n",
    "\n",
    "# Use the pattern on the first tweet in the tweets list\n",
    "regexp_tokenize(tweets[0], pattern1)\n",
    "\n",
    "# Write a pattern that matches both mentions and hashtags\n",
    "pattern2 = r\"([@#]\\w+)\"\n",
    "\n",
    "# Use the pattern on the last tweet in the tweets list\n",
    "regexp_tokenize(tweets[-1], pattern2)\n",
    "\n",
    "'Wann gehen wir Pizza essen? 🍕 Und fährst du mit Über? 🚕'\n",
    "# Use the TweetTokenizer to tokenize all tweets into one list\n",
    "tknzr = TweetTokenizer()\n",
    "all_tokens = [tknzr.tokenize(t) for t in tweets]\n",
    "print(all_tokens)\n",
    "\n",
    "# Tokenize and print all words in german_text\n",
    "all_words = word_tokenize(german_text)\n",
    "print(all_words)\n",
    "\n",
    "# Tokenize and print only capital words\n",
    "capital_words = r\"[A-ZÜ]\\w+\"\n",
    "print(regexp_tokenize(german_text, capital_words))\n",
    "\n",
    "# Tokenize and print only emoji\n",
    "emoji = \"['\\U0001F300-\\U0001F5FF'|'\\U0001F600-\\U0001F64F'|'\\U0001F680-\\U0001F6FF'|'\\u2600-\\u26FF\\u2700-\\u27BF']\"\n",
    "print(regexp_tokenize(german_text, emoji))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 3), ('cat', 3)]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import sys\n",
    "# !{sys.executable} -m install collection\n",
    "from nltk.tokenize import word_tokenize\n",
    "from collections import Counter\n",
    "tokens=Counter(word_tokenize(\"\"\"The cat is in the box. The cat likes the box. The box is over the cat.\"\"\"))\n",
    "Counter(tokens).most_common(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "# Print the 10 most common tokens\n",
    "print(counter(tokens).most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = [w for w in word_tokenize(text.lower()) if w.isalpha()]\n",
    "no_stop=[t for t in tokens if t not in stopwords.words('english')]\n",
    "\n",
    "# Import WordNetLemmatizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Retain alphabetic words: alpha_only\n",
    "alpha_only = [t for t in lower_tokens if t.isalpha()]\n",
    "\n",
    "# Remove all stop words: no_stops\n",
    "no_stops = [t for t in alpha_only if t not in english_stops]\n",
    "\n",
    "# Instantiate the WordNetLemmatizer\n",
    "wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Lemmatize all tokens into a new list: lemmatized\n",
    "lemmatized = [wordnet_lemmatizer.lemmatize(t) for t in no_stops]\n",
    "\n",
    "# Create the bag-of-words: bow\n",
    "bow = Counter(lemmatized)\n",
    "\n",
    "# Print the 10 most common tokens\n",
    "print(bow.most_common(10))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Word vectors = Word vectors are multi-dimensional mathematical representations of words created using deep learning methods. \n",
    "# They give us insight into relationships between words in a corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Dictionary\n",
    "from gensim.corpora.dictionary import Dictionary\n",
    "\n",
    "# Create a Dictionary from the articles: dictionary\n",
    "dictionary = Dictionary(articles)\n",
    "\n",
    "# Select the id for \"computer\": computer_id\n",
    "computer_id = dictionary.token2id.get(\"computer\")\n",
    "\n",
    "# Use computer_id with the dictionary to print the word\n",
    "print(dictionary.get(computer_id))\n",
    "\n",
    "# Create a MmCorpus: corpus\n",
    "corpus = [dictionary.doc2bow(article) for article in articles]\n",
    "\n",
    "# Print the first 10 word ids with their frequency counts from the fifth document\n",
    "print(corpus[4][:10])\n",
    "\n",
    "# Save the fifth document: doc\n",
    "doc = corpus[4]\n",
    "\n",
    "# Sort the doc for frequency: bow_doc\n",
    "bow_doc = sorted(doc, key=lambda w: w[1], reverse=True)\n",
    "\n",
    "# Print the top 5 words of the document alongside the count\n",
    "for word_id, word_count in bow_doc[:5]:\n",
    "    print(dictionary.get(word_id), word_count)\n",
    "    \n",
    "# Create the defaultdict: total_word_count\n",
    "total_word_count = defaultdict(int)\n",
    "for word_id, word_count in itertools.chain.from_iterable(corpus):\n",
    "    total_word_count[word_id] += word_count\n",
    "\n",
    "# Create a sorted list from the defaultdict: sorted_word_count \n",
    "sorted_word_count = sorted(total_word_count.items(), key=lambda w: w[1], reverse=True) \n",
    "\n",
    "# Print the top 5 words across all documents alongside the count\n",
    "for word_id, word_count in sorted_word_count[:5]:\n",
    "    print(dictionary.get(word_id), word_count)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is tf-idf?\n",
    "# You want to calculate the tf-idf weight for the word \"computer\", which appears five times in a document containing 100 words. \n",
    "#Given a corpus containing 200 documents, with 20 documents mentioning the word \"computer\", tf-idf can be calculated by \n",
    "#multiplying term frequency with inverse document frequency.\n",
    "\n",
    "# Term frequency = percentage share of the word compared to all tokens in the document Inverse document frequency = \n",
    "#logarithm of the total number of documents in a corpora divided by the number of documents containing the term\n",
    "\n",
    "#Awnser = (5 / 100) * log(200 / 20)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
