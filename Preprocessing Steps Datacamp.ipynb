{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtype - Object (string values or mixed type), int64, float64 type, datetype64 - datetime\n",
    "#converting the column data type - df[\"c\"] =df[\"c\"].astype(\"float\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(x,y) - by default it split into 75:25 ratio\n",
    "# If you have imbalanced classed then we have to split using statified sampling method\n",
    "#y[\"lables\"].value_counts()\n",
    "#X_train, X_test, y_train, y_test = train_test_split(x,y, stratify=y)\n",
    "#y_train[\"lables\"].value_counts()\n",
    "#y_test[\"lables\"].value_counts()\n",
    "\n",
    "# # Create a data with all columns except category_desc\n",
    "# volunteer_X = volunteer.drop(\"category_desc\", axis=1)\n",
    "\n",
    "# # Create a category_desc labels dataset\n",
    "# volunteer_y = volunteer[[\"category_desc\"]]\n",
    "\n",
    "# # Use stratified sampling to split up the dataset according to the volunteer_y dataset\n",
    "# X_train, X_test, y_train, y_test = train_test_split(volunteer_X, volunteer_y, stratify=volunteer_y)\n",
    "\n",
    "# # Print out the category_desc counts on the training y labels\n",
    "# print(y_train[\"category_desc\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardization Method - It is preprocessing method which tranforms the continious numerical data to make normally distributed.\n",
    "#It is used when features are at different scale of varience.\n",
    "#Sklearn assume it is normally distributed. \n",
    "\n",
    "# When to standardize the data - When model is in linear space or use linear distance to predict like KNN, KMeans, LinearRegre\n",
    "# They assume data is normally distrubuted.\n",
    "#dataset features have high variance. Data set features are continous and on diff scale. Ex- height and weight features.\n",
    "#It should be on same scale or same space.\n",
    "# Log normalization is one of the method to standardize when features has high variance.\n",
    "#Log normalization captures relative change, the magnitude of change and keeps everthing in the positve space.\n",
    "\n",
    "#print(df.var()) - to see the variance - np.log(df[\"col2\"])\n",
    "\n",
    "#Standard Scalling - When different features have different scale and it is used in model which uses linear space.\n",
    "#Standard scale tranaforms the data into normally distributed and make mean 0 and varaince 1.\n",
    "#mean 0 means it removes mean from data and make variance same for all features.\n",
    "\n",
    "#When values relatively close within the column but not accross the column like- Variance is different across column\n",
    "# col1 col2 col3\n",
    "# 1.0  42.0   100.00\n",
    "# 1.20 45.0   101.00\n",
    "\n",
    "#from sklearn.preprocessing import StandardScalar\n",
    "#scaler = StandardScalar() - it doesn't need to rescale every time for test and train data.\n",
    "#df_scale = pd.DataFrame(scalar.fit_transform(df),column=df.columns)\n",
    "#print(df_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering - Creatiion of new features beased on existing features.\n",
    "#Examples - Converting text into vectors in NLP, String data converts into number by dummy variables, \n",
    "#Timestamps with sec or milisec so need to make new features for day or months.\n",
    "#users[\"sub_enc\"]=user[\"Subscribed\"].apply(lambda val: 1 if val==y else 0)\n",
    "\n",
    "#from sklearn.preprocessing import LabelEncoder - it is used when we use pipeliine functionality to merge the things\n",
    "#le=LabelEncoder()\n",
    "#le.fit_transform(user[Subscribed])\n",
    "\n",
    "#OneHotEncoding - mainly used when one or more category in features. Like blue, green, orange -[1,0,0] [0,1,0] [0,0,1]\n",
    "#getdummy function in pandas also do the same.\n",
    "\n",
    "#LabelEncoder is for ordinal data, while OHE is for nominal data. Label ENcoder mainly used for tree based model.\n",
    "#While OHE used for non tree based model but it will have high dimentionally due to increase in columns so must use PCA.\n",
    "#When we can come up with a label encoder that assigns close labels to similar categories: This leads to less spilts in the tress\n",
    "#hence reducing the execution time.\n",
    "\n",
    "#Numerical features engineering in dates column - given temp for 4 days so we will get mean of all the days and make one features\n",
    "# of mean value. If we have date and purchase then we will use month and year or both to analyze the same.\n",
    "# We will need higher level information.\n",
    "\n",
    "#apply(lambda row: row[columns].mean(),axis=1) and apply(lambda row: row.month)\n",
    "\n",
    "#Features Engineering in text - like regerx handles with string - re.compile(\"\\d+\\.\\d+\") - |d+ for digits and \\. for point and then digit\n",
    "#from sklearn.featuresextraction.text import TfidfVectorizer\n",
    "#tfidf_vec =Tfidfvectorizer()\n",
    "#Text_tfidf_vec=tfidf_vec.fir_transform(documents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Selection - selecting features to be used for modeling, improve performance.\n",
    "#It removes unneccesary features from data set which impacts your model like correlated features.\n",
    "\n",
    "#removing reductant features - remove noisy features, remove correlated features or remove duplicate features\n",
    "\n",
    "#Correlated features - features move together directionally. Linear model assume feature independence.\n",
    "#Person correlation coefficient - measure directionally. print(df.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetch Top features from tfidf vector\n",
    "# # Add in the rest of the parameters\n",
    "# def return_weights(vocab, original_vocab, vector, vector_index, top_n):\n",
    "#     zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))\n",
    "    \n",
    "#     # Let's transform that zipped dict into a series\n",
    "#     zipped_series = pd.Series({vocab[i]:zipped[i] for i in vector[vector_index].indices})\n",
    "    \n",
    "#     # Let's sort the series to pull out the top n weighted words\n",
    "#     zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index\n",
    "#     return [original_vocab[i] for i in zipped_index]\n",
    "\n",
    "# # Print out the weighted words\n",
    "# print(return_weights(vocab, tfidf_vec.vocabulary_, text_tfidf, 8, 3))\n",
    "\n",
    "# def words_to_filter(vocab, original_vocab, vector, top_n):\n",
    "#     filter_list = []\n",
    "#     for i in range(0, vector.shape[0]):\n",
    "    \n",
    "#         # Here we'll call the function from the previous exercise, and extend the list we're creating\n",
    "#         filtered = return_weights(vocab, original_vocab, vector, i, top_n)\n",
    "#         filter_list.extend(filtered)\n",
    "#     # Return the list in a set, so we don't get duplicate word indices\n",
    "#     return set(filter_list)\n",
    "\n",
    "# # Call the function to get the list of word indices\n",
    "# filtered_words = words_to_filter(vocab, tfidf_vec.vocabulary_, text_tfidf, 3)\n",
    "\n",
    "# # By converting filtered_words back to a list, we can use it to filter the columns in the text vector\n",
    "# filtered_text = text_tfidf[:, list(filtered_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#End of preprocessing method through PCa or dimentionality reduction technique\n",
    "\n",
    "#PCA - It is unsupervised learning method which combine or decompose a features space.\n",
    "#It uses linear transformation to reduce the features for uncorrelated features. Variance is captured in meaning full way by combining \n",
    "#features into component.\n",
    "\n",
    "#from sklearn.decompossion import PCA - number of component is equals to the no of features.\n",
    "#pca = PCA()\n",
    "#df_pca = pca.fit_transform()\n",
    "#print(df_pca)\n",
    "#print(pca.explained_variance_ratio_) - We will drop those component which dont explain much vaiance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
