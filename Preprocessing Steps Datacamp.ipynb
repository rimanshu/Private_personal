{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dtype - Object (string values or mixed type), int64, float64 type, datetype64 - datetime\n",
    "#converting the column data type - df[\"c\"] =df[\"c\"].astype(\"float\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Outliers - https://towardsdatascience.com/ways-to-detect-and-remove-the-outliers-404d16608dba\n",
    "\n",
    "https://medium.com/datadriveninvestor/finding-outliers-in-dataset-using-python-efc3fce6ce32\n",
    "\n",
    "https://medium.com/codezillas/statistics-review-for-data-scientists-and-management-df8f94760221\n",
    "\n",
    "In statistics, an outlier is an observation point that is distant from other observations.\n",
    "\n",
    "The above definition suggests that outlier is something which is separate/different from the crowd\n",
    "\n",
    "There are two types of analysis we will follow to find the outliers- \n",
    "\n",
    "Uni-variate(one variable outlier analysis using box plot) and Multi-variate(two or more variable outlier analysis using scatter plot).\n",
    "\n",
    "Discover outliers with visualization tools - Box plot-Wikipedia Definition,\n",
    "\n",
    "from scipy import stats   \n",
    "import numpy as np    \n",
    "z = np.abs(stats.zscore(boston_df))   \n",
    "print(z) \n",
    "\n",
    "\n",
    "The intuition behind Z-score is to describe any data point by finding their relationship with the Standard Deviation and Mean of the group of data points. Z-score is finding the distribution of data where mean is 0 and standard deviation is 1 i.e. normal distribution.\n",
    "\n",
    "How can we identify an outlier?   \n",
    "1) using scatter plots  \n",
    "2) using Z score  \n",
    "3) using the IQR interquartile range  \n",
    "4) Box plot for univarient analysis.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Missing Value Imputation Methods "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mean, Median and Mode Imputation\n",
    "\n",
    "Using the measures of central tendency involves substituting the missing values with the mean or median for numerical variables\n",
    "and the mode for categorical variables. The major limitation of using this method is that it leads to biased estimates of the \n",
    "variances and covariance. The standard errors and test statistics can also be underestimated and overestimated respectively. \n",
    "This imputation technique works well with when the values are missing completely at random. \n",
    "\n",
    "Scikit-learn comes with an imputed function in the form sklearn.preprocessing.Imputer(missing_values='NaN', strategy='mean', axis=0, verbose=0, copy=True). \n",
    "Strategy is the imputation strategy and the default is the \"mean\" of the axis (0 for columns and 1 for rows). \n",
    "The other strategies are \"median\" and \"most_frequent\". Another API that can be used for this imputation is fancyimpute.\n",
    "SimpleFill().\n",
    "\n",
    "The scikit-learn library provides the Imputer() pre-processing class that can be used to replace missing values.\n",
    "\n",
    "It is a flexible class that allows you to specify the value to replace (it can be something other than NaN) and the \n",
    "technique used to replace it (such as mean, median, or mode). The Imputer class operates directly on the NumPy array instead \n",
    "of the DataFrame.\n",
    "\n",
    "import numpy as np  \n",
    "from sklearn.impute import SimpleImputer   \n",
    "imp = SimpleImputer(missing_values=np.nan, strategy='mean')   \n",
    "imp.fit([[1, 2], [np.nan, 3], [7, 6]])   \n",
    "SimpleImputer()      \n",
    "\n",
    "import numpy as np   \n",
    "from sklearn.impute import KNNImputer   \n",
    "nan = np.nan   \n",
    "X = [[1, 2, nan], [3, 4, 3], [nan, 6, 5], [8, 8, 7]]    \n",
    "imputer = KNNImputer(n_neighbors=2, weights=\"uniform\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#X_train, X_test, y_train, y_test = train_test_split(x,y) - by default it split into 75:25 ratio\n",
    "# If you have imbalanced classed then we have to split using statified sampling method\n",
    "#y[\"lables\"].value_counts()\n",
    "#X_train, X_test, y_train, y_test = train_test_split(x,y, stratify=y)\n",
    "#y_train[\"lables\"].value_counts()\n",
    "#y_test[\"lables\"].value_counts()\n",
    "\n",
    "# # Create a data with all columns except category_desc\n",
    "# volunteer_X = volunteer.drop(\"category_desc\", axis=1)\n",
    "\n",
    "# # Create a category_desc labels dataset\n",
    "# volunteer_y = volunteer[[\"category_desc\"]]\n",
    "\n",
    "# # Use stratified sampling to split up the dataset according to the volunteer_y dataset\n",
    "# X_train, X_test, y_train, y_test = train_test_split(volunteer_X, volunteer_y, stratify=volunteer_y)\n",
    "\n",
    "# # Print out the category_desc counts on the training y labels\n",
    "# print(y_train[\"category_desc\"].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Standardization Method - It is preprocessing method which tranforms the continious numerical data to make normally distributed.\n",
    "#It is used when features are at different scale of varience.\n",
    "#Sklearn assume it is normally distributed. \n",
    "\n",
    "# When to standardize the data - When model is in linear space or use linear distance to predict like KNN, KMeans, LinearRegre\n",
    "# They assume data is normally distrubuted.\n",
    "#dataset features have high variance. Data set features are continous and on diff scale. Ex- height and weight features.\n",
    "#It should be on same scale or same space.\n",
    "# Log normalization is one of the method to standardize when features has high variance.\n",
    "#Log normalization captures relative change, the magnitude of change and keeps everthing in the positve space.\n",
    "\n",
    "#print(df.var()) - to see the variance - np.log(df[\"col2\"])\n",
    "\n",
    "#Standard Scalling - When different features have different scale and it is used in model which uses linear space.\n",
    "#Standard scale tranaforms the data into normally distributed and make mean 0 and varaince 1.\n",
    "#mean 0 means it removes mean from data and make variance same for all features.\n",
    "\n",
    "#When values relatively close within the column but not accross the column like- Variance is different across column\n",
    "# col1 col2 col3\n",
    "# 1.0  42.0   100.00\n",
    "# 1.20 45.0   101.00\n",
    "\n",
    "#from sklearn.preprocessing import StandardScalar\n",
    "#scaler = StandardScalar() - it doesn't need to rescale every time for test and train data.\n",
    "#df_scale = pd.DataFrame(scalar.fit_transform(df),column=df.columns)\n",
    "#print(df_scale)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Engineering - Creatiion of new features beased on existing features.\n",
    "#Examples - Converting text into vectors in NLP, String data converts into number by dummy variables, \n",
    "#Timestamps with sec or milisec so need to make new features for day or months.\n",
    "#users[\"sub_enc\"]=user[\"Subscribed\"].apply(lambda val: 1 if val==y else 0)\n",
    "\n",
    "#from sklearn.preprocessing import LabelEncoder - it is used when we use pipeliine functionality to merge the things\n",
    "#le=LabelEncoder()\n",
    "#le.fit_transform(user[Subscribed])\n",
    "\n",
    "#OneHotEncoding - mainly used when one or more category in features. Like blue, green, orange -[1,0,0] [0,1,0] [0,0,1]\n",
    "#getdummy function in pandas also do the same.\n",
    "\n",
    "#LabelEncoder is for ordinal data, while OHE is for nominal data. Label ENcoder mainly used for tree based model.\n",
    "#While OHE used for non tree based model but it will have high dimentionally due to increase in columns so must use PCA.\n",
    "#When we can come up with a label encoder that assigns close labels to similar categories: This leads to less spilts in the tress\n",
    "#hence reducing the execution time.\n",
    "\n",
    "#Numerical features engineering in dates column - given temp for 4 days so we will get mean of all the days and make one features\n",
    "# of mean value. If we have date and purchase then we will use month and year or both to analyze the same.\n",
    "# We will need higher level information.\n",
    "\n",
    "#apply(lambda row: row[columns].mean(),axis=1) and apply(lambda row: row.month)\n",
    "\n",
    "#Features Engineering in text - like regerx handles with string - re.compile(\"\\d+\\.\\d+\") - |d+ for digits and \\. for point and then digit\n",
    "#from sklearn.featuresextraction.text import TfidfVectorizer\n",
    "#tfidf_vec =Tfidfvectorizer()\n",
    "#Text_tfidf_vec=tfidf_vec.fir_transform(documents)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Feature Selection - selecting features to be used for modeling, improve performance.\n",
    "#It removes unneccesary features from data set which impacts your model like correlated features.\n",
    "\n",
    "#removing reductant features - remove noisy features, remove correlated features or remove duplicate features\n",
    "\n",
    "#Correlated features - features move together directionally. Linear model assume feature independence.\n",
    "#Person correlation coefficient - measure directionally. print(df.corr())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fetch Top features from tfidf vector\n",
    "# # Add in the rest of the parameters\n",
    "# def return_weights(vocab, original_vocab, vector, vector_index, top_n):\n",
    "#     zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))\n",
    "    \n",
    "#     # Let's transform that zipped dict into a series\n",
    "#     zipped_series = pd.Series({vocab[i]:zipped[i] for i in vector[vector_index].indices})\n",
    "    \n",
    "#     # Let's sort the series to pull out the top n weighted words\n",
    "#     zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index\n",
    "#     return [original_vocab[i] for i in zipped_index]\n",
    "\n",
    "# # Print out the weighted words\n",
    "# print(return_weights(vocab, tfidf_vec.vocabulary_, text_tfidf, 8, 3))\n",
    "\n",
    "# def words_to_filter(vocab, original_vocab, vector, top_n):\n",
    "#     filter_list = []\n",
    "#     for i in range(0, vector.shape[0]):\n",
    "    \n",
    "#         # Here we'll call the function from the previous exercise, and extend the list we're creating\n",
    "#         filtered = return_weights(vocab, original_vocab, vector, i, top_n)\n",
    "#         filter_list.extend(filtered)\n",
    "#     # Return the list in a set, so we don't get duplicate word indices\n",
    "#     return set(filter_list)\n",
    "\n",
    "# # Call the function to get the list of word indices\n",
    "# filtered_words = words_to_filter(vocab, tfidf_vec.vocabulary_, text_tfidf, 3)\n",
    "\n",
    "# # By converting filtered_words back to a list, we can use it to filter the columns in the text vector\n",
    "# filtered_text = text_tfidf[:, list(filtered_words)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#End of preprocessing method through PCa or dimentionality reduction technique\n",
    "\n",
    "#PCA - It is unsupervised learning method which combine or decompose a features space.\n",
    "#It uses linear transformation to reduce the features for uncorrelated features. Variance is captured in meaning full way by combining \n",
    "#features into component.\n",
    "\n",
    "#from sklearn.decompossion import PCA - number of component is equals to the no of features.\n",
    "#pca = PCA()\n",
    "#df_pca = pca.fit_transform()\n",
    "#print(df_pca)\n",
    "#print(pca.explained_variance_ratio_) - We will drop those component which dont explain much vaiance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
